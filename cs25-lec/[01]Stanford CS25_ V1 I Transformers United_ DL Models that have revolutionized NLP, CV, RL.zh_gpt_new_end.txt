大家好，欢迎来到斯坦福大学 CS25 transformers united的第一堂简介课程。
CS25 是我们三个人在 2021 年秋季在斯坦福创建并教授的一门课程。
而这门课的主题并不像图片可能暗示的那样，它不是关于可以变成汽车的机器人。
而是关于深度学习模型，具体来说是一种革命性的深度学习模型，从自然语言处理到计算机视觉，再到强化学习等多个领域都产生了革命性影响。
我们为您准备了一系列令人兴奋的视频。
我们邀请了一些真正出色的演讲者来分享他们如何在自己的研究中应用transformer。
我们希望您会喜欢并从这些演讲中学到东西。
这个视频纯粹是一堂关于transformer的简介课程。
在我们开始之前，我想介绍一下讲师们。
我的名字是 Advair。
我是一家名为 Applied Intuition 的公司的软件工程师。
在这之前，我是斯坦福大学的计算机科学硕士学生。
我是 CS25 的共同讲师之一。
Chaitanya，你们两位可以介绍一下自己。
大家好。
我是斯坦福的博士生。
之前，我在这里攻读硕士学位。
主要研究生成建模，强化学习和机器人技术。
很高兴见到大家。
对了，那个是 Div，因为他没说他的名字。
Chaitanya，如果你想介绍一下自己。
嗨，大家好。
我叫 Chaitanya，目前在一家名为 Moonworks 的初创公司担任机器学习工程师。
在那之前，我是斯坦福的硕士生，专攻自然语言处理，并且是亚马逊 Alexa Prize 挑战的斯坦福团队的成员，获奖者之一。
好的，太棒了。
接下来，我们继续这次演讲的其余部分。
基本上，我们希望你通过观看这些视频能够学到三件事。
一是希望你能够理解 Transformer 的工作原理。
其次，我们希望你能够学习，并在这些讲座结束时理解 Transformer 如何应用于自然语言处理以外的领域。
第三，我们希望这些讲座能在你们中间激发一些新思想，希望能导致研究的新方向、新类型的创新以及类似的事物。
首先，我们将稍微谈谈transformer，并介绍一些transformer背后的背景。
为此，我想把话筒交给Div。
所以，大家好。
欢迎来到我们的transformer研讨会。
我将首先概述注意力时间线以及其形成的过程。
transformer的关键思想是2017年开发的自注意力机制。
这一切都始于一篇名为《Attention is All You Need》的论文，作者是Vohaswani等人。
在2017年之前，我们处于一个前史时期，那时我们有更旧的模型，如RNNs、LSTMs和更简单的注意力机制。
最终，transformer在其他领域的增长已经爆炸性增长，并在机器学习的所有领域中变得突出。
我将去看并展示这是如何被使用的。
所以在前史时期，曾经有RNNs。
有不同的模型，如序列到序列、LSTMs、GRUs。
它们擅长编码某种记忆，但在编码长序列方面效果不佳，并且在编码上下文方面表现非常糟糕。
因此，这里举个例子，如果你有一个句子，比如说，我在法国长大，点点点。
所以我说流利的破折号。
然后你想根据上下文填充这个破折号，但是一个LSTM模型可能不知道这是什么，可能会在这里犯一个非常大的错误。
类似地，我们可以展示一些相关性图，比如说，如果你有一个代词比如它，我们希望将其与我们迄今为止见过的过去的名词之一进行相关联，比如动物。
但是再次强调，旧模型在这种上下文编码方面表现非常糟糕。
所以我们目前所处的位置正处在起飞的边缘。
我们开始意识到transformer在不同领域的潜力。
我们已经开始使用它们来解决蛋白质折叠等长序列问题，比如DeepMind的alpha-fold模型，在离线RL的不同挑战中达到95%的准确率。
我们可以将其用于少样本和零样本泛化，用于文本和图像生成。
我们也可以将其用于内容生成。
这里有一个来自OpenAI的例子，你可以提供不同的文本提示，然后AI生成器就会为你创作一个虚构的图像。
还有一个关于这个话题的讲座，你也可以在YouTube上观看，基本上是说LSTMs已经过时了，transformers才是未来。
那么未来会是什么样子呢？
我们可以让transformers应用于更多的应用场景。
它们可以应用于任何形式的序列建模。
所以我们可以将它们用于视频理解。
我们可以将它们用于金融等等。
所以基本上想象一下各种生成建模问题。
然而，还有很多缺失的要素。
就像人类大脑一样，我们需要一种外部记忆单元，对我们来说就是海马体。
这方面也有一些早期的工作。
一个你可能想看看的好作品叫做神经调谐机。
同样，当前的注意力机制在时间上非常复杂，并且在规模上呈二次增长，我们稍后会讨论这一点。
我们希望让它们变得更线性。
第三个问题是，我们希望将我们当前的语言模型与人类大脑的工作方式和人类价值观相一致。
这也是一个很大的问题。
现在我将更深入地探讨注意力机制，并展示它们是如何产生的。
最初，它们是非常简单的机制。
注意力受到了重要性阅读或将注意力放在图像不同部分的过程的启发，就像人类一样，在看到一张狗的图片时，您可能会更多地关注前景，而不是背景的其他部分。
所以在软注意力的情况下，您要做的是为每个像素学习这个简单的软注意力评分，这可以是介于零到一之间的权重。
这里的问题是这是一个非常昂贵的计算。
然后，如左图所示，您可以看到我们正在为整个图像计算这种注意力。
相反，您可以将其作为零到一的注意力图直接放在狗所在的位置，并在背景处放置零。
这就像是一种较少计算消耗的方法，但问题是它不可区分，并且使训练变得更加困难。
在继续前进时，我们还有在自我注意力之前提出的基本注意力机制的不同品种。
这里的第一个品种是全局注意力模型。
所以在全局注意力模型中，对于每个隐藏层的输入、隐藏层的输出，您学习一个注意力率A，将其与当前输出逐元素相乘，以获得最终输出YT。
类似地，您有局部注意力模型，其中您不是计算整个序列长度的全局注意力，而是仅计算在一个小窗口内的注意力，然后您通过窗口的注意力与当前输出进行加权，以获得所需的最终输出。
接下来，我将交给 Chaitanya 来讨论自注意力机制和转换。
是的，谢谢 Dhruv 简要介绍了注意力的原始版本是如何工作的。
现在，在我们讨论自注意力之前，有一个小小的趣闻，术语是由 Lin 等人提出的，他们提供了一个用于句子嵌入的自注意机制框架。
现在，让我们转向 transformers 论文的主要内容，即自注意力块。
因此，自注意力是使 transformers 模型如此有效并使其功能如此强大的基础，是构建这些模型的主要组件之一。
所以，更容易理解的是，我们可以将自注意力分解为一个搜索检索问题。
所以问题是，给定一个查询 Q，我们需要找到一组最相似的键 K，并返回相应的键值 V。现在，这三个向量可以来自同一源。
例如，我们可以让 Q、K 和 V 都等于一个单一向量 X，其中 X 可以是前一层的输出。
在transformer中，这些向量是通过对 X 应用不同的线性变换获得的，以便使模型能够捕捉句子不同位置的不同令牌之间更复杂的交互。
现在，注意力是如何计算的，只是查询向量和键向量之间相似性的加权求和，其中的权重由这些键的相应值来加权。
而在transformer论文中，他们使用了缩放点积作为查询和键的相似性函数。
transformer的另一个重要方面是引入了多头自注意力。
所谓多头自注意力是指在每一层，自注意力被执行多次，这使得模型能够学习多个表示子空间。
所以从某种意义上说，你可以认为每个头部都有能力查看不同的事物并学习不同的语义。
例如，一个头可能正在学习尝试预测这些标记的词性。
一个头可能正在学习句子的句法结构是什么，以及所有那些可以理解即将到来的句子意义的东西。
现在为了更好地理解自注意力是如何工作的，以及不同的竞争对手，有一个简短的视频。
所以在这里，正如你所看到的，有三个输入标记。
输入一，输入二，输入三。
我们对每个输入应用线性变换来获得键值向量。
然后一旦查询队列到来，我们就会计算与相应键向量的相似性，然后将这些分数与值向量相乘，然后将它们全部相加以获得输出。
然后在所有标记上执行相同的计算，我们得到自注意力层的输出。
所以你可以在这里看到，自注意力层的最终输出是深绿色的，在屏幕顶部。
现在再次，对于最终标记，我们执行相同的步骤，查询乘以键，我们得到相似性分数，然后这些相似性分数加权值向量。
然后我们最终执行消耗以获取transformers的自注意力输出。
除了自注意力之外，还有一些其他必要的因素使得transformer如此强大。
一个重要的方面是存在位置表示或嵌入层。
所以RNNs之所以运行良好，是因为它们处理每个信息时都是按顺序进行的。
所以有这种顺序的概念，对吧？
这在理解语言上也非常重要，因为我们都知道我们阅读任何文本时，大多数语言都是从左到右阅读，而有些语言也是从右到左阅读。
所以有一种顺序的概念在自我注意力中失去了，因为每个词都在关注着其他每个词。
这就是为什么这篇论文引入了一个单独的嵌入层来引入位置表示。
第二个重要的方面是具有非线性。
所以如果你考虑到在自我注意层中发生的所有竞争，它都是线性的，因为它都是矩阵乘法。
但正如我们所知，深度学习模型在能够学习输入与输出之间更复杂的映射时表现良好，这可以通过简单的MLP来实现。
而transformer的第三个重要组成部分是遮蔽。
所以，遮蔽是允许并行化操作的原因。
因为在我们的transformer的解码器部分，每个词都可以关注到其他每个词，Arvay稍后会讲到，出现的问题是你不希望解码器预见未来，因为那会导致数据泄露。
所以这就是为什么遮蔽帮助解码器避免那些未来信息并且只学习模型到目前为止处理的内容。
那么现在讲到transformer的编码器-解码器架构，Arvay。
是的，谢谢Taitanya讲述自注意力。
所以自注意力是一种关键成分或transformer工作得如此好的关键成分之一。
但在很高的层次上，Vaswani等人在2017年的论文中提出的模型，就像之前的语言模型一样，具有编码器-解码器架构。
这意味着，比方说你在处理一个翻译问题。
你想把英语翻译成法语。
这种方式的工作原理是，你会读入整个英文句子的输入。
你会对这个输入进行编码。
所以这就是网络的编码器部分。
然后你会逐个生成令牌，对应法文翻译。
解码器是网络的那部分，负责生成这些令牌。
所以你可以将这些编码器块和解码器块看作基本上像乐高一样的东西。
它们有组成它们的这些子部件。
特别是，编码器块有三个主要的子部件。
第一个是Taitanya之前讲过的自注意力层。
而且如之前讲述，你需要一个前馈层，因为自注意力层只执行线性操作。
所以你需要某样东西来捕捉非线性。
在这之后你还有一个层正则化。
最后，不同编码器块之间有残差连接。
解码器和编码器非常相似，但有一个区别，即它有一个额外的层，因为解码器不仅仅对前一层的输出进行多头注意力操作。
所以为了上下文，编码器在每个编码器块的自注意层中都执行多头注意力，每个编码器块都执行多头注意力，看着前面的编码器块的层。
然而，解码器在这方面也是如此，它还看着解码器前面的层，但它也看着编码器的输出。
所以它需要一个跨编码器块的多头注意力层。
最后，还有掩码。
所以如果你因为每个令牌都可以看到其他每个令牌，你希望在解码器中确保你不会看到未来。
所以如果你在位置三，例如，你不应该能够看到位置四和位置五。
所以这些都是导致 Vaswani 等人在论文中创建模型的所有组件。
让我们来谈谈这种模型的优点和缺点。
所以两个主要优点，这些优点非常巨大，也是为什么transformer在深度学习的许多领域中取得了如此大的成功的原因如下。
所以第一点是，序列中任意两个位置之间存在着常数路径长度，因为序列中的每个令牌都与序列中的其他每个令牌相互关联。
这基本上解决了 Div 早些时候提到的关于长序列的问题。
在长序列中，你不会遇到这样的问题，即如果你试图预测一个令牌，而这个令牌依赖于句子中遥远位置的一个单词，你不会遇到失去上下文的问题。
现在，它们之间的距离在路径长度方面只有一。
另外，由于正在发生的计算的性质，transformer模型非常适合并行化。
由于我们在 GPU 方面取得的进展，如果你拿一个具有 N 个参数的转换模型和一个不是transformer的模型，比如 MSDM，但参数也是 N，创建transformer模型会快得多，因为它利用了并行化。
所以这些是优点。
缺点基本上是自注意力需要二次时间，因为每个令牌都要关注其他每个令牌。
按 N 平方的顺序，你可能知道，不具备可扩展性。
实际上，已经有很多工作在尝试解决这个问题。
所以我们在这里链接了一些内容，BigBird、Linformer和Reformer都是试图使这个线性或准线性的方法。
是的，我们强烈推荐阅读Jay Alomar的博客，《插图transformer》，其中提供了很好的可视化，并详细解释了我们刚才讨论的一切。
是的，我想把它传给Chaitanya，讨论一下transformer的应用。
是的，现在我们转向一些最近的工作，一些紧随transformer论文之后的工作。
所以其中一个模型是GPT，GPT架构是由OpenAI发布的。
OpenAI有最新的模型，OpenAI的GPT系列是GPT-3。
它仅包含transformer中的解码器块，并在我们传统的语言建模任务上进行训练，即预测当前标记，即给定模型最后看到的最后T个标记，创建下一个标记。
对于任何下游任务，现在模型只需在最后隐藏状态上训练一个分类层，该层可以有任意数量的标签。
由于这个模型具有生成性质，您也可以将预训练网络用于生成型任务，例如总结、自然语言生成等。
另一个使GPT-3广受欢迎的重要方面是其能够进行上下文学习，作者称之为上下文学习。
这是模型能够在少量样本设置下学习的能力，任务是在不执行任何梯度更新的情况下完成任务。
例如，假设模型展示了一堆加法示例，然后如果您输入一个新的输入并将其留在等号处，模型会尝试预测下一个令牌，这个令牌很可能是展示的数字的总和。
另一个例子也可以是拼写校正任务或翻译任务。
这就是使GPT-3在自然语言处理领域备受关注的能力。
现在许多应用程序也使用了GPT-3，其中包括其中一个是VS Code协作者，它试图生成一段代码，给定我们的文档字符串类似的自然语言文本。
另一个基于transformer架构的重要模型是BERT。
所以BERT的名字来源于它是一个缩写，双向编码，编码器表示的transformer。
它仅由transformer的编码器块组成，这与仅有解码器块的GPT-3不同。
现在，由于这一变化，因为BERT只有编码器块，所以它看到了整个文本片段。
由于未来数据泄露的问题，它不能在实时语言建模任务上进行预训练。
因此，作者想出了一个聪明的想法，他们提出了一种新的任务，称为掩码语言建模，其中包括用占位符替换某些单词。
然后模型尝试预测这些单词，给定整个上下文。
现在，除了这个令牌级任务，作者还增加了第二个目标，称为下一句预测，这是一个句子级任务，给定两块文本，模型尝试预测第二句是否跟随另一句，是否跟随第一句。
现在，在预训练这个模型用于任何下游任务之后，可以通过增加一个额外的分类层来进一步微调模型，就像在GPT-3中一样。
所以这两个模型一直非常受欢迎，并且在许多应用中发挥了作用，但自从我们开设这门课以来，景观已经发生了很大的变化。
现在有了采用不同预训练技术的模型，如Electra、Deberta，也有在其他模态中表现良好的模型，我们将在其他讲座系列中讨论这些模型。
所以，这就是本讲的全部内容，感谢您的收听。
是的，我只想以此结束，感谢大家观看这个视频，我们有一系列真正令人兴奋的视频和真正了不起的演讲者，我们希望您能从中获得价值。
好的，非常感谢。
谢谢。
谢谢大家。
