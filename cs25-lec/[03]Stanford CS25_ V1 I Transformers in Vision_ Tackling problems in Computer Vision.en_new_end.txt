Today, I'm going to talk to you about vision transformers, since this is all about transformers, specifically their application for visual representation learning.
But before we jump into transformers, I'm going to spend like 10 or 15 minutes giving you a lot of context on all of this, and specifically also on the vision part of things, because I think the majority of what you have seen and will see will be about language.
Right, so let's get started.
My goal and that of my close collaborators is to find general visual representation, and you're going to soon see what that means.
And why or what can we do if we imagine we have a general visual representation?
The hope is that with this, we can really kickstart all kinds of tasks that require visual input.
That means most tasks that you do when you have your eyes open, basically, because if you have a good understanding of what you see, then you can much quicker understand what's going on and what you should do.
And eventually, I have now a little kid since the year, and so I really want that when he's grown up, that there is like some kind of robot.
It doesn't need to be nice and pretty like in movies, just maybe an arm or whatever, that my kid could teach or my parents who cannot program can teach to do some boring task that they really don't want to do.
And I believe one component of this is a good visual representation that generalizes to understanding the world visually everywhere.
It's not all that's required, but it's one part and the part that I'm trying to push.
So this is for context and motivation on working on general visual representation.
And the one good example of a general visual representation is the humans.
And I'm going to show you what I mean by that.
So here is a task that I give you.
There is three classes, class A, B, and C.
And I give you five images of each class, okay?
And here I give you a new image.
And I'm sure that by now you all know which class it is.
I'm not going to ask because I don't actually see you.
If I was in the room, I would raise hands.
But I'm sure you know it's class A now.
Okay, this is fine.
We have seen millions of flowers in our lives, hopefully.
But there is other kinds of pictures like this satellite images that you don't see much in your life.
Some people may have never seen it.
Sometimes like when you fly or maybe on TV or in the internet or so, but it's rather rare.
But still, same story.
Three classes, class A, B, C, five images of each.
And I show you a new image.
This might be a little bit less trivial than the flower, but I think I've spent enough time talking that by now most of you should know that this is class B. Shows a, this is basketball court, right?
All right, now even more abstract.
You don't see this in real life, right?
But still, I give you images of class A and B.
I adjust two to make it a bit easier here because you need to use your brain a little bit more.
And I show you this image.
And now I should do a little bit of smart talk to let you think.
Like you see that there is like spheres, boxes and whatnot.
And by now, I hope that most of you know that this is class A.
Why?
Because there is three objects in class A.
And class B is always, what is it?
Five objects, no matter what they are, what they look like.
Okay, I think by now you more or less understand what I mean when I mean a good visual representation, general visual representation, right?
Some, I don't know how to call it in your brain, in your eyes, such that you can quickly see something new and understand what's going on with just a few examples and then generalize from that.
Right.
And that's the goal.
Then the next step, if you have the goal, how do we measure progress towards it?
And this is a paper we did a few years ago with Michael operators, which you call the visual task adaptation benchmark.
It's kind of formalization of the little game that we just played.
So it's a benchmark.
And there is some component that you or anybody who participated in benchmark does, which is creating a model with some data.
We don't really care what data, what model, how, whatnot.
Just you come with a model.
Then we come with this landscape of all possible visual tasks that kind of make sense, which is a big statement.
And we sample some tasks from that.
And this is kind of the task that you have just seen.
They were actually taken out of this task adaptation benchmark.
And we have for our first step made 19 such tasks where we try to cover broad types of visual tasks, not just classes of natural images like these dogs and cats things, but also a very specialized images like satellite image, also non -classification tasks that involve counting, like the one I showed you before.
But that can be expressed in this simple classification API, but that logically requires some more thinking.
Some things like distance, we have something with cars and with distance of the closest car and things like that.
It should cover a broad range of variation.
And then with the model that you came to this benchmark, you can do some adaptation step on each of the data sets, one after another, all at the same time, doesn't really matter.
But then you should have as a result a model of this data set, which is very small.
It just has seen a few examples for each class that then performs well there.
And then we just take the average score across all of these tasks.
And this is what we call the beta task.
And this is how for now we judge how good of a general visual representation does your model and adaptation algorithm have.
And now just for some nomenclature, this preparation, we have words that we often use pre -training.
Sometimes we call it the upstream, like upstream data, upstream training, something.
So I may use this word interchangeably with pre -training.
And then there is the second part, which we usually call transfer.
And then sometimes we say downstream.
And the adaptation for our, in principle, it's whatever you want.
But for our work, we almost always just use very simple fine -tuning without any bugs and whistles, because it's simple and works well.
In general, we try to do things as simple as possible that still work well.
And so sometimes I even just say, like fine -tuning, when fine -tuning.
That means moving from this pre -training through the transfer.
All right, so, so far for the setting.
So far, so good?
Good.
Then the question is, how do we get there?
And we spend a lot of time thinking about this and trying different things.
And this is also roughly the outline of all that I have available to talk about, which doesn't mean we're gonna cover everything.
So I'm not gonna go through the outline exactly, but you will see this again and again.
And as you see, vision transformer, the transformer only comes a little bit later.
There's some stuff before that.
So this one, just really quickly, because it doesn't matter for this course, is that we spend some time trying self -supervised pre -training, which is very popular in language, and vision only recently has become popular.
And it doesn't work that way.
You don't need to understand these bars, but basically higher is better.
And here, just look at the blue ones.
That's the VTAP score for this few -shot VTAP.
And self -supervised learning performs like this bar.
We tried multiple methods and multiple models and so on.
It was also a proper good benchmark, but it was a couple of years ago.
Then we moved on to semi -supervised training.
So a few labeled examples and a ton of unlabeled examples.
That's this next blue bar.
Did you actually see the mouse cursor?
Sorry.
We don't see the mouse cursor.
Maybe I need to do some.
We can see it.
I see it.
Yeah.
Okay.
Yeah, so then semi -supervised is that blue bar, which is a lot higher than this other blue bar.
So what this means to us is that by adding a few labeled examples, we're able to get much better or much more general visual representation.
Then I'm not going to spend more time on this and how exactly and so on.
But I'm going to move to the next one, which was for us kind of a breakthrough.
When we figured out that if we just skate up, fully supervised pre -training, then we get really much better representations than everything we've seen before.
And here, I want to briefly spend some time on that one because it's the precursor to using vision or transformers in vision.
So it is simple.
There are tons of images on the internet.
That's always what you hear is motivation for self -supervised or unsupervised learning, right?
But actually, where these images come from, there's almost always some extra information, like surrounding the image on the web, or if you collect it otherwise, there's some extra information there that you could use as some weak source of information or some weak label, right?
Then it happens that in Google, there's some team that actually does this for production.
And they have collected already a large dataset with some pipeline that from the surrounding signals somewhat automatically, but very noisily annotates the images.
And we wanted to figure out how far can we go when we scale up pre -training.
Then, long story short, you need a couple ingredients.
One is patience.
I really like this plot.
This is one of the curves of just pre -training on large data with large models.
The details don't really matter.
The gist is that if I zoom into this little box, I see this here, and this is the metric for the training, like the performance in upstream that I see after spending eight GPU weeks of compute.
What does GPU week mean?
It means eight GPUs for a week, or sorry, one GPU for eight weeks, or eight GPUs for one week, or 16 GPUs for a half week and so on, right?
But this looks flat.
A reasonable person would say, yeah, there's no progress for a week on eight GPUs.
This is flat.
I'm going to stop and try something else.
But we are not reasonable, so we keep going.
And this is what the exact same spot looks like after eight GPU months of training.
And you can clearly see the things are progressing, right?
So it may not always be obvious, and you need patience.
The second thing is that you actually need to scale up everything.
So this was work done with ResNet, not yet with transformers.
I see you see a lot of ResNet models here.
The x -axis is the number of images available.
In vision, there is this image net dataset, which is a very common, super common dataset for pre -training, which has 1 .3 million images.
There's another one which has 10 times more images that stay public, and then there is one subset from this internal group that has 300 million labeled images.
So the y -axis is measure of accuracy on some tasks, and we tried many, they all look similar.
And the dots are different besides ResNet.
The blue dot is the standard ResNet PP that everybody uses.
If this one, you trained on more data, it looks promising at first, but if you go to even more data, it looks like, oh, okay, this doesn't really seem that useful.
And this is what most people have been doing for a long time.
And a lot of people, even in Google, were like, yeah, I tried this internal checkpoint on these tons of data.
It doesn't really have that much.
However, what we found out, and in hindsight is kind of obvious, is that you actually need to scale not just the data, but also the model.
Here, this blue dot is a gigantic ResNet that is slow as hell.
But when you scale this up together with the data, you keep getting benefit with adding more data.
And then if you do these two things, scale up everything and be patient, be patient could also be quite scale up your patients, then you get a lot of benefits.
So here there is a few shots transfer learning, what I showed you before.
And on the x -axis is size of the model.
On the y -axis is the accuracy on one of these tests, but again, others look similar.
And these three different curves are featuring with different data set sizes.
The green one being the standard one, you don't really see benefit or small benefit from going with larger models.
The blue one is 10 times larger.
You start seeing some slope upwards, but really only with this giant data, you start getting better and better and better at this future transfer learning when you're pre -trained on more and more data with larger and larger models.
Second benefit that we did not anticipate really at all, but then found out is that these models are super robust when you scale everything up.
This is ObjectNet.
It's a data set that's specifically designed to measure robustness and it shows things in crazy, like a chair in the bathtub and things like that.
And you should recognize it as chair.
And here the pink dots are basically how existing models and exact species, again, how large is the model and pink dots is existing ones from the literature.
And then these lines, same color coding is what we found out.
And again, you see this large data and then going to large model just gives you amazing benefits on like in this case out of distribution robustness.
Okay, so this was like amazing.
Scale up everything, be patient.
And get huge benefit.
Sorry, Lucas, sorry for interrupting you, but there is a question from a student in the class.
Yep.
Right.
Do you want to unmute yourself and ask it yourself?
Yeah, I can ask my question.
Can people hear me?
Maybe there's some action on one side.
One second, let me just step away real quick.
Yeah, so the question I wanna know is what work has been done characterizing the parameters after pre -training finishes?
Like the reason why I'm motivating this question is it seems like we do this tremendous amount of pre -training, but it seems like we might be able to significantly reduce that if we just have smarter initialization schemes.
Yeah, you know, I've been thinking this for a long time actually also.
And they've come to conclude that I think not.
I think there is like two parts.
One is like what I like to call a hand wavy the numerics of the weights, you know, that everything is in a nice range such that it can have nice input out of functions and so on.
And that your optimizer can do steps that, you know, make reasonable change to the input output function, but not too large and so on.
I think that is part of it and that you can get through a good init or good normalizations and whatnot.
But then I also think there is, I do think that these models memorize a lot.
And then personally I believe, but I don't know of evidence or so, that these models do more kind of, you know, remembering similarity to things they've seen in training.
And then as you grow things up, they have more memory and they have seen more things.
So they should be better on more newer things because there's more similar things that they have seen.
And this, I don't think you can like just create one shot from initialization, but I don't have the immediate pointer to a paper at the top of my head now to answer your question.
Okay, thank you.
I think we also have more questions so has posted on the chat and is raising her hand.
Maybe in this order, you wanna ask your question first?
Yeah, for sure I can go ahead.
So I just have a quick clarification on this chart right here the chart number three, the bit L and bit M and bit S, are they the same model architecture, but just trained on different data sets?
So the bit S is trained on the 1 .3 million all the way to the 300 million image data set for bit L?
Yes and no, the architecture is here on the X axis.
So within one vertical slice, these are the same architecture.
And then the different points are random restarts because when you're learning, there is a lot of variance in which few examples do you see?
And then again, these next vertical slice is the same model and so on.
And as you go to the right, the model gets larger.
And so you can see that for this little data going to larger model doesn't really help you much for pre -training only for this giant data.
It means the giant data, not necessarily giant model in this case.
Right, it makes more sense, thank you.
Okay.
Do you have a question?
Oh, I see you're raising your hand as well.
You wanna go in the model?
Hey, yeah, thanks.
What is the intuition for the upstream performance in figure one spiking so suddenly at three points in training?
Here, right?
Yeah.
Yeah, yeah, I'm gonna get like around one point, like, I don't know, that just seems like an odd looking training curve.
So like, yeah, what's the intuition behind that?
Yeah, this is old school computer vision thing or old school, I mean, a few years ago.
It is when the learning rate changes.
In computer vision, it used to be very common to have the learning rate in a kind of staircase pattern.
So it's constant for a while, and then you stop, you divide the learning rate by 10 usually, boom, smaller, and then you continue.
And this gives you this huge jump.
And nowadays people don't use this much anymore.
And this work was like three years ago, I think, or two or three years ago, I don't remember.
It was very common back then.
And nowadays people use more continuously changing learning rate schedule, and then you don't really have this sudden change anymore.
But if you would overlay it, it would be like more continuously, but going roughly the same.
And then in language, I think most people or many people use just linearly the pain learning rate schedule, where also you don't see this effect because learning rate continuously decreases.
Yeah, sounds good, thanks.
And then this is what, because you asked about this dotted line, actually here, if you're like here, you could say, okay, but this is excessive, right?
Maybe it does really seem almost flat.
Maybe you could have started the decay earlier and earlier and earlier, and then you would get the same, but much quicker.
And this one shows what would happen then.
And you do learn that much worse place in the end than with the patient.
Okay, yeah, yeah, that makes sense.
Thanks.
Was there more question or I continue?
I think both of you have your answers.
Because I need to mention, I don't see you, I just see my slide.
Yeah, it's fine, we can coordinate that with this.
Hi, yeah, so I just want to make sure that I'm on the same page.
So basically what you're trying to do is multitask learning with convolutional neural network slash LSTM, right?
That's kind of like ResNet.
But you're doing multitask learning, correct?
No, where does the multitask come from?
Or where does it come from?
Because initially you showed different...
Okay.
So there is two phases.
The first one is the pre -training.
And this pre -training, I didn't mention it.
Yeah, I just said, I don't care what you're doing in the pre -training, just pre -train somehow and give me the model.
And then I test it on multiple tasks independently.
And I'm testing on multiple tasks means that I transfer it to the task, which in our case means fine tune it just on the task and then see how well it does and so on.
But it could mean other things like later we moved to just learning a linear regression on top of the embeddings for each task.
And now during the pre -training, what we do is just regular supervised learning, but just scaling everything up.
And regular supervised learning is just, well, not multitask, but multi -label in the sense that an image could have a couple of labels or not, but it usually doesn't have.
This is a minor difference.
Okay, got it.
Thanks.
Yeah, just have a quick follow up about the question rather than the discussion rather than started about this.
It's like in the hesitation or it's more memorizing the data in pre -training data set.
So I know in the language side, there's a quite interesting phenomenon that you can pre -train on a synthetic language that's, it doesn't have any semantic meaning, but they only have structural, like paired premises or things like that.
And that actually gives you almost the same boost in your downstream transfer as a normal pre -training.
So I wonder if say like, so in this means like in for language, right?
The structure seems to make a lot of contribution, which can be replaced by utilization, but I don't know if it's an image is a different case.
Maybe you have people done maybe some synthetic pre -training data set for image.
Yeah, there was a paper.
I forgot the name and the authors, but it creates completely synthetic images and like not even rendering of some realistic things, but just completely patterns, waves and shapes and so on.
And uses this for pre -training.
And then it shows that they get almost the same performance as ImageNet QuickLink.
They actually do this with vision transformers, but yeah, they never go further or it is not clear.
You know, they kind of show that you can almost get to this point here.
That is not clear how much further can you go with it?
And I think probably not much further, but it's just me guessing that not much further.
I don't have evidence for it.
So one question and then we can continue the talk.
Said that we think like a large vision models are like learning some sort of similarity, the data set they're trained on.
So do you think like they are like behaving like prototypical networks in a sense?
They're behaving like what networks?
Oh, so like prototypical networks, essentially like when you're like doing like future learning, you just say like, I'm going to learn a network and learn the metric space.
Probably not exactly, but close each.
I mean, I cannot really say because this is just some intuitive guess that I have that's what they do, but nobody really knows what the models do, right?
Yeah, I mean, we do get much work when we do something like prototypical networks for the future learning with these pre -trained models, we do get worse performance than when we do fine tuning.
So there is a bit more to it still.
However, I don't know what is this more.
Okay, thanks.
All right, let's continue.
Okay, yeah, so all right, and I didn't mention, but like on ImageNet, which is the top benchmark in computer vision, this with this work with the big transfer, we finally were able to increase the score after there was like a long period of a couple of years of no improvement, but many attachments that you see the grade outside.
This was yay, awesome.
Pre -training, scaling up everything and leveraging the data.
And then, okay, let's not care about that.
Yeah, that's okay.
This is just a little aside that if we are in the setting that I mentioned of pre -training on huge amounts of data and then testing on many other tasks, you should of course be careful that you don't have images from the other tasks in your pre -training data, right?
Otherwise you have seen them during training and then you're not really generalizing and you're just putting yourself with good scores.
And this is a real danger when we get huge amounts of data because like ImageNet images can totally be in huge amounts of data, right?
So we actually use internal pipeline that is really good at finding duplicates and also new duplicates.
Like when they are shifted, rotated, squeezed, color changed a bit, whatnot, it's still fine.
And we use this to completely remove our images from the test data sets that we test on later.
And we actually found that a lot of classic just vision data sets have clear duplicates between the training and validation set, between the training set of ImageNet and CIFAR, 10 and 100 test sets and so on.
So new duplicates are quite widespread problem in vision.
And this slide is just to say, hey, there are problems, but in all that we present, we actually took care that in the pre -training as best as we can, we don't have new duplicates.
Right, now back to being like, hey, we figured out large data, a large model, and then things get really good.
And that's how we got to transformers basically.
In computer vision, everything was convolutional networks for many years.
And basically there was nothing else still in this case.
However, in language, we saw transformation recently, right?
That everything used to be LSTM everywhere, LSTM was key.
And then came the transformer.
And in the case when there is a lot of data available, suddenly transformers work much better than LSTM.
For little data, that was still not the case exactly.
So what we then thought is that, okay, so we are now in this regime where we have tons of data and we see benefit from it.
Can we see even more benefit if we try also of the transformer architecture in vision?
And that's basically what we did.
To be fair, there were a few other attempts at trying out transformer in vision before that I don't want to detail too much here because I don't want to point fingers too much, but they were all not really using transformers for learning everything from the data.
It was always like, get something out of a resonant first, like object detection proposals or high level feature maps or things like that.
And then stick a little transformer on top.
But we wanted to go all the way, just transformer everything.
And so we came up with the simplest and more natural, I believe, way of applying transformers to vision, which is you take the image, you cut it into pieces and that's it, like a puzzle.
Like, tack tack, patches.
And that's it.
Each of these patches, you take it and you project it into your embedding space, which is the input to the transformer.
Embedded space is just abstract space of let's say 768 dimensions, for example.
How do you embed it?
You just take the pixel values and put the linear projection layer on top.
So take all the pixels, flatten the vector, matrix multiply into whatever size you want and use the same matrix for other patches.
And here we just went the simplest way ever with non -overlapping patches and everything.
You can, and people later did go on and say, hey, this is almost the convolution.
Let's make proper convolution.
Let's make stack of them, whatnot.
But this is all for web work later.
This is just the simplest way to do it first.
Then we have these embedded patches and we treat them exactly literally like the word or the tokens in language.
And then give them to exactly the bird transformer from language folks.
And just like in language, we add this class token or I think the language is like end of sentence token or something.
And we add the position embeddings to the tokens that can be learned.
And then we feed all of this to a transformer encoder, which has MLT head, which reads out this class token and then maps it to softmax layer for classification, for example.
And that's it.
That is the region transformer.
So it's literally take bird transformer, but instead of words or sentence tokens, feed in patches, transform this into tokens and that's it.
And then just same story as before, scale everything up, compute, data set, model size, patients, everything.
And see what happens.
Is this good or not?
That was the question.
And now we can see a plot here.
This is similar plot as before.
The gray area is actually what were all of the dots before.
And now the bubbles are vision transformers of different sizes.
And the bubble is kind of the size of the model.
I'll do it a bit hard to say exactly.
And what you can see first is that with little data image, net is the 1 .3 million images.
It works worse than resonance.
So if we would not believe in this idea and just try this, okay, this is a crap idea.
And 1 .3 million images is not that little.
Then the 10 times larger data sets started like in the same ballpark as a resonant.
And when we go to much larger data with a much larger transformer, then we actually start outperforming this resonant.
And we outperform it just by little.
But this resonant was really hard to get and is extremely clumsy and slow and big.
So we were very excited by this.
Then we did more controlled studies and everything.
And one of them is like using subset of the same data set.
And there is lots of curves, but basically just look at the dark gray one and the light blue one.
These are roughly similarly fast and clumsy or easy to use or difficult to use bit, which is a resonant variant and bit division transformer.
And what you can see vision transformer when we have little in quotes, little data is really bad compared to resonance.
But as we start having a lot of data actually, it starts outperforming the resonant.
And this is very promising because I think like everything that looks huge and a lot and so on now in five or 10 years, it's maybe a regular, like 10 years ago, imagine that this one seemed to be huge and massive amount of data, no not anymore.
So we should look to the future and this looks promising for the future.
Then back to the same benchmark, that was another little jump.
Yeah, we have some questions.
Yep, there is also this section about bit.
Yep, so it's in that order, if you wanna mute yourself and ask the questions.
True, yeah.
And I think Dimal already answered part of the question, but I was wondering in the input to the transformer when you're chunking up the image into little puzzle pieces and then finding them, does the order of feeding these patches in matter?
Like if you switch the order, does the prediction maybe change?
Yeah, that's a good question.
And I actually have a slide on something like this, but not exactly.
Let me jump there.
So first of all, if the order is consistent during training, right?
And you don't shuffle the order again for each new image, then it's literally the exact same.
You get the same curve, same everything because we don't encode the order anywhere.
If you start randomizing the order all the time during training, then performance gets quite a lot worse.
And let me show you why.
This is the slide was on my plan to present anyways.
Then if you ask about it, let's jump here.
These are visualization of the position embeddings.
What does it mean?
So in this case, we had 14 by 14 patches that we cut the image in.
So it means we have also 14 by 14 position embeddings.
Although we just see them as one run sequence of, what is it?
150 something or I don't know, 140 something.
And now each of these pictures shows the position embedding, which corresponds to this location.
How similar is it to all the other position embeddings?
So let's look at this one, for example.
Yellow means a perfectly similar, like exactly the same and blue means opposite in terms of percent similarity.
So this position embedding is most similar to itself, which is the pixel here.
And then the neighboring pixels is how similar is it to the position embeddings that correspond originally to the neighboring patch.
And we do see a very clear pattern that each position embedding is very similar to the embedding from its surrounding patches.
And we didn't implement any of this.
We just had this position embeddings at randomly initialized variables and they are learned as freely as the rest of the parameters of the model, but they learn to recover this notion of what are my neighbor patches, even though we don't give this information anywhere at any time, besides the raw image data and the task to please classify this image.
So that's pretty cool, I think, but it also means that if you take the train model now and give in patches in a completely differently shuffled order, it's gonna perform poorly because these position embeddings don't make sense anymore.
We did try also to implement like position embeddings, which encode the location as hard coded by us and other fancy position embeddings like relative ones.
But basically none of that really outperformed these freely learned.
And then the freely learned is simpler.
You just run them in it, let it learn as part of SGD and that's it.
And so we go with that and see just doing that.
Nice, it's awesome.
Okay, we have one more question from.
Hey, yeah, I was really good.
Yeah, this slide.
I think something that's really interesting we're talking about scaling up the data and scaling up the model as well.
But it seems like you're reaching an awesome job, right?
When you keep doing this thing.
So I'm curious if you have any thoughts on that, like is that where these points just look like that or is there kind of a best you can sort of do where when people are pre -training of like the data or the parameters, you're actually not gonna get much.
Yeah, I have another slide, but much further in the talk about this where I would like to not jump on it, if you don't mind, and then maybe in 10, 15 minutes, we will be there.
Sounds good, yes.
Yeah, but maybe to be a bit optimistic, it does seem like the transformers have a better slope here in the end and there is an explore earlier.
Sorry, Lucas, I didn't mean, did not mean to interrupt.
Are there any more questions before we proceed?
Yeah, can I ask my question real quick?
Sorry about that.
So what I'm curious to know is how does this VIT compare to if you equip a ConvNet, so for example ResNet with an attention mechanism?
Like how much of this is due to the structure of a transformer and the particular way it operates versus just the benefit of attention that a vanilla ConvNet does not have access to?
Yeah, so this has been tried many times before and the first thing that I know of was actually from, I mispronounce his name, but Jaime, the inventor of ResNet and some of his colleagues, they called it non -local networks.
It was way, I think even before the transformer paper, if I remember correctly.
And they basically inserted attention blocks at various locations in the ResNet and then they showed improvement, but it was like tiny improvements that are, it was a cool block and a simpler paper, but it was not really worth it.
And if people usually place the attention, you can imagine if you place the attention just on the pixels and don't do this patch cutting, this is way too expensive computation, right?
If you have two to four by two to four pixels, that's like, yeah, I cannot do this in my head.
I don't know, 40 ,000 or so maybe pixels attending to 40 ,000 others, that doesn't work.
So people just do it in the very high and very final layers of the ResNet, like where it's maybe seven by seven and then they add a bit of, sprinkle a bit of attention there, but then you don't really get much benefit of scaling because it's essentially still a ResNet.
And there is in ResNet, there is this block called Squeeze Excite that has been getting really, or has gotten really popular and improves the ResNet quite a bit.
And that is also kind of a form of attention, but like nicely tailored to images.
So I'm not doing it, it's arguable.
But yeah, it has been tried many times before, but it just, it doesn't show, or it hasn't been shown to have this scaling benefit as much as the video.
So I think I'm missing something critical here, which is you just said, or it's computationally difficult to do an attention layer at a low level in the ResNet.
But why is it any different than doing an attention layer in Division Transformer?
Because we cut the patches first.
So we have maybe 14 by 14 patches, which is not that much.
Okay, but I'm confused.
You could imagine not at a high level, not at a high layer in the ResNet, but at a relatively low layer, after you've applied one or two convolutional filters, convolutional layers, excuse me, then you have something like the size of the patches.
That's still 50 by 50 at the early layers.
And that's - But 50 by 50 is significantly less than, I don't know, like 400 by 400 or whatever.
But it's still 2 ,500 tokens attending, 2 ,500 tokens, which - I mean, it's a lot, but it's not comparable.
I don't know.
Okay, cool, thank you.
Yeah, I mean, it could be tried.
Okay, maybe another answer to your question is then we're slowly getting to this, my next slide after the set of questions, where we do try something almost like what you said, have a very small part of the ResNet and then stick Transformer on top of it.
But like the full Transformer encoder on top of it, and not just sprinkle a few attention layers and then continue with cons and so on.
And this is this process.
And we call them hybrid, but that's literally what you said, actually.
Like a few early layers from the ResNet and with different varying amount and then stick the whole Transformer encoder.
And this seems to work well too, especially for the, when you exact it in this case is amount of compute.
So for the little compute, it seems to work well.
But then the scaling behavior of the pure ResNet is a little better.
So we focused on that.
I think we later tried also hybrid further to the right and it was a bit lower, but it was after the paper.
So it's not on this plot, which I just put out of the paper, but you can already see the trend here.
Yeah, so if you don't scale all the way up, then this is a totally reasonable thing to do, have a little bit of ResNet and then the encoder from Transformer.
Do you wanna ask your question?
Yeah, I was just wondering about the, basically there's like a short section of paper about like fine tuning and like higher resolution.
And in that case, right, like the pre -trained like position embeddings, sorry, are like skewed, right?
And then it basically says that you guys are like interpolating.
Can you like talk a little bit, like how do you interpolate what's going on?
Yeah, actually when I checked the slides earlier today, I was like, oh, I could have a slide on that.
And we don't have a nice visualization in the paper either because it's a bit difficult to explain, but this is the best starting point we have.
So if you want to increase the resolution of the image, and you keep the patch size fixed, it means you have more patches suddenly, right?
And then as you say, the pressure embeddings, like what do you even use as position embeddings, right?
And basically you can see here that we see that they learn a very regular structure, right?
We don't really know what is the structure of this position embeddings that I learned, we just see the similarity to each other and that it is very, very regular.
And so this gave us the intuition that we may be able to just take them, kind of imagine these boxes, they slide apart and new boxes appear between them and they are just the interpolation of the surrounding ones.
And that's basically what we do with the position embeddings.
We create new ones where they are missing ones because we need more, and by interpolating the surrounding.
Or more precisely, we basically see them as a picture, in this case, 14 by 14 with 700 something channels or whatever is the dimensionality.
And then we basically resize this like you would resize a picture by linear interpolation.
And that way we get more and new position embeddings that we don't understand what they are, but they follow the same pattern as the learned ones and just as at a higher resolution basically.
Yeah, go ahead.
Yeah, I saw a quick question.
So when creating the embedding as input, right now you're doing a learning projection, at least in the difference.
Has there been work to do or is there a short talk on pixels that are close to each other?
Yeah, there were quite a few works that tried varying other things.
One that I especially liked recently, it's called early convolutions help transformers see better or something like that.
And they basically say, okay, instead of this linear projection, instead of this one big linear projection, we replace it by a stack of three by three convolution with a straight two.
And then they have also non -linearities between them, normalizations between them but such that the overall strike is the same as the patch, the dispatch if I am.
So the outcome would then be the same dimensionality as after this patch cutting and then projecting.
And then they showed that it supposedly it makes it a bit easier to optimize in the sense that more optimized settings are good settings.
In many scenarios, it performs the same, but like more robustly to get there.
And they also show some scenarios where this performs much better.
Like for example, when pre -training on, actually when they pre -train on more data, that seems to perform even better.
I have played a bit with it and tried to reproduce it.
I don't have it fully reproduced, but I don't see as much benefit as in the paper yet.
But that's not to say that the paper is wrong, just that I didn't get there yet.
That is one example of them.
There are other papers that do stuff.
But this one I found especially interesting because it's simple.
Thank you.
All right.
Continue.
We don't have more questions.
All right.
Then let's see.
Yeah, I have like three more interesting details from the paper and then depending on if we want more discussion or more content, I have more content.
Also the question about does it saturate here or not.
All right.
So another interesting thing that we had in the paper, but it is buried in the appendix.
And then follow -up papers from others have been written on this by now actually.
It's like how should we scale these transformers?
Are you right in the high level shape of the transformer?
There's lots of settings that you could choose.
And we actually tried many of them.
So we started with the like reasonable medium -sized transformer, this block in the middle.
And then we varied things one by one.
Such that we always double the compute.
So for example, this pink line, if we go to the right, this point increases the width such that we double the compute.
X -axis is compute relative to this stacking point.
And we have all of these different settings.
There's the width, which is the, how wide is, are the vectors with which self -attention is done, which is for the base model 768 and then goes larger or smaller.
There is like, as you see, scaling this does not seem promising.
So we didn't scale that much.
Then there's other things like the width of the multilayer perceptron, or some people call it the one by one convolution in these attentions.
And this seems to scale a bit nicer.
This orange part, I actually wonder where it went to the left, I don't remember.
I don't know if it's hidden somewhere or if we just didn't scale it down, but any risk.
Then another thing to scale, which does not exist in the transformers from text is the patch size.
As you make the pitch smaller, you get more and more tokens out of an image and thus more and more compute capacity.
This is the green one, which also seems to scale nicely.
Then the depth is interesting on this yellow one.
And this is the number of encoder blocks.
As we scale it first seems like, wow, this is the thing you want to scale, but then it does seem to plateau and it scales really badly if you decrease the depth.
So that's not a good thing to decrease.
However, the width seems to be a good thing to decrease if you want to go through smaller models.
And then the blue is just getting everything together such that the compute is kept, like everything by roughly the same one.
That seems to scale nicely as well as the rest.
And these are relatively simple or at least conceptually.
So we like this, so we went with that whenever we scale up or down the model.
And this I want to really like is the inference speed, because if you have the image size of two to four pixels, it actually means you have two to four by two to four pixels.
So if you have a, then you patchify it with a 16 by 16 patch, for example, patch size, then you have 14 by 14 patches.
So that is the sequence length is actually 150.
And then on top of the sequence length, you have the self -attention operation, which is square again.
So overall with the image, with respect to image size, the self -attention operation is to the fourth power, what is it called, quartic.
So that is really bad.
Like everybody who sees all of something to the fourth is like, what the hell are you doing?
This is never going to escape.
So we checked what does it look like in practice with the image sizes that we operate in.
And this is what you see here.
On the y -axis is the how fast it goes, basically how fast it does inference.
And on the x -axis is varying the input size.
And this, what this means, it doesn't look so bad yet.
Basically, when you go here to the 512, to the real large image, then you see that the transformers actually started going down a lot more than the resonance.
But in this reasonable image size, let's call it very typical, it doesn't seem so bad in practice yet.
So we're not getting hit by the big O yet.
But as we go larger, it will likely be a problem.
And there have been a lot of follow up works trying to make that better.
Right, then this is the last one from the original bit paper.
This is looking at the inputs receptive field size.
So in the self -attention operation, how far ago do heads typically attend?
And here on the x -axis, we see the layer in the network.
To the right is more towards the output, the classes, and to the left is more towards the input, the patches.
And the y -axis is how far on average across, I think, the whole validation set, does the self -attention loop.
And does loop means that the peak of the self -attention or the max, how far is it away, something like that.
And each dot is a different head because we can use multi -head self -attention, right?
And so what this shows is that in the early layers, actually you have some heads that go far, but also a lot of heads that look very nearby them, so locally.
And as we go deeper in the model, we only are left with heads that on average look further.
So it's just some kind of analysis.
There is not immediately action to take about this.
But it's interesting to see that earlier layers, they were in a mixture of looking to a local neighborhood and looking globally, and later layers only look globally anymore, right?
So that is about the original vision transformers.
Now, I don't know how long you want me to continue speaking or discussing.
I have a couple of options that I can talk about, which is one project that was further scaling up bits.
And this one also has the answer to the...
I can also jump straight to the answer if you don't want to get dressed, but to the question of like, how does it continue to the right?
Are we separating?
There is another project about how to train vision transformers when you don't have massive amounts of data, can you still do it?
Is it reasonable or is it maybe just unreasonable to do?
This one is maybe too unrelated.
Let's not talk about this.
And the last one is like, I talk all about these benefits of a really large model when you pre -train them on lots of data.
Okay, that's nice.
That's how we get a good model.
But then actually using a model that is massive is not fun at all.
Like it doesn't fit on your GPU.
You need like multiple GPUs to even use it.
So people are not happy to use it and usually still go back to smaller models, even though they know like larger model should be better.
What can we do about it?
That's another project we had, which is about distillation.
So I would say it's up to you guys what you prefer to do, or if you have plenty of questions, we can continue with the questions now, because I think now the original one hour would be over, right?
So I think one suggestion was like, we can continue the talk and we will also be recording it so people like can just like go and see it if they miss out something.
So we could do that.
Yeah, the other thing is two people have their hands raised so we can.
Questions for us.
Up to you guys and fight either way.
So you guys wanna ask your question?
Yeah, I just had a pretty basic question.
So if an object lies on the border between the patches, does that impact the model's performance in any way?
Yeah, I mean, that's not a basic question.
It's a good question.
There is a mix of answers.
So one is we didn't specifically go and test this.
It would be an interesting thing to test in a very controlled way with some of the trained models.
That's for sure.
The other thing is that when you have a massive data set, like 300 million images, it's an insane amount.
I used to try to conceptualize how much is ImageNet, one million images.
And I think I did the math is like, if you go to an image and look at all of the images, each image for a couple of seconds, you were sitting there for a month or something like that.
Don't remember.
But so 300 million is just insanely massive.
And then on top of that, we do actually use random augmentations like random crop out of the image.
So I would say it's the default that you see objects that don't fall on a patch during the training already.
And if you look at here, basically this is the standard model, like how the patches are when we have 14 by 14, they look roughly this size also.
Then an object is usually scattered across many patches actually, because objects in typical images are relatively large, people don't take a picture where the object of interest is super tiny in the corner.
So that's the default that you see during pre -training.
And so I believe that the model just learns to do that much better actually.
Then the other answer to the question is like, okay, maybe if you did some nicer thing than this very crude patch cutting, like for example, this stack of convolutions that I mentioned, maybe this is even better.
Thank you.
So you mentioned that for the new transformers, or at least you mentioned in the paper that they lack locality and like, I was just thinking, are these sort of offered me and probably, and especially when you're in the, so why is it that you would prefer that?
The audio was not that good, but I believe I understood the question is that we say that transformer lack locality bias or prior or whatever.
And why is this even something that we want?
Wouldn't we want our models to know about locality if they are both pictures in the first place?
Yes and no.
So this, that's why I gave the context in the beginning.
This is all about what happens when you scale things up.
And specifically in ideal world, at least in our mind, we want gigantic amounts of data.
And we believe that it will just keep growing as the years go by, and there will be more and more data, just generally there.
And then we want the model to have as little of our thinking built in, because what we may think that is good to solve the task may actually not be best to solve the task.
Maybe like analogy would be like, what was it?
Alpha go that made some moves that experts would say, this is crazy, this is a silly move, but it actually then was much better.
And in a similar way, we want to encode as little as possible into the model, such that if we just throw massive amounts of data and the difficult task at it, that it might make things that are even better that we didn't think of before.
This is our approach because we believe that, like, as I mentioned, I think already, what seems massive and excessive now will be the norm in five years or so.
So that's where we want to go and what's the direction.
However, if you want to like just get something working now and don't have massive amounts of data and don't want to use pre -trained model for some reason, which always is a pre -trained model.
But if you don't want to, then it makes total sense to build in some of your prior intuition and knowledge of what should probably help the model like locality.
I hope this answered your question.
This is a follow up, like what sort of point do you think that is like you can have like any vision task?
Like, is it that sort of, like, I don't know, maybe I'm not seeing like, that is why we do not want those in that hypothesis.
You may be elaborate on that.
Why is it that we don't want like locality or what translation that could bring?
Well, ideally we want the model that is powerful enough to learn about this concept itself, if it is useful to solve the task.
If it's not useful to solve the task, then if we hard code it in, there is no way for the model not to do this, right?
That is ideally the outcome.
And in a similar way also that in language, it seemed to be nonsense to not encode from left to right direction of text, like in RNNs.
But then comes transformer and just doesn't and works much better if you throw a lot of data at it.
And it recovers that plus some more or more flexible variant of it or something like that.
That is even better for solving tasks.
So basically the idea being that we are not as smart to design the thing, the model in the way that will be best for the task.
Let's rather give it all the flexibility and all the data it needs to figure out what is the best way of solving the task.
I really, this is a philosophy of approaching it.
I'm not saying that is the only true way, right?
Yeah, so we have around seven minutes left before the scheduled end of the talk.
And Lucas, we want to be mindful of your time as well, because it is evening where you are.
So one thing we could do is you could, I don't see any more questions right now.
So you could quickly sort of go over the last few bits, maybe skipping through the details and just talking about the final results.
I will do this two in the high level and those two that are still very, very tight to transformers and answer some question that happened before.
Like the first question was like, okay, are we saturating, yes or no?
And here, no.
This was the bit on this benchmark from the original transformer paper, but then it's like these transformers, when we use them, we just noticed they have really nice scaling properties and they seem actually to be easier to scale up without paying massive compute as much as ResNet, just from that feeling from us having experience with both.
And so we went and looked what happens if we scale vision transformer just as far up as we possibly can.
And we spent quite a lot of our blood into making this happen.
One part of it is scaling the dataset.
So we went back to this Google internal team that this 300 million dataset is just one out of many that they work with.
And we asked around and they basically had the 3 billion, like 10 times larger dataset that we could also like play around with.
So there we go, we want to scale up the dataset.
And this is just showing, yes, just scaling up the dataset and switching it gives you benefits, but that's not all of it.
Then the next thing is we needed to figure out how to use less memory on device, like on GPU or TPU, because already previously with this score, we fitted the model as much as we could fit.
So we did a lot of tricks that I will skip for now and are able to scale much larger.
This is like, this plot shows the size of the model in the different shape factors that I mentioned before, like the width of the MIP on X axis, the self -advention width on the Y axis, and then the different plots at different layers for the depth.
These blocks are how large the transformer we did in the original paper, and then boom, one step further and two steps further.
This is just super massive transformer we did in this scaling paper and with all of our tricks from how much larger we could go, a lot larger.
Then yeah, some learning rate stuff, and it is really cool.
I recommend people to look at square root learning rate schedule, which is cool.
And often just mentioned as a side note.
It is also cool, but I'm gonna skip it for the interest.
And basic interest of time.
And basically we scaled it up a lot.
And of course, again, we get always this in region.
ImageNet number a bit higher.
This is actually plus 2 % on what we had before, which is very significant in this high percentage range there.
But also what's very interesting is the few shot again.
By just keep scaling up everything, we get super large boost in few shot again.
This is ImageNet top one accuracy.
And for example, we just 10 images per ImageNet class, which means 10 ,000 images total because thousand classes.
We get this big of a job, we get 85 % of one accuracy, which is what you typically get when using the four data set basically.
So this is the gap makes actually a few shot work significantly better.
And then I'm gonna skip on this.
Well, this actually has an interesting message.
This is three times the same story, but measured in a slightly different way, which is that if you make the model larger, it actually needs to see fewer images to get to a similar score.
Like this blue line is a tiny vision transformer and the base vision transformer in the large one and the Y axis is the error.
So lower is better.
And actually you need to see, still we're talking in millions of images and here it's hundred million images, but still you need to see a lot fewer images with the larger models.
Does it mean a lot less compute, right?
Because the model is larger and the slower, but that's interesting.
And then there's some scaling loss that are popular in language.
And we, I think maybe for the first time in discriminative image learning show that, yeah, they appear to be here too.
And then, right, then we want to, sorry, I had the order of the slides mixed up in my head.
So I'm a bit surprised, but then another thread was that besides further scaling up the model, we wanted to push even further into this direction of less hand engineering of things into the model architecture.
And then with the vision transformer or transform in general, what is the obviously most hand engineered part of it?
It's the self -attention.
So we tried to what can we do something like more generic than that and less smart than that basically.
And we ended up by replacing it essentially with just a multi -layer perceptron.
That however has a little bit of structure, but much less than self -attention.
So there is skip the structure for the sake here of time.
And we're coming back to this plot where the question was, aren't we saturating?
Now this plot is slightly different.
We again have this bit resonant here in black and the full green line is the vision transformer.
And the other color also the full lines are the vision transformers.
It is exactly the same numbers as from before.
However, now we also throw in this mix architecture, which we believe is even more flexible and less hand engineered than the transformer.
And as you see with less data, it's even worse.
However, with much more data, it may be surpassing the transformer or it may be random loads, not clear at this point, right?
Because it's the only point where this happens.
So we need to go further.
So we use this 3 billion data set, for example, from the previous paper that I mentioned here and try to extend these lines to the right to see what happens.
We don't extend many of them because these are very expensive experiments that require a ton of patients, but we extended two most interesting and it seems that it continues.
And that first of all, yes, the vision transformer keeps increasing.
We don't have such experiment with the ResNet because it doesn't look promising enough to pay the cost of doing it.
But it also seems that the mixer, what we believe is even more flexible architecture actually is consistently above the transformer now, which is good news.
And yeah, it is good news.
So we're now right at the time when I should stop, right?
Or open to more questions again.
Yeah, I guess as a question.
Can I ask a follow up on the scaling that you showed earlier?
It's really just my previous question.
I'm curious how this model size compares to model sizes for the natural language.
Like, especially going from smaller models to much bigger models, were they comparable at all in terms of model size?
And if not, like, why do you think, what is different for the models for these three point houses?
Yeah, actually a colleague of mine has a slide which I hate, but he knows it's the model number of parameters in NLP and in vision.
And the question is, how do you measure a model size?
If you just measure a number of parameters, then these vision models are much smaller.
However, the language models, the number of parameters, like a huge chunk of it is in the dictionary, for example, which for us just doesn't exist.
It is linear embedding, which is trivial number of parameters.
So in terms of number of parameters, it's much smaller.
My personal opinion is number of parameters doesn't mean that much.
Then the other way that you could measure is maybe in terms of compute, like how much floating point operations does it do on one data point?
And in terms of this, it's in the same ballpark.
However, last time I checked, which is quite a few months ago, the largest language model was still like four times more or five times more in the vision model, I believe.
Yeah, so that's the two ways of measuring model size.
I don't think either of the ways is the one true way to measure model size.
And I think it's actually an interesting research topic, like how to properly measure and like order models in terms of capacity is not clear.
I'll open that.
Do you know why the vision is, or sorry, the vision is much smaller?
I think it's just there is less interest in it, and so less resources spent on it, basically.
Like in Google, there are many more groups doing research with language than with vision.
And I think we are one of the few groups that have access to a lot of resource and are interested in scaling up things in vision so much.
Whereas in language, it seems there are a lot of groups that are doing that.
I think that's the main reason, actually.
It's not that we don't want to go beyond that, or like if we can't, we would go a little bit more.
Awesome.
Thank you.
Right.
So we are actually over time at this point.
So anyone who has to leave, please feel free to do so.
Before we do that, Lucas, thank you so much for joining, for all the way from across the ocean.
And we know it's in the evening, so thank you for taking your free time to come and talk to us here.
Yeah, thanks for the invitation.
Always like to talk about the work.
