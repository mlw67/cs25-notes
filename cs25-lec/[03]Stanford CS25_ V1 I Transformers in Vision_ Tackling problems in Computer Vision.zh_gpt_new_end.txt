今天，我将向你们介绍视觉transformer，因为这完全是关于transformer的，特别是它们在视觉表征学习上的应用。
但在我们深入transformer之前，我会花大约10到15分钟给你们大量的背景知识，特别是关于视觉部分的，因为我认为你们看到的和将要看到的大部分将是关于语言的。
好的，那我们开始吧。
我的目标以及我密切合作者的目标是找到通用的视觉表征，你们很快就会明白这意味着什么。
为什么或我们可以做什么，如果我们想象我们有一个通用的视觉表征？
希望是通过这个，我们可以真正启动所有需要视觉输入的任务。
这意味着大多数你在睁开眼睛时做的任务，基本上，因为如果你对你所看到的有一个好的理解，那么你可以更快地理解正在发生什么以及你应该做什么。
最终，我现在有了一个小孩子，自从那年以来，所以我真的希望当他长大了，会有某种类型的机器人。
它不需要像电影中那样好看和漂亮，只是可能是一个手臂或任何东西，我的孩子可以教，或我的父母不能编程的人可以教它做一些他们真正不想做的乏味任务。
而我相信其中一个组成部分是一个良好的视觉表现，能够泛化到到处理解视觉世界。
这并不是所需的全部，但它是一部分，也是我试图推动的部分。
所以，这是关于在通用视觉表现上工作的背景和动机。
而一个通用视觉表现的好例子就是人类。
我将向你展示我所说的意思。
这里有一个我给你的任务。
有三个类别，A类，B类和C类。
我给你每个类别的五张图片，好吗？
这里我给你一张新图片。
我确信你们现在都知道它是哪个类别了。
我不会问，因为我实际上看不到你们。
如果我在房间里，我会举手。
但我确信你知道它现在是A类。
好的，这是没问题的。
希望我们一生中已经看过数百万朵花了。
但还有其他类型的图片，比如卫星图片，这在你们生活中并不常见。
有些人可能从未见过。
有时候，比如当你飞行时，或者在电视上或者在互联网上，但这相当罕见。
但故事还是一样的。
三类，A、B、C，每类五张图片。
我给你们展示一张新图片。
这可能比花稍微不那么琐碎，但我认为我已经花了足够多的时间来讲述，到现在为止，你们大多数人应该知道这是B类。显示的是篮球场，对吧？
好的，现在更抽象一些。
在现实生活中你们不会看到这个，对吧？
但我还是给你们展示A类和B类的图片。
我稍微调整了两张图片，使之变得更容易，因为你们需要多动动脑筋。
然后我给你们展示这张图片。
现在我应该稍微聪明一点，让你们思考一下。
就像你们看到的，有球体、箱子等等。
到现在为止，我希望你们大多数人知道这是A类。
为什么？
因为A类中有三个对象。
B类总是什么？
五个对象，不管它们是什么，看起来像什么。
好的，我认为到现在为止，你们或多或少明白我所说的好的视觉表示，一般的视觉表示是什么意思了，对吧？
有些东西，我不知道该怎么在你们的大脑中，你们的眼睛中称呼它，这样你们就可以很快地看到一些新的东西，并且只需要几个例子就能理解发生了什么，并从中归纳出一般性的规律。
对。
这就是目标。
然后下一步，如果你有了目标，我们如何衡量朝着目标的进展呢？
这是我们几年前与迈克尔操作员一起完成的一篇论文，你们称之为视觉任务适应基准。
这有点像我们刚刚玩的小游戏的形式化。
所以这是一个基准。
还有一些组件，你或者任何参与基准测试的人都会做的，就是用一些数据创建一个模型。
我们不关心数据是什么，模型是什么，如何创建，等等。
只要你拿出一个模型。
然后我们提出了这个所有可能的视觉任务的景观，这种景观在某种程度上是有意义的，这是一个很大的陈述。
然后我们从中抽样一些任务。
这就是你刚刚看到的任务。
它们实际上是从这个任务适应基准中取出来的。
我们已经完成了第一步，制定了19个这样的任务，我们尝试涵盖各种广泛的视觉任务，不仅仅是自然图像的类别，比如这些狗和猫之类的东西，还包括非常专业的图像，比如卫星图像，还有一些不涉及分类但涉及计数的任务，就像我之前给你展示的那个。
但是，尽管可以用这种简单的分类API来表达，但逻辑上需要更多的思考。
有些东西，比如距离，我们有一些关于汽车和最近汽车距离之类的内容。
它应该涵盖广泛的变化。
然后，对于你用来进行这项基准测试的模型，你可以对每个数据集进行一些适应性步骤，一个接一个地进行，都在同一时间，实际上并不重要。
但是，你应该得到一个这个数据集的模型作为结果，这个模型非常小。
它只是看到了每个类别的一些例子，然后在那里表现良好。
然后，我们只需取所有这些任务的平均分数。
这就是我们称之为beta任务的东西。
这就是我们目前判断你的模型和适应算法有多好的一般视觉表征的方式。
现在就是一些术语，这个准备阶段，我们经常使用的词汇是预训练。
有时我们称之为上游，比如上游数据，上游训练，诸如此类。
所以我可能会将这个词与预训练交替使用。
然后有第二部分，我们通常称之为迁移。
有时我们说下游。
而对我们的适应，原则上是随你所愿。
但对我们的工作，我们几乎总是仅仅使用非常简单的微调，不加任何花哨的东西，因为这简单而且效果好。
总的来说，我们尽量做到尽可能简单但仍然效果好。
所以有时候我甚至只是说，像微调，当微调。
这意味着从这个预训练通过迁移。
好的，到目前为止的设置。
到目前为止，还好吗？
好的。
那么问题是，我们如何达到那里？
我们花了很多时间思考这个问题并尝试不同的事情。
这也大致是我有可讲的所有内容的大纲，这并不意味着我们会涵盖一切。
所以我不会逐字逐句地讲大纲，但你会一遍又一遍地看到这个。
正如你所看到的，视觉Transformer，transformer只是稍后才出现。
在此之前还有一些内容。
关于这个，只是简单提一下，因为这对本课程来说并不重要，就是我们花了一些时间尝试自监督预训练，在语言领域非常流行，而视觉领域则是最近才变得流行。
但事实并非如此。
你不需要理解这些柱状图，但基本上高的更好。
这里，只看蓝色的那些。
这是这个few-shot VTAP的VTAP分数。
自监督学习表现得像这根柱子那样。
我们尝试了多种方法和多种模型等等。
这也是一个很好的基准，但那是几年前的事了。
然后我们转向了半监督训练。
就是一些有标签的例子和大量无标签的例子。
这是接下来的这根蓝色柱子。
你真的看到鼠标光标了吗？
抱歉。
我们看不到鼠标光标。
也许我需要做一些。
我们可以看到。
我看到了。
是的。
好的。
是的，所以半监督是那个蓝色条，比另一个蓝色条高得多。
对我们来说，这意味着通过添加一些有标签的示例，我们能够获得更好或更一般的视觉表示。
然后我不会再花更多时间在这上面，以及如何确切地等等。
但我会转到下一个，对我们来说是一种突破。
当我们发现如果我们只是进行完全监督的预训练，那么我们得到的表示比我们以前看到的任何东西都要好得多时，我们就想到了这一点。
在这里，我想简要地花一些时间在这个上面，因为它是在视觉中使用视觉或转换器的前身。
所以很简单。
互联网上有大量的图像。
这总是你听到的自我监督或无监督学习的动机，对吧？
但实际上，这些图像来自何处，几乎总是有一些额外信息，比如在网络上围绕图像，或者如果你以其他方式收集它，那里有一些额外的信息，你可以将其用作一些弱信息源或一些弱标签，对吧？
然后偶然发生的是，在谷歌中，有一个团队实际上正在为此进行生产。
他们已经收集了一个大数据集，并且有一些流水线，可以从周围的信号中自动地，但非常嘈杂地标注图像。
我们想要弄清楚，在扩大预训练时我们可以走多远。
简而言之，你需要一些要素。
其一是耐心。
我真的很喜欢这个图。
这是仅在大型数据上使用大型模型进行预训练的曲线之一。
细节并不重要。
要点是，如果我放大这个小框，我会看到这里，这是训练中的指标，就像我在计算了八个GPU周之后在上游看到的性能一样。
GPU周是什么意思？
它表示八个GPU连续工作一周，或者抱歉，一个GPU工作八周，或者八个GPU工作一周，或者16个GPU工作半周，依此类推，对吧？
但这看起来是平的。
一个理性的人会说，是的，在八个GPU上工作一周没有任何进展。
这是平的。
我要停下来尝试其他的方法。
但我们不是理性的，所以我们继续前进。
这是在经过八个GPU月的训练后，完全相同的位置是什么样子的。
你能清楚地看到事情正在进展，对吧？
所以这可能并不总是显而易见的，你需要耐心。
第二件事是，你实际上需要扩大一切。
所以这是使用 ResNet 完成的工作，还没有用到 transformers。
我看到你在这里看到了很多 ResNet 模型。
x 轴是可用图像的数量。
在视觉中，有这个图像网数据集，这是一个非常常见的、超级常见的用于预训练的数据集，其中有 130 万张图像。
还有另一个拥有 10 倍更多图像的公开数据集，然后有一个来自这个内部组的子集，拥有 3 亿标记的图像。
y 轴是在某些任务上的准确度衡量，我们尝试了许多任务，它们看起来都很相似。
而这些点除了 ResNet 之外都是不同的。
蓝色的点是每个人都使用的标准 ResNet PP。
如果你在更多的数据上训练，起初看起来很有希望，但如果你再增加更多数据，看起来就像，噢，好吧，这似乎并不那么有用。
这是大多数人长期以来一直在做的事情。
很多人，甚至在谷歌，都尝试过在这些大量数据上使用这个内部检查点。
它其实并没有那么多。
然而，我们发现，事后看来很明显，您实际上需要扩展的不仅是数据，还有模型。
在这里，这个蓝点是一个巨大的 ResNet，速度非常慢。
但是当您将其与数据一起扩展时，随着添加更多数据，您会不断获得好处。
然后，如果您做这两件事，扩展所有内容并且要有耐心，要有耐心也可以是扩展您的患者，那么您将获得很多好处。
所以在这里有一些关于迁移学习的快照，就是我之前给您展示的。
在 x 轴上是模型的大小。
在 y 轴上是其中一个测试的准确度，但其他测试看起来也很相似。
这三条不同的曲线分别代表不同的数据集大小。
绿色的那条是标准的，您不会真正看到从使用更大的模型中获得好处或者只有一点点好处。
蓝色的那条大约是 10 倍大。
您会开始看到一些向上倾斜，但只有当您使用这个巨大的数据时，您才会在这个未来的迁移学习中变得越来越好，当您在更多数据和更大模型上进行预训练时，您将获得更多和更多的好处。
我们完全没有预料到的第二个好处是，当你把所有东西都扩大时，这些模型非常稳健。
这是ObjectNet。
这是一个专门设计用来衡量稳健性的数据集，它展示了一些疯狂的东西，比如一个椅子在浴缸里之类的。
你应该把它识别为椅子。
这里的粉色点基本上是现有模型和确切的物种，同样，模型的大小和粉色点是文献中现有的。
然后这些线，相同的颜色编码是我们发现的。
同样，你可以看到这么大的数据，然后扩大到大模型只是给你带来了惊人的好处，比如在这种情况下的分布鲁棒性。
好的，这真是太棒了。
扩大一切，要有耐心。
就能获得巨大的好处。
抱歉，卢卡斯，抱歉打断你，但是班上有一个学生有问题。
是的。
好的。
你想解除静音并亲自提问吗？
是的，我可以问我的问题。
大家能听到我说话吗？
也许某一边有些动作。
等一下，让我稍微离开一下。
是的，我想知道的问题是，在预训练完成后，已经对参数进行了什么样的特征化工作？
我提出这个问题的原因是，似乎我们做了大量的预训练，但如果我们有更智能的初始化方案，似乎可以显著减少这一过程。
是的，你知道，我其实也考虑了很长时间。
他们得出的结论是，我认为不是这样。
我认为有两个方面。
一个是我喜欢称之为手摆动的权重数字，你知道，一切都在一个很好的范围内，这样它才能有很好的输入输出函数等等。
而且，你的优化器可以采取一些步骤，使输入输出函数发生合理的变化，但不会太大等等。
我认为这是其中的一部分，通过良好的初始化或良好的归一化等方法可以实现。
但我也认为，我认为这些模型记忆力很强。
个人认为，但我不知道是否有证据支持，这些模型更多地是记忆训练中看到的东西的相似之处。
然后随着它们的成长，它们的记忆力会变得更强，它们见过的东西也会更多。
所以它们在处理更新的东西时应该会更好，因为它们见过更多类似的东西。
我认为这个不能仅仅通过初始化来完成，但是我现在脑海中没有一篇论文的直接指针来回答你的问题。
好的，谢谢。
我觉得我们还有更多问题，有人在聊天中发了问题并且举手。
也许按照这个顺序，你想先问你的问题吗？
是的，我可以先说。
所以我只是想在这个图表上做一个快速澄清，就是图表三，bit L 和 bit M 以及 bit S，它们是相同的模型架构，只是在不同的数据集上训练过吗？
所以 bit S 是在 1.3 百万到 3 亿图像数据集上训练的，而 bit L 呢？
是和否，这里的架构是在 X 轴上的。
所以在一个垂直切片内，这些都是相同的架构。
然后不同的点是随机重启，因为在学习过程中，你看到的例子有很大的差异。
然后再次，下一个垂直切片是相同的模型，依此类推。
当您向右移动时，模型会变得更大。
因此，您可以看到，对于这个小数据集，扩大模型并不会在预训练方面有太大帮助，只有对于这个庞大的数据集才会有所帮助。
这意味着，在这种情况下，不一定需要庞大的模型。
是的，这更合理，谢谢。
好的。
您有问题吗？
哦，我看到您也举手了。
您想进入模型吗？
嘿，是的，谢谢。
图一中上游性能为什么在培训的三个时间点突然激增的直觉是什么？
这里，对吧？
是的。
是的，我会得到大约一个点，就像，我不知道，这看起来就像是一个奇怪的训练曲线。
所以，就像，是什么原因导致了这种情况？
是的，这是一种老派的计算机视觉技术，或者说，是几年前的一种技术。
这是当学习率发生变化时。
在计算机视觉中，以前很常见的是将学习率设置为一种阶梯状模式。
所以它会保持一段时间恒定，然后您停止，通常会将学习率除以10，降低了，然后继续。
这会给你带来巨大的进步。
而现在人们不太再使用这个了。
而这项工作是在三年前，我想是三年前，我不记得了。
那时非常普遍。
而现在人们更多地使用连续变化的学习率表，然后你就不会再有这种突然的变化了。
但是如果你叠加它，它会更连续，但大致相同。
然后在语言方面，我认为大多数人或许很多人只是线性地使用疼痛学习率表，其中你也不会看到这种效果，因为学习率会不断降低。
是的，听起来不错，谢谢。
然后这是为什么，因为你问起这条虚线，实际上在这里，如果你在这里，你可以说，好的，但这是过度的，对吧？
也许看起来几乎是平的。
也许你本来可以更早地开始衰变，然后越来越早地开始，然后你会得到相同的结果，但会更快。
这个图显示了接下来会发生什么。
而你最终学到的地方比忍耐者那里要糟糕得多。
好的，是的，那很有道理。
谢谢。
还有其他问题吗，还是我继续？
我觉得你们两个都有答案了。
因为我需要提一下，我看不到你们，我只看到我的幻灯片。
好的，没问题，我们可以协调一下。
嗨，是的，我只是想确认一下我理解的是否正确。
所以你们正在尝试用卷积神经网络和LSTM进行多任务学习，对吗？
这有点像ResNet。
但你们是在做多任务学习，对吗？
不是的，多任务学习是从哪里来的？
或者说它从哪里来的？
因为最初你展示了不同的...
好的。
所以有两个阶段。
第一个是预训练。
在这个预训练阶段，我没有提到它。
是的，我只是说，我不在乎你在预训练阶段做什么，只要以某种方式进行预训练并给我模型。
然后我在多个任务上独立测试它。
我在多个任务上进行测试意味着我将其转移到任务上，这在我们的情况下意味着仅在任务上微调它，然后看它表现如何，等等。
但这可能意味着其他事情，比如后来我们只是在每个任务的嵌入上学习一个线性回归。
而现在在预训练中，我们所做的就是常规的监督学习，但只是放大了一切。
常规的监督学习就是，嗯，不是多任务，而是多标签的意义上，即一幅图像可能有几个标签或没有，但通常不会有。
这是一个小差别。
好的，明白了。
谢谢。
是的，只是关于这个问题而不是讨论的快速后续。
就像在犹豫中，或者更多的是在预训练数据集中记住数据。
所以我知道在语言方面，有一个相当有趣的现象，你可以在一个合成语言上进行预训练，它没有任何语义意义，但它们只有结构性，比如配对的前提或类似的东西。
而这实际上给你在下游转移中几乎相同的提升，就像正常的预训练一样。
所以我想知道，比如，在语言中，对吧？
结构似乎做出了很大的贡献，可以被利用替代，但我不知道在图像中是否是不同的情况。
也许你们有人做过一些用于图像的合成预训练数据集。
是的，有一篇论文。
我忘了名字和作者，但它创造了完全合成的图像，就像不是对一些现实事物的渲染，而是完全的图案、波浪和形状等。
并使用这个进行预训练。
然后它显示，他们获得的性能几乎与ImageNet QuickLink相同。
他们实际上是用视觉transformer做这个的，但是，他们从未进一步探索，或者不清楚。
你知道，他们有点展示你几乎可以达到这里的这个点。
但不清楚你能进一步达到多远？
我认为可能不会更进一步，但这只是我猜的不会更进一步。
我没有证据证明。
所以有一个问题，然后我们可以继续讲话。
说我们认为大型视觉模型就像在学习它们训练所用数据集的某种相似性。
那么你认为它们就像在表现得像原型网络一样吗？
它们表现得像什么网络？
哦，就像原型网络一样，基本上就像当你在做未来学习时，你只是说，我要学习一个网络并学习度量空间。
可能不是完全一样，但很接近。
我是说，我不能真正确定，因为这只是我有的一些直觉猜测，但没有人真正知道模型在做什么，对吧？
是的，我是说，当我们像原型网络这样为未来学习使用这些预训练模型时，我们的工作效果确实会变差，比起微调时会差一些。
所以还有一些更多的内容。
但是，我不知道这更多的是什么。
好的，谢谢。
好的，让我们继续。
好的，是的，所以好的，我没有提到，但是在ImageNet上，这是计算机视觉中的顶级基准，在这项工作中，通过这种大规模迁移，我们终于能够提高分数，之前有好几年没有改进，但是很多附件你看到了外面的分数。
这真是太棒了。
预训练，扩展所有内容并利用数据。
然后，好的，让我们不去关心那个。
是的，没问题。
这只是一个小插曲，如果我们处于我提到的预训练大量数据然后在许多其他任务上进行测试的环境中，你当然要小心，不要在预训练数据中有来自其他任务的图像，对吧？
否则，在训练过程中你已经看到了它们，那么你就不是在真正泛化，而是只是让自己得到了好的分数。
当我们获取大量数据时，这是一个真正的危险，因为像ImageNet图像完全可以在大量数据中找到，对吧？
因此，我们实际上使用内部流水线来查找重复项和新的重复项。
比如当它们被移动、旋转、挤压、颜色稍微改变等，这都没关系。
我们利用这一点完全从我们以后测试的测试数据集中移除我们的图像。
实际上，我们发现很多经典的视觉数据集在训练集和验证集之间，ImageNet和CIFAR的训练集之间，10和100的测试集之间都存在明显的重复项。
因此，新的重复项在视觉领域相当普遍。
而这张幻灯片只是想说，嘿，有问题，但在我们呈现的所有内容中，我们实际上尽可能地在预训练中处理，我们没有新的重复。
好的，现在回到像，嘿，我们弄清楚了大量数据，一个大模型，然后事情变得非常好。
这就是我们基本上到达transformers的方式。
在计算机视觉中，多年来一切都是卷积网络。
基本上，在这种情况下还没有别的。
然而，在语言方面，最近我们看到了转变，对吧？
以前一切都是LSTM，LSTM是关键。
然后出现了transformer。
在有大量数据可用的情况下，突然transformers的工作效果比LSTM好得多。
对于少量数据来说，情况仍然不完全一样。
所以我们接下来想的是，好吧，我们现在处于这样一个阶段，我们有大量数据，并且我们从中看到了好处。
如果我们在视觉中也尝试transformer架构，我们能否看到更多好处？
这基本上就是我们所做的事情。
公平地说，以前在视觉中尝试Transformer的确还有一些其他尝试，我不想在这里详细介绍，因为我不想过多指责，但它们都不是真正使用Transformer从数据中学习。
总是像从共鸣中获取一些东西，比如目标检测提议或高级特征映射之类的。
然后在上面添加一个小的Transformer。
但是我们想要一直走下去，就是把一切都用Transformer。
所以我们提出了将Transformer应用于视觉的最简单、最自然的方式，我认为，那就是你拿图像，把它切成小块，就像拼图一样。
像，啪啪，小块。
就是这样。
每个这些小块，你拿出来并将其投射到你的嵌入空间中，这是Transformer的输入。
嵌入空间只是一个抽象空间，比如768维。
你如何嵌入它？
你只需取像素值并在其上放置线性投影层。
所以取出所有像素，展平向量，矩阵乘以你想要的任何大小，并且对其他小块使用相同的矩阵。
然后我们就以最简单的方式进行，使用非重叠的块，等等。
你可以，而后人们确实说，嘿，这几乎就是卷积。
让我们进行适当的卷积。
让我们堆叠它们，等等。
但这一切都是为以后的网络工作做的。
这只是首先执行它的最简单的方法。
然后我们有这些嵌入式块，我们对待它们就像对待语言中的单词或标记一样。
然后将它们准确地传递给语言专家的Bird Transformer。
就像在语言中一样，我们添加了这个类令牌，或者我认为语言中是类似于句子结束标记的东西。
然后，我们为可以学习的标记添加位置嵌入。
然后，我们将所有这些馈送到一个Transformer编码器，它具有MLT头，它读取这个类标记，然后将其映射到用于分类的Softmax层，例如。
就是这样。
这就是Region Transformer。
所以它就是采用Bird Transformer，但不是单词或句子标记，而是馈送块，将其转换为标记，就是这样。
然后，就像之前一样，将一切都扩大，计算数据集、模型大小、患者等等。
看看会发生什么。
这个好还是不好？
这就是问题所在。
现在我们可以看到一个图表。
这是之前的类似图表。
灰色区域实际上是之前所有点的位置。
现在，气泡是不同大小的视觉transformer。
气泡的大小有点像模型的大小。
确切地说有点难以确定。
首先你可以看到，对于少量数据的图像，网络是130万张图像。
它的效果比共振要差。
所以如果我们不相信这个想法，只是尝试一下，好的，这是一个糟糕的想法。
而130万张图像并不算少。
然后10倍大的数据集开始，与共振大致相同。
当我们使用更大的数据集和更大的transformer时，我们开始超越这个共振。
我们仅仅是略微领先。
但这个共振真的很难得到，而且非常笨重、缓慢和庞大。
所以我们对此感到非常兴奋。
然后我们进行了更多的受控研究和其他一些。
其中一个是使用同一数据集的子集。
有很多曲线，但基本上只看深灰色和浅蓝色的那个。
这些大致上是同样快速和笨拙或易于使用或难以使用的比特，这是一个共振变体和比特分割transformer。
当我们有少量数据时，可以看到视觉transformer实际上非常糟糕，与共振相比。
但是当我们开始有大量数据时，实际上它开始胜过共振。
这是非常有希望的，因为我认为现在看起来巨大的一切，在五到十年内，也许会成为常规，就像十年前，想象这个似乎是巨大的和大量的数据，不再是了。
所以我们应该展望未来，这对未来看起来很有希望。
然后回到同样的基准，又有了另一个小跃迁。
是的，我们有一些问题。
是的，关于比特也有这部分内容。
是的，所以如果你想要静音并提问，请按顺序。
对，没错。
我觉得 Dimal 已经回答了部分问题，但我想知道，在将图像分成小拼图块并找到它们的输入时，这些块的顺序是否重要？
如果更改顺序，预测会发生变化吗？
是的，这是一个很好的问题。
我确实有一张幻灯片是关于这个话题的，但不完全相同。
让我跳到那里。
首先，如果在训练期间顺序一致，对吧？
并且不会为每个新图像重新洗牌顺序，那么结果完全相同。
你会得到相同的曲线，完全相同，因为我们没有在任何地方编码顺序。
如果你开始在训练期间随机化顺序，那么性能会大大下降。
让我告诉你为什么。
这是我计划要呈现的幻灯片之一。
如果你问起，我们就跳到这里。
这些是位置嵌入的可视化。
这意味着什么？
在这种情况下，我们将图像分成了 14x14 的小块。
这意味着我们也有 14x14 的位置嵌入。
虽然我们只是将它们视为一个连续的运行序列，它是什么？
150多还是我不知道，140多。
现在，每个图片都显示了对应于这个位置的位置嵌入。
它与所有其他位置嵌入有多相似？
所以，让我们以这个为例。
黄色表示完全相似，就像完全相同，蓝色表示相反的百分比相似度。
因此，这个位置嵌入与自身最相似，也就是这里的像素。
然后，相邻像素是它与最初对应于相邻补丁的位置嵌入有多相似。
我们确实看到一个非常明显的模式，即每个位置嵌入与其周围补丁的嵌入非常相似。
我们没有实现任何这些。
我们只是随机初始化了这些位置嵌入变量，并且它们像模型的其余参数一样自由地学习，但是它们学会了恢复这个概念，即我的邻居补丁是什么，尽管我们在任何时候除了原始图像数据和分类此图像的任务外，都没有提供这些信息。
这真的很酷，我认为，但这也意味着如果你现在拿取训练模型并以完全不同的洗牌顺序提供补丁，它会表现不佳，因为这些位置嵌入不再有意义。
我们也尝试实现像位置嵌入这样的东西，它们将位置硬编码为我们设定的位置以及其他花式的位置嵌入。
但基本上所有这些都没有真正超越这些自由学习的方法。
而自由学习的方法更简单。
你只需运行它们，让它在SGD的一部分中学习，就这样。
所以我们选择了这个，并看看它是如何运行的。
很好，真棒。
好的，我们还有一个问题。
嘿，是的，我觉得非常好。
是的，这张幻灯片。
我认为有趣的是，我们正在谈论扩大数据规模和扩大模型规模。
但似乎你们正在做一项了不起的工作，对吧？
当你不断地进行这种事情时。
所以我很好奇你对此是否有任何想法，就像这些点是否只是看起来像那样，还是在进行预训练时，无论是数据还是参数，你实际上不会有太多提升。
是的，我有另一张幻灯片，但是谈论这个的内容要更深入一些，我想在这之前不要介入，如果你不介意的话，也许再过10、15分钟，我们就会到那里了。
听起来不错，是的。
是的，但也许要稍微乐观一点，看起来transformer在最后有更好的斜率，而且在早期也有一些探索。
抱歉，卢卡斯，我不是故意打断的。
在我们继续之前还有其他问题吗？
是的，我能快速问一个问题吗？
对此感到抱歉。
我好奇的是，VIT与如果您给ConvNet装备了注意力机制有何区别，例如ResNet？
就像这到底有多少是由于transformer的结构以及它操作的特定方式，而不仅仅是注意力的好处，普通的ConvNet无法访问到？
是的，这已经尝试过很多次了，我所知道的第一件事实际上是来自于，我发音错误，但是ResNet的发明者之一杰米和他的一些同事，他们称之为非局部网络。
这是在transformer论文之前很久，如果我记得正确的话。
他们基本上在ResNet的各个位置插入了注意力块，然后他们显示出改进，但这些改进很小，这是一个很酷的块和一篇更简单的论文，但真的不值得。
如果人们通常将注意力放在像素上，而不进行这种补丁切割，这会导致计算量太大，对吧？
如果你有两到四个像素，那就是，是的，我无法在脑海中完成这个计算。
我不知道，大概是40,000个像素参与到其他40,000个像素，这是不可行的。
所以人们只是在ResNet的最高和最后几层中执行，比如可能是七乘七，然后他们在那里添加了一点点注意力，但是你真的不会因为它本质上仍然是ResNet而获得太多好处。
在ResNet中，有一个被称为Squeeze Excite的块变得非常流行，并且大大改进了ResNet。
这也是一种形式的注意力，但是很好地定制给了图像。
所以我不这样做，这是值得商榷的。
但是，它以前已经尝试过很多次，但是，它并没有显示出或者说它还没有显示出与视频中所示的那样具有这种规模化的好处。
所以我认为我在这里可能缺少了一些关键的东西，就是你刚才说过，或者说在ResNet的低层进行注意力层是计算上困难的。
但是，这与在Division Transformer中进行注意力层有什么不同呢？
因为我们首先对图像进行了划分。
所以我们可能有14乘以14的图像块，这并不多。
好的，但是我感到困惑。
你可以想象不是在ResNet的高层，而是在相对较低的层次上，当你应用了一个或两个卷积滤波器、卷积层之后，然后你就有了类似于图像块大小的东西。
那在早期的层次上仍然是50乘以50。
但是 - 但是50乘以50比，我不知道，400乘以400或者其他什么的要少得多。
但是它仍然有2,500个令牌在参与，2,500个令牌，这个 - 我是说，这很多，但是不可比。
我不知道。
好的，很好，谢谢你。
是的，我是说，可以尝试一下。
好的，也许对你的问题的另一个回答是，我们正在逐渐接近这个问题，在我的下一个幻灯片中，在一系列问题之后，我们尝试了与你所说的几乎相似的东西，将ResNet的一个非常小的部分与Transformer结合起来。
但是像完整的Transformer编码器放在上面，而不仅仅是撒上几个注意力层，然后继续使用cons等。
这就是这个过程。
我们称之为混合，但这实际上就是你说的。
从ResNet中取出几个早期层，然后以不同的数量粘贴整个Transformer编码器。
这似乎也很有效，尤其是在这种情况下的计算量。
因此，对于小型计算来说，这似乎效果很好。
但是纯ResNet的扩展行为稍微好一些。
所以我们把重点放在了这一点上。
我认为我们后来也尝试了更进一步向右的混合方法，结果有点低，但这是在论文之后。
所以它没有在这张图上，我只是把它放在了论文之外，但你已经可以看到这里的趋势了。
是的，所以如果你不是完全扩展，那么这是完全合理的做法，有一点点的ResNet然后用Transformer的编码器。
你想问什么问题吗？
是的，我只是想问一下，关于，基本上有一小部分论文关于微调和更高分辨率的论文。
在那种情况下，对吧，像预先训练的位置嵌入，对不起，是有偏的，对吧？
然后它基本上说你们像插值。
你能不能像说一点点，你们是怎么插值的？
是的，实际上，今天早些时候我检查幻灯片时，我想，哦，我可以在那上面加一张幻灯片。
并且我们在论文中也没有一个很好的可视化，因为解释起来有点困难，但这是我们最好的起点。
所以如果你想增加图像的分辨率，并且你保持补丁大小固定，这意味着你突然有了更多的补丁，对吧？
然后正如你所说，压力嵌入，你甚至怎么作为位置嵌入？
基本上你可以在这里看到我们看到他们学习了一个非常规则的结构，对吧？
我们真的不知道这些位置嵌入的结构是什么，我们只是看到它们彼此之间的相似性以及它们是非常，非常规则的。
这样我们就有了这样的直觉，也许我们能够只是拿走它们，想象这些盒子，它们滑开，新的盒子出现在它们之间，它们只是周围盒子的插值。
这基本上就是我们对位置嵌入所做的。
我们在缺少的地方创建新的位置嵌入，因为我们需要更多，通过插值周围的位置。
更准确地说，我们基本上将它们看作是一幅图像，在这种情况下，是14乘14，有700多个通道或者是维度。
然后我们基本上通过线性插值来调整大小，就像你调整一幅图片一样。
这样我们就得到了更多和新的位置嵌入，我们不理解它们是什么，但它们遵循与学习到的位置嵌入相同的模式，并且基本上是以更高的分辨率。
是的，继续。
是的，我看到一个快速的问题。
所以在创建嵌入作为输入时，现在你正在进行学习投影，至少在差异方面。
是否有研究或者有关于彼此接近的像素的简短讨论？
是的，有很多尝试改变其他事物的工作。 
最近我特别喜欢的一个，它叫做早期卷积有助于改善transformers的视觉或类似的东西。
他们基本上说，好吧，不要使用这个线性投影，不要使用这个大的线性投影，我们用一个三乘三的卷积堆栈替换它，带有直通的二。
然后它们之间还有非线性，它们之间有归一化，但总的来说，与patch相同，dispatch如果我在的话，这总体的打击是相同的。
所以结果应该与此patch切割然后投影后的维度相同。
然后他们展示说，据说这样做会使优化变得更容易，更优化的设置是好的设置。
在许多情况下，它的性能相同，但更加稳健。
他们还展示了一些情景，在这些情景中，它的性能要好得多。
比如，当预训练时，实际上当他们在更多数据上预训练时，似乎表现得更好。
我已经玩弄了一下，并试图复现它。
我没有完全复制它，但我还没有在论文中看到的那么多好处。
但这并不是说这篇论文是错误的，只是我还没有达到那个水平。
这就是其中的一个例子。
还有其他的论文在做一些事情。
但我觉得这篇特别有趣，因为它很简单。
谢谢。
好的。
继续。
我们没有更多的问题。
好的。
那么让我们看看。
是的，我还有三个论文中更有趣的细节，然后根据我们是否想要更多的讨论或更多的内容，我有更多的内容。
还有一个关于它是否在这里饱和的问题。
好的。
所以我们在论文中有另一个有趣的事情，但它被埋在附录中。
之后其他人的跟进论文现在已经写出来了。
就像我们应该如何扩展这些transformers？
你是否对transformer的整体形状感到满意？
有很多设置可以选择。
我们实际上尝试了许多。
所以我们从像合理大小的transformer开始，这是中间的模块。
然后我们逐一地变化事物。
这样我们就总是使计算量翻倍。
例如，这条粉色的线，如果我们向右走，这一点会增加宽度，使计算量翻倍。
X轴是相对于这个堆叠点计算的。
而我们有所有这些不同的设置。
有宽度，即用于自注意力的向量有多宽，对于基础模型是768，然后会变得更大或更小。
就像你看到的，调整这个似乎并不太有前途。
所以我们没有调整那么多。
然后还有其他一些东西，比如多层感知机的宽度，或者有些人称之为这些注意力中的一对一卷积。
这似乎调整得更好一些。
这个橙色部分，我实际上想知道它去了哪里，我不记得了。
我不知道它是隐藏在某个地方，还是我们只是没有缩小它，但是任何风险。
然后还有另一件需要调整的事物，在文本变换中不存在，那就是补丁的大小。
当你把补丁做得更小，你就会从图像中得到更多的令牌，从而获得更多的计算能力。
这是绿色的部分，似乎也调整得很好。
然后深度对这个黄色的部分很有趣。
这是编码器块的数量。
当我们首次扩展时，似乎哇，这就是你想要扩展的东西，但是当你减少深度时，它似乎会达到一个瓶颈，扩展效果非常差。
所以减少深度不是一个好主意。
然而，如果你想要通过更小的模型，减少宽度似乎是个好办法。
然后蓝色就是把所有东西都整合在一起，使得计算量大致相同。
这种方式似乎也能很好地扩展，就像其他部分一样。
而且这些相对来说都比较简单，或者至少在概念上是这样。
所以我们喜欢这样，所以我们在扩展模型时无论是放大还是缩小都采用了这种方式。
我真的很喜欢的另一点是推理速度，因为如果你有两到四个像素的图像大小，实际上意味着你有两到四乘以两到四个像素。
所以如果你有一个图像，然后你用一个 16×16 的补丁，例如，补丁大小，那么你就会有 14×14 个补丁。
所以序列长度实际上是 150。
然后在序列长度之上，你有自注意力操作，这又是一个平方。
所以总的来说，对于图像大小，自注意力操作是四次方，叫什么来着，四次的。
那真的很糟糕。
就像每个看到四次方的人一样，你到底在干什么？
这永远不会消失。
所以我们检查了在我们操作的图像大小下它是什么样子。
这就是你在这里看到的。
在y轴上是它的速度，基本上是它的推理速度。
在x轴上是变化的输入大小。
而这，这意味着它看起来还不算太糟。
基本上，当你到达512，到达真正大的图像时，你会看到transformer实际上下降得比共振多得多。
但在这个合理的图像大小中，让我们称之为非常典型的大小，实际上看起来还不算太糟。
所以我们还没有受到大O的打击。
但随着我们变得更大，这可能会成为一个问题。
已经有很多后续的工作试图改进这一点。
对，这是原始位纸上的最后一个。
这是查看输入的感受野大小。
所以在自注意操作中，头通常关注多远的距离？
在这里，我们在网络中看到层。
向右更倾向于输出，即类别，而向左更倾向于输入，即补丁。
而 y 轴表示的是自注意力循环在整个验证集上的平均距离，我想是这样。
而循环意味着自注意力的峰值，或者说最大值，它有多远，类似这样。
每个点代表一个不同的头部，因为我们可以使用多头自注意力，对吧？
所以，这显示了在早期层中，实际上有一些头部的范围很大，但也有很多头部看起来非常接近它们，所以是局部的。
随着模型的深入，我们只剩下那些平均看得更远的头部。
所以这只是某种分析。
目前没有立即采取的行动。
但有趣的是看到，在早期层，它们是在局部和全局之间进行混合观察的，而后来的层只关注全局，对吧？
所以这就是原始的视觉transformer。
现在，我不知道你想让我继续讲话或讨论多久。
我有几个选项可以谈论，其中一个是进一步扩展比特的一个项目。
而且这个也有答案是关于...
如果你不想穿衣服，我也可以直接跳到答案，但问题是，它如何继续向右边发展？
我们是在分离吗？
还有另一个项目是关于如何在没有大量数据的情况下训练视觉转换器，你仍然能做到吗？
这是合理的吗，还是说可能是不合理的？
这个也许太不相关了。
我们不谈论这个。
最后一个是，我详细讲解了在大量数据上预训练这些好处的项目。
好的，很好。
这就是我们获得一个好模型的方式。
但是实际上使用一个庞大的模型一点也不有趣。
就像它不能适应你的GPU。
你甚至需要多个GPU才能使用它。
所以人们不愿意使用它，通常还是回到较小的模型，即使他们知道较大的模型应该更好。
我们能做些什么呢？
这是我们另一个项目，关于蒸馏。
所以我会说这取决于你们喜欢做什么，或者如果你们有很多问题，我们现在可以继续问问题，因为我想现在原来的一个小时应该已经结束了，对吧？
所以我认为有一个建议是，我们可以继续谈话，我们也会记录下来，这样人们就可以随时去看，如果他们错过了什么。
所以我们可以这样做。
是的，另一件事是有两个人举手了，所以我们可以。
对我们的问题。
你们两个随便怎么样都可以。
所以你们想问你们的问题吗？
是的，我只是有一个非常基本的问题。
所以如果一个物体位于补丁之间的边界上，这会对模型的性能有任何影响吗？
是的，我的意思是，这不是一个基本的问题。
这是一个好问题。
有各种回答。
所以一个是我们没有专门去测试这个。
这将是一个有趣的事情，以一种非常受控制的方式来测试一些经过训练的模型。
那是肯定的。
另一件事是，当你有一个庞大的数据集，比如三亿张图片，这是一个疯狂的数量。
我曾经试图概念化ImageNet有多少，一百万张图片。
我认为我做的数学是这样的，如果你打开一张图并查看所有的图像，每个图像只看几秒钟，你就得坐上一个月左右。
记不清了。
但是3亿张图简直太庞大了。
而且除此之外，我们实际上还使用随机增强，比如从图像中随机裁剪出一部分。
所以我会说，在训练过程中，你看到的对象不落在一个图像块上是很正常的。
如果你看这里，基本上这是标准模型，就是我们有14乘14的图像块时，它们的大小大致是这样的。
然后一个对象通常会分布在许多图像块上，因为典型图像中的对象相对较大，人们不会拍一张图，其中感兴趣的对象在角落里非常微小。
所以这是在预训练期间你看到的正常情况。
而且我相信模型学会了更好地做到这一点。
然后对这个问题的另一个回答是，好吧，也许如果你做一些比这个非常粗糙的图像块切割更好的事情，比如我提到的这个卷积堆叠，也许效果更好。
谢谢。
所以你提到对于新的transformers，或者至少你在论文中提到它们缺乏局部性和类似的东西，我在想，这些是否对我提供了可能，尤其是当你在，那么为什么你会更喜欢那个呢？
音频不是很好，但我相信我理解了问题，就是我们说transformer缺乏局部偏差或先验或其他什么。
而且为什么这是我们想要的东西呢？
如果我们的模型本质上都是处理图片的，难道我们不希望它们了解局部性吗？
是的也好，不是也好。
所以，这就是为什么我在开头给出了背景。
这一切都与当你扩大规模时会发生什么有关。
尤其是在理想的世界中，至少在我们的想象中，我们希望有巨大量的数据。
我们相信随着时间的推移，数据量会不断增长，那里会有越来越多的数据。
然后我们希望模型尽可能少地内建我们的思维，因为我们可能认为对解决任务有益的东西实际上可能不是解决任务的最佳方法。
也许类比可以是什么来着？
AlphaGo做出了一些专家会说，这太疯狂了，这是一个愚蠢的举动的走法，但事实上，它实际上要好得多。
同样地，我们希望尽可能少地编码到模型中，这样，如果我们只是向其提供大量的数据和困难的任务，它可能会产生我们之前没有想到的更好的东西。
这是我们的方法，因为我们相信，正如我之前提到的，我认为，现在看起来庞大和过度的东西将在五年左右成为常态。
这就是我们想要去的方向。
但是，如果你只是想现在让某些东西运行起来，并且没有大量的数据，也不想出于某种原因使用预训练模型，这总是一个预训练模型。
但是如果你不想，那么构建一些你先前的直觉和知识，像局部性这样的东西，对模型可能有帮助，这完全是有道理的。
我希望这回答了你的问题。
这是一个后续问题，就像你认为这是一个你可以有任何视觉任务的点吗？
就像，是那种，我不知道，也许我没有看到，那就是我们为什么不想要那些在那个假设中的原因。
你可以详细解释一下。
为什么我们不希望像位置相关性或可能带来的翻译这样的东西呢？
嗯，理想情况下，我们希望模型足够强大，能够自己学习这个概念，如果这对解决任务有用的话。
如果对解决任务没有用处，那么如果我们硬编码进去，模型就无法避免这样做，对吧？
那理想情况下就是这个结果。
同样的方式，在语言上，好像不按文本左到右的顺序进行编码就不合理，就像在 RNNs 中一样。
但是然后出现了 Transformer，它并没有这样做，如果你向它输入大量数据，它的效果要好得多。
它恢复了那种方式，甚至是一种更加灵活的变体，或者类似的东西。
这对解决任务来说甚至更好。
所以基本上的想法是，我们并不像设计这个东西、模型的方式那样聪明，以至于最适合这个任务。
让我们宁愿给它所有的灵活性和所有它需要的数据，来找出什么是解决任务最好的方式。
我真的，这是一种处理它的哲学。
我不是说这是唯一正确的方式，对吧？
是的，所以我们还有大约七分钟左右，就到了讲话预定结束的时间。
Lucas，我们也要注意你的时间，因为你所在的地方已经是晚上了。
所以我们可以这样做，你可以，我现在没有看到更多的问题。
你可以快速地浏览一下最后几个部分，也许可以跳过细节，只谈论最终的结果。
我会以高层次的方式讲这两个，并且还有两个与transformers紧密相关的问题，回答一些之前发生的问题。
比如第一个问题是，好的，我们是否达到了饱和状态，是或否？
而在这里，是否定的。
这是原始transformer论文中的一部分，但是当我们使用这些transformers时，我们注意到它们具有非常好的扩展特性，实际上似乎更容易进行规模化，而不像ResNet那样需要大量的计算资源，这是我们通过与两者都有经验的感觉而得出的结论。
所以我们看了一下，如果我们将vision transformer扩展到尽可能大的程度会发生什么。
我们付出了相当多的努力来实现这一点。
其中一部分是扩展数据集。
所以我们回到了这个谷歌内部团队，这个3亿数据集只是他们所使用的众多数据集中的一个。
我们询问了一些人，他们基本上有 30 亿的数据集，是我们也可以玩弄的 10 倍大。
所以，我们要扩大数据集。
这只是在展示，是的，仅仅扩大数据集并切换会给你带来好处，但这还不是全部。
然后，接下来的事情是我们需要想办法在设备上使用更少的内存，比如 GPU 或 TPU，因为在之前已经用这个得分，我们尽可能地适配了模型。
所以，我们做了很多技巧，我现在会跳过，能够扩展得更大。
这就像，这个图显示了我之前提到的不同形状因素下模型的大小，比如在 X 轴上的 MIP 的宽度，Y 轴上的自我调整宽度，然后在深度的不同层次上的不同绘图。
这些块是我们在原始论文中所做的 transformer 有多大，然后嘭，又进了一步，又进了两步。
这只是我们在这篇扩展论文中做的超级大 transformer，加上我们所有的技巧，我们能做得更大。
然后是一些学习率的东西，而且真的很酷。
我建议大家看一下平方根学习率表，这很酷。
经常只是作为一个旁注提到。
这也很酷，但出于兴趣我会跳过它。
基本上是时间的基本利益。
基本上我们将其扩展了很多。
当然，我们总是在这个区域内。
ImageNet的数字有点高。
这实际上比我们以前的数字高出了2%，在这个高百分比范围内非常重要。
但同样有趣的是这个few shot再次出现。
通过简单地扩展所有内容，我们再次在few shot上获得了极大的提升。
这是ImageNet的前一准确率。
例如，我们只有每个ImageNet类别10张图像，这意味着总共有10,000张图像，因为有一千个类别。
我们得到了这么大的一个提升，我们的准确率达到了85%，这基本上是使用四个数据集时通常能得到的。
所以这个差距实际上使few shot的效果显著提高了。
然后我要跳过这个。
嗯，这实际上有一个有趣的信息。
这是三次相同的故事，但以稍微不同的方式衡量，即如果使模型变大，它实际上需要看到更少的图像才能达到类似的分数。
像这条蓝线是一个微小的视觉Transformer，而大型的则是基础视觉Transformer，Y轴是误差。
所以，越低越好。
而且你需要看到，我们仍然在谈论数百万张图像，而这里是亿级图像，但仍然需要看到更少的图像来训练更大的模型。
这是否意味着计算量大大减少了呢？
因为模型更大更慢，但这很有趣。
然后还有一些在语言领域流行的缩放损失。
我们，我认为可能是第一次在图像识别学习中表明，是的，它们在这里也存在。
然后，对，然后我们想要，抱歉，我把幻灯片的顺序搞混了。
所以我有点惊讶，但另一个线索是，除了进一步扩大模型规模，我们还想进一步推进这种少手工工程的方向，将其融入模型架构中。
而对于视觉Transformer或转换而言，最明显的手工工程部分是什么？
就是自注意力机制。
所以我们尝试着能否做一些比那更通用、更笨拙的事情。
然后我们基本上用一个多层感知器来替换它。
但是它的结构要简单一些，远比自注意力小。
所以这里为了时间考虑，跳过了结构。
现在我们回到了这个情节，问题是，我们是否饱和了？
现在这个情节略有不同。
我们再次在这里有这一小段共振的黑色，整条绿线是视觉transformer。
其他颜色也是整条线是视觉transformer。
这些数字与之前完全相同。
然而，现在我们还加入了这种混合架构，我们认为它甚至比transformer更灵活，更少需要手工调整。
正如你看到的，使用更少的数据，情况变得更糟。
然而，使用更多的数据，它可能会超越transformer，或者可能是随机负载，目前还不清楚，对吧？
因为这是唯一发生这种情况的点。
所以我们需要进一步研究。
因此，我们使用了先前提到的来自前一篇论文的 30 亿数据集，尝试将这些线延伸到右边，看看会发生什么。
我们不扩展其中许多，因为这些都是非常昂贵的实验，需要大量的耐心，但我们扩展了两个最有趣的，似乎它还在持续。
首先，是的，视觉transformer不断增长。
我们没有对ResNet进行这样的实验，因为它看起来不够有前途，不值得付出这样做的代价。
但似乎混合器，我们认为是更灵活的架构，现在始终高于变换器，这是个好消息。
是的，这是个好消息。
那么现在是我应该停下的时候了，对吗？
或者再接受更多问题。
是的，我猜是一个问题。
我能就您之前展示的扩展提出一个跟进问题吗？
实际上就是我的前一个问题。
我很好奇这个模型大小与自然语言模型大小相比如何？
比如，从较小的模型到更大的模型，它们在模型大小方面是否可比？
如果不是，那么，为什么您认为，对于这三个点的模型来说，有什么不同？
是的，实际上我的一个同事有一张幻灯片我很讨厌，但他知道这是自然语言处理和视觉中的模型参数数量。
问题是，你如何衡量一个模型的大小？
如果你只是衡量参数的数量，那么这些视觉模型要小得多。
然而，语言模型中的参数数量，例如，其中大部分都在字典中，而对我们来说根本不存在。
它是线性嵌入，参数数量微不足道。
所以就参数数量而言，它要小得多。
我个人认为参数数量并不意味着太多。
然后，你可以另一种衡量方式也许是根据计算量，比如它在一个数据点上执行多少浮点运算？
在这方面，它处于同一水平。
然而，上次我检查，大概是几个月前，最大的语言模型仍然比视觉模型多四倍或五倍，我相信。
是的，这就是衡量模型大小的两种方式。
我认为这两种方式都不是衡量模型大小的唯一正确方式。
我认为这实际上是一个有趣的研究课题，就像如何正确衡量和排序模型的容量一样并不清楚。
我会打开这个。
你知道为什么视觉是，或者抱歉，视觉要小得多吗？
我认为只是对它的兴趣较少，因此在上面花费的资源也较少。
就像在谷歌，进行语言研究的团队比进行视觉研究的团队要多得多。
而我认为我们是少数几个拥有大量资源并且对视觉方面的扩展非常感兴趣的团队之一。
而在语言方面，似乎有很多团队在做这件事。
我认为这实际上是主要原因。
并不是我们不想超越这个，或者如果可以的话，我们会再走一点。
太棒了。
谢谢你。
对。
所以我们实际上已经超时了。
所以任何需要离开的人，请随时离开。
在我们做这之前，卢卡斯，非常感谢你的加入，一路从大洋彼岸赶过来。
我们知道现在已经是晚上了，所以感谢你抽出宝贵的时间来和我们交谈。
是的，谢谢你的邀请。
总是喜欢谈论工作。
