So I'm excited to talk today about our recent work on using transformers for reinforcement learning.
And this is joint work with a bunch of really exciting collaborators, most of them at UC Berkeley, and some of them at Facebook and Google.
I should mention this work was led by two talented undergrads, Lily Chen and Kevin Blue.
And I'm excited to present the results we had.
So let's try to motivate why we even care about this problem.
So we have seen in the last three or four years, net transformers since the introduction in 2017 have taken over lots and lots of different fields of artificial intelligence.
So we saw them having a big impact for language processing.
We saw them being used for vision, in the vision transformer very simply.
They were in nature trying to solve protein folding and very soon they might just replace as computer scientists by having automatically generate code.
So with all of these advances, it seems like we are getting closer to having a unified model for decision making for artificial intelligence.
but artificial intelligence is much more about not just having perception, but also using the perception knowledge to make decisions.
And this is what this talk is going to be about.
But before I go into actually thinking about how we will use these models for decision -making, here's a motivation for why I think it is important to ask this question.
So unlike models for RL, When we look at transformers for perception modality, like I showed in the previous slide, we find that these models are very scalable and have very stable training dynamics.
So you can keep, as long as you have enough computation and you have more and more data that can be sourced, you can train bigger and bigger models and you'll see very smooth reductions in the loss.
The overall training dynamics are very stable and this makes it very easy for practitioners and researchers to build these models and learn richer and richer distributions.
Like I said, all of these advances have so far occurred in perception.
What we're interested in this talk is to think about how we can go from perception, looking at images, looking at text, and all these kinds of sensory signals to then going into the field of actually taking actions and making our agents do interesting things in the world.
And here throughout the talk, we should be thinking about why this perspective is going to enable us to do scalable learning, like I showed on the previous slide, as well as bring stability into the whole procedure.
So sequential decision -making is a very broad area.
And what I'm specifically gonna be focusing on today is the one route to sequential decision -making that's reinforcement learning.
So just as a brief background, what is reinforcement learning?
So we are given an agent who is in a current state and the agent is going to interact with the environment by taking actions.
and by taking these actions, the environment is going to return to it a reward for how good that action was, as well as the next state into which the agent will transition and this whole feedback loop will continue.
The goal here for an intelligent agent is to then using trial and error, so try out different actions, see what rewards will lead to, learn a policy which maps your states to actions such that the policy maximizes the agent's cumulative rewards over time horizon.
So you take a sequence of actions and then based on the reward you accumulate for that sequence of actions, we'll judge how good your policy is.
This talk is also going to be specifically focused on a form of reinforcement learning that goes by the name of offline reinforcement learning.
So the idea here is that what changes from the previous picture where I was talking about online reinforcement learning is that here, now instead of doing actively interacting with the environment, you have a collection of log data of interactions.
So think about some robot that's going out in the fields and it collects a bunch of sensory data and you've all logged it.
Using that log data, you now want to train another agent.
it could be another robot to then learn something interesting about that environment just by looking at the log data.
So there's no trial and error component, which is currently one of the extensions of this framework, which would be very exciting.
So I'll talk about this towards the end of the talk, why it's exciting to think about how we can extend this framework to include an exploration component and have trial and error.
Okay, so now to go more concretely into what the motivating challenge of this talk was, now that we have introduced RL.
So let's look at some statistics.
So large language models have billions of parameters and today they have roughly about 100 layers in Transformer.
They are very stable to train using supervised learning style losses, which are the building blocks of autoregressive generation, for instance, or for mass language modeling as input.
This is a field that's growing every day.
There's a course on that at Stanford that we're all taking just because it has had such a monumental impact on AI.
RL policies on the other hand, and I'm talking about deep RL, the maximum they would extend to is maybe millions of parameters or training layers.
And what's really unnerving is that they're very unstable to train.
So the current algorithms for reinforcement learning, they build on a mostly dynamic programming, which involves solving an inner loop optimization problem that's very unstable.
And it's very common to see practitioners in RL looking at reward curves that look like this.
So what I really want you to see here is the variance in the returns that we tend to get in RL.
It's really huge, even after doing multiple rounds of experimentation.
And that is really how the code got it done with the fact that our algorithms alone injectors need better improvements so that the performance can be stably achieved by agents in complex environments.
So what this work is hoping to do is it's going to introduce transformers.
And I'll first show in one slide what exactly that model looks like and then we're going to go into deeper details of each of the components.
I have a question.
Yeah, can I ask a question real quick?
Yes.
Thank you.
What I'm curious to know is what is the cause for why RL typically has several orders of magnitude fewer parameters?
That's a great question.
So typically when you think about reinforcement learning algorithms in deep RL in particular, so the most common algorithms, for example, will have different networks playing different roles in the task.
So you'll have a network for instance, playing the role of an actor.
So it's trying to figure out a policy and then there'll be a different network that's playing the role of a critic.
And these networks are trained on data that's adaptively gathered.
So unlike perception where you will have a huge dataset of interactions on which you can train your models.
In this case, the architectures and even the environments to some extent are very simplistic because of the fact that we are trying to train very small components, the functions that we are training and then bringing them all together.
And these functions are often trained in not super complex environments.
So it's a mix of different issues.
I wouldn't say it's purely just the fact that the learning objective is at fault, but it's a combination of the environments we use, the combination of the targets that each of the neural networks are predicting, which leads to networks which are much bigger than what we currently see tending to overfit.
And that's why it's very common to see neural networks with much fewer layers being used in RL as opposed to perception.
Thank you.
You want to ask a question?
Yep.
Yeah, I was going to.
Is there a reason why you chose offline RL versus online RL?
That's another great question.
So the question is why offline RL as opposed to online RL?
And the plain reason is because this is the first work trying to look at reinforcement learning.
So offline RL avoids this problem of exploration.
You are given a lot of data set of interactions.
You're not allowed to further interact with the environment.
So just from this data set, you're trying to unearth a policy of what the optimal agent would look like.
So it would - Right, but like if you do online RL, wouldn't that like just give you this opportunity of exploration basically?
It would, it would.
What it would also do, which is technically challenging here, is that the exploration would be harder to encode.
So offline RL is the first step.
There is no reason why we should not study why online RL cannot be done.
It's just that it provides a more contained setup where ideas from transformers will directly extend.
Okay, sounds good.
So let's look at the model and it's really simple on purpose.
So what we're going to do is we're going to look at our offline data, which is essentially in the form of trajectories.
So offline data would look like a sequence of states, actions, returns over multiple time steps.
It's a sequence, so it's natural to think of us as directly feeding it as input to a transformer.
In this case, we use a causal transformer transformer as it's common in GPT.
So we go from left to right.
And because this data set comes with the notion of time step, causality here is much more well -intended than the general meaning that's used for perception.
This is really causality, how it should be in perspective of time.
What we predict out of this transformer are the actions conditioned on everything that comes before that token in the sequence.
So if you want to predict the action at this t minus 1 step, we'll use everything that came at time step t minus 2, as well as the returns and states at time step t minus 1.
So we will go into the details of how exactly each of these are encoded.
But essentially, this is in a one -liner.
it's taking the trajectory data from the offline data, treating it as a sequence of tokens, passing it through a causal transformer and getting a sequence of actions as the output.
Okay, so how exactly do we do the forward pass through the network?
So one important aspect of this work, which is the use states actions and this quantity called returns to go.
So these are not direct rewards.
These are returns to go and let's see what they really mean.
So this is our trajectory that goes as input to this number.
And the returns to go are the sum of rewards starting from the current time step until the end of the episode.
So really what we want the transformer is to get better at using a target return.
This is how you should think of returns to go as the input in deciding what action to take.
This perspective is gonna have multiple advantages.
It will allow us to actually do much more than offline RL and generalizing to different tasks by just changing the returns to go.
And here it's very important.
So at time step one, we will just have the overall sum of rewards for the entire trajectory.
At time step two, we subtract the reward we get by taking the first action and then have the sum of rewards for the remainder of the trajectory.
Okay, so that's how we call it returns to go.
Like how many more rewards in accumulation you need to acquire to fulfill your reward, your return goal that you set in the beginning.
What is the output?
The output is the sequence of predicted actions.
So as I showed in the previous slide, we use a causal transformer so we'll predict in sequence the desired actions.
The attention which is going to be computed inside the transformer will take in an important hyperparameter, K, which is the context length.
We see that in perception as well here and for the rest of the talk, I'm going to use the notation K to denote how many tokens in the past would we be attending over to predict the action and the current time step.
Okay, so again, digging a little bit deeper into code, there are some subtle differences with how a decision transformer operates as opposed to a normal transformer.
The first is that here, the time step notion is going to be, have a much bigger semantics that extends across three tokens.
So in perception, you just think about the time step for word, for instance, like an NLP or per patch for vision.
And in this case, we will have a time step encapsulating three tokens, one for the states, one for the actions and one for the rewards.
And then we'll embed each of these tokens and then add the pollution embedding as is common in the transformer.
And we feed those inputs to the transformer.
At the output, we only care about one of these three tokens in this default setup.
I will show experiments where even the other tokens might be of interest as target predictions, but for now let's keep it simple.
We want to learn a policy.
A policy is just trying to predict actions.
So when we try to decode, we'll only be looking at the actions from the hidden representation in the pre -final there.
Okay, so this is the forward pass.
Now, what do we do with this network?
We train it.
How do we train it?
Sorry, just a quick question on semantics there.
If you go back one slide, the plus in this case, the syntax means that you are actually adding the values element -wise and not concatenating them.
Is that right?
That is correct.
Cool, sorry to check.
Okay, so what's the last function?
Follow up on that.
I thought it was concatenate.
Why are we just adding it?
Sorry, can you go back?
Yeah, I think it's a design choice.
You can concatenate, you can add it.
It leads to different functions being encoded.
In our case, it is addition.
Okay, did you try the other one and it just didn't work?
Or why is that?
Because I think intuitively concatenating would make more sense.
So I think both of them have different use cases for the functional encoding.
One is really mixing in the embeddings for the state and basically shifting it.
So when you add something, if you think of the embedding of the states as a vector, and you add something, you are actually shifting it.
Whereas in the concatenation case, you are actually increasing the dimensionality of this space.
So those are different choices, which are doing very different things.
We found this one to be work better.
I'm not sure I remember if the results was very significantly different if you would concatenate them, but this is the one which we operate with.
But wouldn't there, because like if you're shifting it, right, like if you have an embedding for a state and let's say like you perform certain actions and you end up at the same state again, you would want these embeddings to be the same.
However, now you're at a different time step.
So you shifted it.
So wouldn't that be like harder to learn?
So there's a bigger and interesting question in that what you said is basically, are we losing the Markov property?
Because as you said that if we come back to the same state at a different time step, shouldn't we be doing similar operations?
And the answer here is yes, we are actually being non -Markov.
And this might seem very non -intuitive at first that why is non -Markovness important here?
And I want to refer to another paper which came very much in conjunction with this to try to transform what that actually shows in more detail.
And it's basically saying that if you were trying to predict the transition dynamics, then you could have actually had a Markovian system built in here, which would do just as good.
However, for the perspective of trying to actually predict actions, it does help to look at the previous time steps, even more so when you have missing observations.
So for instance, if you have the observations as being a subset of the true state.
So looking at the previous states and actions helps you better fill in the missing pieces in some sense.
So this is commonly known as partial observability, where you, by looking at the previous tokens, you can do a better job at predicting the actions that you should take at the current time step.
So non -Markovness is on purpose and it's not intuitive, but I think it's one of the things that separates this framework from existing ones.
So it will basically help you because like RL usually works on like infinite like works better on like infinite horizon problems right so technically the way you formulate it it would work better on finite horizon problems I'm assuming because you want to take different actions based on like the history based on like given the fact that now you have to be different timestamp.
Yeah, yeah.
So if you wanted to work on Infinite Horizon, maybe something like discounting would work just as well to get that effect.
In this case, we were using a discount factor of 1, or basically like no discounting at all.
But you're right.
If I think we really want to extend it to Infinite Horizon, we would need to change the discount factor.
Thanks.
OK, so.
Quick question.
I think it was just answered in chat.
But I'll ask it anyways.
I think I might have missed this, or maybe you're about to talk about it.
The offline data that was collected, what policy was used to collect it?
So this is a very important question, and it will be something I mentioned in the experiment.
So we were using the benchmarks that exist for offline RL, where essentially the way these benchmarks are constructed is you train an agent using online RL, and then you look at its replay buffer at some time step.
So while it's training, so while it's like a medium sort of expert, you collect its the transitions it's experienced so far and make that as the offline data.
It's something which is like our framework is really agnostic to what offline data that you use.
So I'm not discussing it so far, but something that in our experiments is based on traditional benchmarks.
Got it.
So the reason I ask isn't, I'm sure that your framework can accommodate any offline data, but it seems to me like the results that you're about to present are gonna be heavily contingent on what that data collection policy is.
Indeed, indeed.
And also, so we will, I think I have a slide where we show an experiment where the amount of data can make a difference in how we compare with baselines.
And essentially we will see how this in transformer, especially shines when there is small amounts of offline data.
Okay, cool.
Thank you.
Okay, great questions.
So let's go ahead.
So we have defined our model, which is going to look at these tragic trees.
And now let's see how we train it.
So very simple, we are trying to predict actions.
We try to match them to the ones we have in our error if they're discrete and we can use a cross entropy.
But there is something very deep in here for our research, which is that these objectives are very stable to train and easy to regularize because they've been developed for supervised learning.
In contrast, what R is more used to is dynamic programming style objectives which are based on the Bellman equation and those end up being much harder to optimize and scale, and that's why you see a lot of the variance in the results as well.
Okay, so this is how we train the model.
Now, how do we use the model?
And that's the point about trying to do rollout for the model.
So here again, this is going to be similar to doing an autoregressive generation.
There is an important token here, which was the returns to go.
And what we need to set during evaluation, presumably we want export level performance because that will have the highest returns.
So we set the initial returns to go, not based on our trajectory because now we don't have a trajectory.
You're gonna generate a trajectory.
So this is at inference time.
So we will set it to the export return for instance.
So encode what this whole procedure would look like and basically you set this returns to go token to as some target return and you set your initial state to run from the environment distribution of initial states and then you just roll out your decision transformer.
So you get a new action.
This action will also give you a state and reward from the environment.
You append them to your sequence and you get a new returns to go and you take just the context and key because that's what's used by the transformer to making predictions and then feed it back to the distant transformer.
So it's regular autoregressive generation, but the only key point to notice is how you initialize the transformer for RL.
Sorry, I had one question here.
So how much does the choice of the export targeted matter?
Does it have to be like the mean expert reward or can it be like the maximum reward possible in department?
Like does the choice of the number really matter?
That's a very good question.
So we generally would set it to be slightly higher than the max return in the data set.
So I think the fact that we use was 1 .1 times, But I think we have done a lot of experimentation in the range, and it's fairly robust to what choice you use.
So for example, for Hopper expert returns about 3600, and we have found very stable performance all the way from 35, 3 ,400 to even going to very high numbers like 5 ,000, it works.
However, I would want to point out that this is something which is not typically needed in regular RL, like knowing the expert return.
Here we are actually going beyond regular RL and that we can choose a return we want.
So we also actually need this information about what the expert return is at test.
There's another question.
Yes.
So, it's just that you're kind of beyond the regular IRE, but I'm curious about, do you also like restrict this framework to only offline IRE?
Because if you want to run this kind of framework in online IRE, you have to determine the returns to go a priori.
So this kind of framework, I think it's kind of restricted to only offline IRE.
Do you think so?
Yes, and I think asking this question as well earlier that, yes, I think for now, this is the first group.
So we were focusing on offline RL where this information can be gathered from the offline data set.
It is possible to think about strategies on how you can even get this online.
What you'll need is a curriculum.
So early on during training as you're gathering data, you will set, when you're doing rollouts, you will set your expert return to whatever you see in the data set.
And then incremented as and when you start seeing that the transformer can actually exceed that performance.
So you can think of specifying a curriculum from slow to high for what that expert return could be for which you roll out the decision transformer.
I see.
Cool.
Thank you.
Yeah, this was about the model.
We discussed how this model is, what the input to this model are, what the outputs are, what the loss function is used for training this model, and how do we use this model at test time.
There is a connection to this framework as being one way to instantiate what is often known as RL as probabilistic inference.
So we can formulate RL as a graphical model problem where you have the states and actions being used to determine what the next state is.
And to encode a notion of optimality, Typically, you would also have these additional auxiliary variables, O1, O2, and so on and so forth, which are implicitly saying that encoding some notion of reward.
And conditioned on this optimality being true, RL is the task of learning a policy, which is the mapping from states to actions such that we get optimal behavior.
And if you really squint your eyes, you can see that these optimality variables and decision transformers are actually being encoded by the returns to go.
So if when we give a value that's high enough at test time during rollouts, like the expert return, we are essentially saying that conditioned on this being the mathematical form quantification of optimality, roll out your decision transformer to hopefully satisfy this condition.
So yeah, so this was all I want to talk about the model.
Can you explain that, please?
What do you mean by optimality variables in the decision transformer and how do you mean like return to go?
Right.
So optimality variables, we can think in the most simplest context as legislative or binary.
So one is if you solve the goal and zero as if you did not solve the goal.
And what basically in that case, you could also think of your decision transformer as at test time and we encode the returns to go, we could set it to one, which would basically mean that conditioned on optimality.
So optimality here means solving the goal as one.
generate me the sequence of actions such that this would be true.
Of course, our learning is not perfect, so it's not guaranteed we'll get that, but we have trained the transformer in a way to interpret the returns to go as some notion of optimality.
So is it, if I'm interpreting this correctly, it's roughly like saying, show me what an optimal sequence of transitions look like, because you've learned, model has learned both successful and unsuccessful transitions.
Exactly.
Exactly.
And as we shall see in some experiments, we can...
For the binary case, it's either optimal or non -optimal, but rarely this can be a continuous variable, which it is in our experiments.
So we can also see what happens in between experimentally.
Okay.
So let's jump into the experiments.
So there are a bunch of experiments and I've picked out a few which I think are interesting and give the key results in the paper, but feel free to refer to the paper for an even more detailed analysis on some of the components of our model.
So first we can look at how well does it do on offline RL.
So there are benchmarks for the Atari suite of environments and the OpenAI gym.
And we have another environment key to which is especially hard because it contains sparse rewards and requires it to do credit assignment that I'll talk about later.
But across the board, we see that this is in transformer is competitive with the state of the art model free offline RL methods.
In this case, this was a version of Q -learning designed for offline RL and it can do excellent when, especially when there is long -term paired assignment where traditional methods based on TD learning would fail.
Yeah, so the takeaway here should not be that we are at the stage where we can just substitute the existing algorithms for the decision transformer, but this is a very strong evidence in favor that this paradigm which is building on transformers will permit us to better iterate and improve the models to hopefully surpass the existing algorithms uniformly.
And there's some early evidence of that in hard environments, which do require credit assignment, long -term credit assignment.
Can I ask a question here about the baseline, specifically TD learning?
I'm curious to know, cause I know that a lot of TD learning agents are feed forward networks.
Are these baselines, do they have recurrence?
Yeah, yeah.
So I think the conservative two learning baselines here or did have recurrence, but I'm not very sure.
So I can check back on this offline and get back to you on this.
Okay.
We'll be okay.
Thank you.
Also another quick question.
So just how exactly do you evaluate the decision transformer here in the experiment?
So, cause you need to supply the returns to go.
So do you use optimal like policy to get what's optimal rewards and speed that in?
Yes.
So here we basically look at the offline data setup as useful training, and we set whatever was the maximum return in the offline data, we set the desired target return to go as slightly higher than that.
So 1 .1 was the coefficient used.
I see.
So the performance, sorry, I'm not really well aware of the RLS of that, but how is the performance defined here?
just like is it how much reward you get actually from this?
Yes, yes.
So you can specify a target return to go, but there's no guarantee that the actual actions that you take will achieve that return.
So you measure the true environmental return based on that.
Yeah, I see.
But then just curious, so are these performance the percentage you get like for like how much I guess reward you recover from the actual environment.
Yeah so these are not percentages these are some way of normalizing the returns so that everything was between three to hundred.
Yeah yeah I see then just wonder if you have a like a rough idea about how much like reward actually is recovered by decision transformers like does it say like if you specify I want to sometimes or?
That's an excellent question and my next slide.
I see.
Thanks.
So here we're going to answer this question that was asked is like, if you feed in the target return, it could be exported or it could also not be exported.
How well does the model actually do in attaining it?
So the X axis is what we specify as the target return we want.
and the Y axis is basically how much, how well do we actually get.
For reference, we have this green line, which is the oracle.
So which means whatever you desire that your system transformer gives it to you.
So this would have been the ideal case.
So it's a diagonal.
We also have, because of offline RL, we have in orange, what was the best trajectory in data set.
So the offline data is not perfect.
So we just plot what is the upper bound on the offline data performance.
And here we find that for the majority of the environments, there is a good fit between the target return we feed in and the actual performance of the model.
And there are some other observations which I want you to take from the slide is that because we can vary this notion of reward, we can in some sense do multitask RL by return conditioning.
This is not the only way to do multitask RL.
You can specify a task via natural language, you can via goal state and so on, but this is one notion where the notion of a task could be how much reward you want.
And another thing to notice is occasionally the smallest extrapolate.
This is not a trend we have been seeing consistently, but we do see some signs of it.
So if you look at, for example, SeaQuest, here, the highest return trajectory in a data set was pretty low.
And if we specify a return higher than that for our decision transformer, we do find that the model is able to achieve.
So it is able to generate trajectories with returns higher than it ever saw in the data set.
I do believe that future work in this space trying to improve this model should think about how can the strength be more consistent across environments, because this would really achieve the goal of offline RL, which is given suboptimal behavior, how do you get optimum behavior out of it?
That remains to be seen how well the strength can be made consistent across environments.
Can I jump in with a question?
Yes.
So I think that last point is really interesting and it's cool that you guys occasionally see it.
I'm curious to know what happens.
So this is all condition.
You give as an input what return you would like and it tries to select a sequence of actions that gets it.
I'm curious to know what happens if you just give it ridiculous inputs.
Like for example, here are the order of magnitude for the return is like 50 to 100.
What happens if you put in 10 ,000?
Good question.
And this is something we tried early on.
I don't want to say we went up to 10 ,000, but we try really high returns it's not even an expert would get.
And generally we see this leveling performance.
So you can see hints of it in half cheetah and pong as well, or walker to some extent.
If you look at the very end, things start saturating.
So if you exceed what is certain threshold, which often corresponds with the best trajectory threshold, but not always.
Beyond that, everything is similar returns.
At least one good thing is it does not degrade in performance.
It would have been a little bit worrying if you specified a return of 10 ,000 and it gives you a return which is 20 or something really low.
It's good that it stabilizes, but it's not that it keeps increasing on and on.
There would be a point where the performance when it gets saturated.
Okay, thank you.
I was just curious, so usually for transfer models, you need a lot of data.
So do you know how much data do you need?
Like how does it scale the data, the performance?
This is a transformer?
Yeah, so here we use the standard data, like the D4 RL benchmarks for Mijuku, which I think have million transitions in the order of millions.
Got it.
For Atari, we used 1 % of the replay buffer, which is smaller than the one we used for the MuJoCo benchmarks.
And I have a result in the very next slide, which shows this in transformer, especially being useful when you have a little data.
So yeah, so I guess one question to ask before you move on in the last slide, what do you mean again by return conditioning for the multitask part?
Yeah, so if you think about the returns to go at this time, the one you have to feed in as the starting token, as one way of specifying what policy you want.
How is that multitask?
So it's multitask in the sense that because you can get different policies by changing your target return to go, you're essentially getting different behaviors encoded.
So think about, for instance, a hopper and you specify a return to go that's really low.
So you're basically saying, get me an agent, which will just stick around its initial state and not go into unchartered territory.
unchartered territory and if you give it really really high, then you're asking it to do the traditional task, which is to hop and go as far as possible without falling.
Can you qualify those multitask because that basically just means that your return conditioning is a cue for it to memorize right, which is usually like one of the pitfalls of multitask.
So So I'm not sure if it's a task identifier.
That's what I'm trying to say.
So I'm not sure if it's memorization because like I think the purpose of this, I mean, like having an offline data set that's fixed is basically saying that it's very, very specific to if you had the same start state and you took the same actions and you had the same target returns, that would qualify as memorization.
But here at test time, we allow all of these things to change and in fact they do change.
So your initial state would be different, your targeted turn, could be a different scaler than one you ever saw during training.
So essentially the model has to learn to generate that behavior, starting from a different initial state and maybe a different value of the target return than it saw during training.
If the dynamics are stochastic, that also makes it that even if you memorize the actions, you're not guaranteed to get the same next state.
So you would actually have a bad correlation with the performance if the dynamics are also stochastic.
Also, I was very curious, how much time does it take to train this in transformer in general?
So it takes about a few hours.
So I want to stay like about four to five hours, depending on what quality GPU you use.
But yeah, that's a reasonable estimate.
Yep, got it, thanks.
Okay, so actually while doing this experiment, this project, we thought of a baseline which we were surprised is not there in previous literature on offline RL, but makes very much sense.
And we thought we should also think about whether the decision transformer is actually doing something very similar to that baseline.
And the baseline is what we call as person behavioral cloning.
So behavioral cloning, what it does is basically it ignores the returns and simply imitates the agent by looking, by just trying to map the actions given the current states.
This is not a good idea with an offline data set, which will have project trees of both low returns and high returns.
So traditional behavioral cloning, it's common to see that as a baseline in offline RL methods.
And it is, unless you have a very high quality data set, it does, it is not a good baseline for offline RL.
However, there is a version that we call this person BC, which actually makes quite a lot of sense.
And in this version, we filter out the top trajectories from our offline data set.
One stop, the ones which have the highest rewards.
You know the rewards for each transition.
You calculate the returns of the trajectories and you take the trajectories with the highest returns and keep a certain percentage of them, which is going to be hyperparameter here.
And once you keep those top fraction of your trajectories, you then just ask your model to imitate them.
So imitation learning also uses, especially when it's used in the form of behavioral cloning, it uses supervised learning essentially, it's a supervised learning problem.
So you could actually also get supervised learning objective functions if you did this filtering step.
And what we find actually that for the moderate and high data regimes, the descent transformer is actually very comparable to post NVC.
So it's a very strong baseline, which I think all of future work in offline RL should include.
There's actually an ICLR submission from last week, which has a much more detailed analysis on just this baseline that we introduced in this paper.
And what we do find is that for low data regimes, the descent transformer does much better than person -behavioral cloning.
So this is for the Atari benchmarks where, like I previously mentioned, we have a much smaller dataset as compared to the MuJoCo environments.
And here we find that even after varying the different fraction of the percentage hyperparameter here, we are generally not able to get the strong performance that a decision transformer gets.
So 10 % BC basically means that we filter out and keep the top 10 % of the trajectories.
If you go even lower, then the start data set becomes fairly small, so the baseline will become meaningless.
But for even the reasonable ranges, we never find the performance matching that of the same transformers for the Atari benchmarks.
Did you have any?
So I noticed in table three, for example, which is not this table with the one just before in the paper, there's a report on the CQL performance, which to me also feels intuitively pretty similar to the percent BC in the sense of like, you pick trajectories you know are performing well and you try and stay roughly within sort of the same kind of policy distribution and state space distribution.
I was curious on this one, do you have a sense of what the CQL performance was relative to say the percent BC performance here?
So that's a great question.
So the question is that even for CQL, you rely on this notion of pessimism, where you wanna pick trajectories where you're more confident in and try to make sure policy remains in that region.
So I don't have the numbers of CQL on this table, but if you look at the detailed results for Atari, then I think they should have the SQL for sure, because that's the numbers we are reporting here.
So I can tell you what the SQL performance is actually pretty good.
And it's very competitive with the decision transformer for Atari.
So this TD learning baseline here is SQL.
So by naturally by extension, I would imagine it doing better than post MDC.
And I apologize if this was mentioned, I just missed it.
But do you have the sense that this is basically like a failure of CQL to be able to extrapolate well or sort of stitch together different parts of trajectories, whereas the decision transformer can sort of make that extrapolation between you have like the first half of one trajectory is really good, the second half of one trajectory is really good.
And so you can actually piece those together with decision transformer where you can't necessarily do that with CQL because the path connecting those may not necessarily be well covered by the behavior policy.
Yeah, yeah.
So this actually goes to one of the intuitions which I did not emphasize too much, but we have a discussion on that paper where essentially why do we expect a transformer or any model for that matter to look at offline data that's suboptimal and get something, a policy that generates optimal rollouts.
The intuition is that as Scott was mentioning, you could perhaps stitch together good behaviors from suboptimal trajectories and that stitching could perhaps lead to a behavior that is better than anything you saw in individual trajectories in your dataset.
It's something we find early evidence of in a small scale experiment for graphs.
And that's really our hope also that something that the transformer is really good at because it can attend to very long sequences.
So it could identify those segments of behavior, which when stitched together would give you optimal behavior.
So, and it's very much possible that is something unique to this in transformers and something like CQL would not be able to do person BC because it's filtering out the data is automatically being limited and not being able to do that because the segments of good behavior could be in trajectories which overall do not have a high return.
But so if you filter them out, you are losing out on that information.
Okay, so I said there is a hyperparameter, the context and key and like with most of perception, one of the big advantages of transformers as opposed to other sequence models like LSTMs is that they can process very large sequences.
And here at a first glance, it might seem that being Markovian would have been helpful for RL, which also was a question that was raised earlier.
So we did this experiment where we did compare performance with context and k equals one.
And here we had context and between 30 for the environments and 50 for Pong.
And we find that increasing the context length is very, very important to get good performance.
Okay, now, so far I've showed you how this in transformer, which is very simple, There was no slide I had which was going into the details of dynamic programming, which is the crux of most RL.
This was just pure supervised learning in an autoregressive framework that was getting us this good performance.
What about cases where this approach actually starts outperforming some of the traditional methods RL?
So to probe a little bit further, we started looking at sparse reward environments.
And basically we just took our existing MuJoCo environments and then instead of giving it the information for reward for every transition, we fed in the cumulative reward at the end of the trajectory.
So every transition will have a zero reward except the very end where you get the entire reward at once.
So it's a very sparse reward scenario for that reason.
And here we find that compared to the original dense results, the delayed results for DT, they will deteriorate a little bit, which is expected because now you are withholding some of the more fine -grained information at every time step, but the drop is not too significant compared to the original DT performance here.
Whereas for something like CQL, there's a drastic drop in performance.
So SQL suffers quite a lot in sparse reward scenarios, but the decision transformer does not.
And just for completeness, you also have performance of behavioral cloning and person behavioral cloning, which because they don't look at reward information, except maybe person BC looks at only for pre -processing the data set, these are agnostic to whether the environments have sparse rewards or not.
Would you expect this to be different if you were doing online RL?
What's the intuition for it being different?
April, I would say no, but maybe I'm missing out on a key piece of intuition behind that question.
I think that because you're training like offline, right?
Like you're, the next input will always be the correct action in that sense.
So you don't just like deviate and like go off the rails technically because you just don't know.
So I could see like how online would have like a really hard, like cold start basically, because it just doesn't know.
And it's just happening in the dark until it like maybe eventually hits the jackpot.
Right.
Right.
I think I agree.
That's a, that's a good piece of, uh, uh, intuition out there, but yeah, I think here that because offline RL is really getting rid of the trial and error aspect of it and for sparse reward environments that would be harder.
So the drop in DT performance should be more prominent there.
I'm not sure how it would compare with the drop performance for other algorithms.
But it does seem like an interesting setup to test DTN.
Well, and it did to, maybe I'm wrong here, but my understanding with the decision transformer as well is this critical piece that in the training you use the rewards to go, right?
So is it not the sense that essentially like for each trajectory from the initial state based on the training regime, the model has access to whether or not the final result was a success or failure, right?
But that's sort of the unique aspect of the training regime for decision transformers.
Whereas in CQL, my understanding is that it's based on sort of a per transition training regime.
And so each transition is decoupled somewhat to what the final reward was.
Is that correct?
Yes.
Although like one difficulty, which at a first glance, we kind of imagined the different transformer having is that that initial token will not change throughout the trajectory because it's a sparse reward scenario.
Except the very last token where it will drop down to zero all of a sudden, this token remains the same throughout.
But I think you're right that maybe just even at the start feeding it in a manner which looks at the future rewards that you need to get to is perhaps one part of the reason why the drop in performance is are not noticeable.
Yeah, I mean, I guess one sort of ablation experiment here would be if you change the training regime so that only the last trajectory had the reward.
But I'm trying to think about whether or not that would just be compensated for by sort of the attention mechanism anyway, and vice versa, right?
If you embedded that reward information into the CQL training procedure as well, I'd be curious to see what would happen there.
Yeah.
That's a good experience.
Okay, so related to this, there's another environment we tested.
I gave you a brief preview of the results in one of the earlier slides.
So this is called the key -to -door environment.
And it has three phases.
So in the first phase, the agent is placed in a room with the key.
A good agent will pick up the key, and then phase two, it will be placed in an empty room.
And in phase three, it will be placed in a room with a door where it will actually use the key that it collected in phase one if it did to open the door.
So essentially the agent is going to receive a binary reward corresponding to whether it reached and opened the door in phase three, conditioned on the fact that it did pick up the key in phase one.
So there is this national notion on that you want to assign credit to something that happened to an event that happened really in the past.
So it's a very challenging and sensible scenario if you want to test your models for how well they are at long -term credit assignment.
And here we find that, so we tested it for different amounts of project trees.
So here are the number of project trees, basically saying how often would you actually see this kind of behavior.
And the Listen transformer is, and person -behavior cloning both of these actually baselines do much better than other models that struggle at this task.
There's a related experiment there, which is also of interest.
So generally a lot of other algorithms have this notion of an actor and a critic.
Actor is basically someone that takes actions conditioned on the states of think of a policy.
A critic is basically evaluating how good these actions are in terms of achieving a long term, in terms of the succumulative sum of rewards and the long term.
This is a good environment because we can see how well the distant transformer would do if it was trained as a critic.
So here, what we did is instead of having the actions as the output target, what if we substituted that with the rewards?
So that's very much possible.
We can again use the same color transformer machinery to only look at transitions in the previous time step and try to pick the reward.
And here we see this interesting pattern where in the three phases that we had in that key -to -door environment, we do see the reward probability changing very much in how we expect.
So basically the three scenarios, So the agent, the first scenario, let's look at Bloom, in which the agent does not pick up the key in phase one.
So the reward probability, they all start around the same, but as it becomes apparent that the agent is not going to pick up the key, the reward starts going down.
And then it stays very much close to zero throughout the episode, because there's no way you will have the key to open the door in the future phases.
If you pick up the key, there are two possibilities, which remain, which are essentially the same in phase two, where you had an empty room, which is just a distractor to make the episode really long.
But at the very end, the two possibilities are one that you take the key and you actually reach the door, which is the one we see in orange and brown here, where you see that the reward probability goes up, and there's this other possibility that you actually pick up the key but do not reach the door, in which case, again, you start seeing that the reward probability that's predicted starts going down.
So the takeaway from this experiment is that the same transformers are not just great actors, which is what we've been seeing so far in the results from the optimized policy, but they're also very impressive critics in doing this long -term assignment where the reward is also very sparse.
So just to be correct, are you predicting the rewards to go at each time step, or is this the reward at each time step that you're predicting?
So this is the rewards to go, and I can also check.
My impression was in this particular experiment it didn't really make a difference whether we're predicting rewards to go or the actual rewards, but I think we're gonna chance to go for this one.
Well, I was curious, so how do you get the probability distribution on the rewards?
Is it just like, you just evaluate a lot of different episodes and just find the rewards or are you explicitly predicting some sort of distribution?
Oh, so this is a binary reward, so you can have a probabilistic outcome.
Any other question?
Yeah, so generally we will call like something predicts a state value or state actual value as critic.
But in this case, you asked decision transformer to only predict a reward.
So why should you still call it a critic?
So I think the analogy here gets a bit clear with returns to go.
Like if you think about returns to go, it's really capturing that essence that you want to see the future rewards.
Oh, I see.
So you mean it's just gonna predict the return to go instead of single step reward, right?
Yeah, yeah.
Okay, so if we're gonna predict the returns to go is kind of counterintuitive to me because in phase one when the agent is still in the K room, I think it should have a like high returns to go if it did after K, but in the plot, you're not in the K room, the agents pick up K and the agent that didn't pick up K has a same kind of level returns to go.
So that's quite counterintuitive to me.
I think this is reflecting on a good property, which is that your distribution, like if you interpret returns to go in the right way, in phase one, you don't know which of these three outcome are really possible.
And phase one also I'm talking about the very beginning basically.
Slowly you will learn about it, but essentially in phase one, if you see the returns to go as a one or zero, all three possibilities are equally likely.
And all three possibilities, so if we try to evaluate the predicted reward for these possibilities, it shouldn't be the same because we really haven't done, we don't know what's gonna happen in phase three.
Sorry, it's my mistake because previously I thought the green line is the agent which doesn't pick up the k, but it turns out the blue line is agent which doesn't pick up k.
So yeah, it's my mistake.
It makes sense to me.
Thank you.
Also, I was not fully clear from the paper, but did you do experiments where you're predicting what the actions and what the rewards should go?
And does it make any different performance if you're doing both together?
So actually we did some preliminary experiments on that and it didn't help us much.
However, I do want to again put in a plug for a paper that came concurrently, trajectory transformer, which tried to predict states, actions and rewards, actually all three of them.
They were in a model based setup where it made sense also to try to learn each of the components like the transition dynamics, the policy, and maybe even the critic in their setup together.
We did not find any significant improvements.
So in favor of simplicity and keeping it models free, we did not try to predict them together.
Okay, so the summary, we showed this in transformers, which is a first word in trying to approach RL based on sequence modeling.
The main advantages of our previous approaches is it's simple by design.
The hope is that for their extensions, we will find it to scale much better than existing algorithms.
It is stable to train because the loss functions we are using have been tested and I traded upon a lot by research and perception.
And in the future, we will also hope that because of these similarities with the architecture and the training with how perception -based tasks are conducted, it would also be easy to integrate them in this loop.
So the states, the actions, or even the task of interest, they could be specified based on perceptual -based senses, so you could have a target task being specified by natural language instruction.
and because these models can very well play with these kinds of inputs, the hope is that they would be able to integrate within the decision -making process.
And empirically, we saw strong performance in the range of offline RL settings and especially good performance in scenarios which required us to do long -term credit assignment.
So there's a lot of future work.
This is definitely not the end.
This is a first work in rethinking how do we build RL agents that can scale and generalize.
A few things that I picked out which I feel would be very exciting to extend.
The first is multimodality.
So really one of our big motivations with going after these kinds of models is that we can combine different kinds of inputs, both online and offline, to really build decision -making agents which work like humans.
We process so many inputs around us in different modalities and we act on them.
So we do take decisions and we want the same to happen in artificial agents.
And maybe this is in transformers is one important step in that route.
Multitask, so I should or I described a very limited form of multitasking here, which was based on the desired returns to go, but it could be more richer in terms of specifying a command to be a robot or a desired goal state, which could be, for example, even visual.
So trying to better explore the different multitask capabilities of this model would also be an interesting extension.
Finally, multi -agent.
As human beings, we never act in isolation.
We are always acting within an environment that involves many, many more agents.
Things become partially observable in those scenarios, which plays to the strengths of this in transformers being non -Markovian by design.
So I think there's great possibilities of exploring even multi -agent scenarios where the fact that transformers can process very large sequences compared to existing algorithms could again help build better models of other agents in your environment and act.
So that, yeah, there's some just useful links in case you're interested.
the project website, the paper and the code are all public.
And I'm happy to take any more questions.
Okay, so thanks for the good talk.
Really appreciate it.
Everyone had a good time here.
So I think we are like near the class limit.
So usually I have like a round of reference five questions for the speaker that is like the students usually know.
But if someone is afraid, you can just ask general questions first before we stop the recording.
So if anyone wants to leave earlier at this time, just feel free to ask your questions.
Otherwise, I would just continue on.
So what do you think is the future of transformers in RL?
Do you think they will take over like, they've already taken over language and vision?
So do you think for model base and model feed learning, do you think you'll see a lot more transformers pop up in RL literature?
Yes, I think we'll see a flurry of work.
If not already, we have good.
There's so many works using transformers at this year's ICLR conference.
Having said that, I feel that an important piece of the puzzle that needs to be solved is expiration.
It's non -trivial And it will have to, my guess is that you will have to forego some of the advantages that I talked about for transformers in terms of loss functions to actually enable exploration.
So it remains to be seen whether those modified loss functions for exploration actually hurt performance significantly.
But as long as we cannot cross that bottleneck, I think it is, I'm not, I do not want to comment that this is indeed the future of RL.
Got it.
Also, like you think like something like - Can I just follow up question?
Sure.
I'm not sure I understood that point.
So you're saying that in order to apply transformers in RL to do exploration, there have to be particular loss functions and they're tricky for some reason?
Could you explain more like what are the modified loss functions and why do they seem tricky?
So essentially in exploration, you have to do the opposite of exploitation, which is non -aggravated.
And there is right now, no, nothing inbuilt in the transformer right now, which encourages that sort of random behavior where you seek out unfamiliar parts of the state space.
That is something which is inbuilt into traditional RL algorithms.
So you usually have some sort of entropy bonus to encourage exploration.
And those are the sort of modifications which one would also need to think about if one were to use this in transformers for online RL.
So what happens if somebody, I mean, just naively, suppose I have this exact same setup and the way that I sample the action is I sample epsilon greedily, or I create a Boltzmann distribution and I sample from that.
I mean, just what happens?
It seems that's what RL does.
So does what happens?
So RL does a little bit more than that.
It indeed does those kinds of things where it would change the distribution, for example, the post -mal distribution and sample from it.
But it's also, there are these, as I said, the devil lies in the detail.
It's also about how it controls that exploration component with the exploitation.
And it remains to be seen whether that is compatible or disinform transformers.
I don't want to jump the gun, but I would say it's, I mean, preliminary evidence suggests that it's not directly transferable, the exact same setup to the online case.
That's what we have found.
There has to be some adjustments to be made, which we are still figuring it out.
So the reason why I ask is, as you said, the devil's in the details.
And so if someone naively like me might just come along and try doing what's in RL, I want to hear more about this.
So you're saying that what works in RL may not work for decision transformers.
Can you tell us why, like what pathologies emerge?
What are those devils hiding in the details?
Also remind me like, sorry, we are like also over time, so I'll try not to hurry.
Feel free to like.
Also, I'll send an email and follow up.
But like to me, that's really exciting.
And I'm sure that it's tricky.
Yeah, I will just ask like two more questions and like you can finish after this.
So one is, like, I'll tell you think, like something like this in transformer is a way to solve the credit assignment problem in RL instead of using some sort of like this contactor.
Sorry, can you repeat the question?
Oh, sorry.
So I think usually in RL, we have to learn some sort of this contactor to encode the rewards to go something like that.
But it's in transformer, transformer is able to do this credit assignment without that.
So do you think like something like this book is like the way we should do it?
like we should, like instead of having some discount, try to directly predict rewards?
So I would go on to say that I feel that discount factor is an important consideration in general, and it's not incompatible with the slim transformers.
So basically what would change, and I think the code actually gives that functionality where the returns to go would be computed as the discounted sum of rewards.
Got it.
And so it is very much compatible.
So there are scenarios where our context length is not enough to actually capture the long -term behavior we really need for credit assignment.
Maybe traditional tricks that are used could be brought in back to solve those kinds of problems.
Got it.
Yeah.
I also thought like when I was reading the listening transformer work that the interesting thing is that you don't have a fixed So like a gamma is usually a hyper parameter, but like you don't have a fixed gamma.
So do you think like, can you also like learn this thing?
And could this also be like, can you have a different gamma for each time for something possibly?
That would be interesting actually.
I had not thought of that, but maybe learning to predict the distance factor could be another extension of this work.
Also, do you think like this, like this in transformer work is like, like is it compatible with Q -learning?
So if you have something like CQL, stuff like that, can you also implement those sort of loss functions on top of like this in a transformer?
So I think maybe I could imagine ways in which you could encode pessimism in here as well, which is key to how CQL works.
and actually most of often our algorithms work including the model -based ones.
Our focus here deliberately was to go after simplicity because we feel that part of the reason why our literature has been so scattered as well if you think about different sub -problems everyone tries to solve has been because everyone's tried to pick up on ideas which are very well suited for that narrow problem.
Like for example, you have whether you're doing offline or you're doing online or you're doing imitation, you're doing multitask and all these different variants.
And so by design, we did not want to incorporate exactly the components that exist in the current algorithms because then it just starts looking more like architecture change as opposed to a more conceptual change into thinking about RLS sequence modeling very generally.
Yeah, that sounds interesting.
So do you think like we can use some sort of like 3D learning objectives instead of supervised learning?
It's possible.
And maybe like I'm saying that for certain, like for online RL, it might be necessary.
Often RL, we were happy to see it was not necessary, but it remains to be seen more generally for a transformer model or any other model for that matter encompassing RL more broadly whether that becomes a necessity.
All right, yeah.
Well, thanks for your time.
This was great.
