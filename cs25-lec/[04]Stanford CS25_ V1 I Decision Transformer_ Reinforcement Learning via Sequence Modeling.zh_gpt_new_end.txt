所以，我很兴奋今天能谈谈我们最近在使用Transformer进行强化学习方面的工作。
而且这是与一群非常令人振奋的合作者共同完成的工作，其中大部分是在加州大学伯克利分校，还有一些在Facebook和谷歌。
我应该提一下，这项工作是由两位才华横溢的本科生Lily Chen和Kevin Blue领导的。
我很高兴能呈现我们的研究结果。
所以让我们试着激发一下为什么我们要关心这个问题。
我们在过去三四年中看到，自2017年引入以来，Transformer已经在人工智能的许多不同领域取得了主导地位。
我们看到它们在语言处理方面产生了巨大影响。
我们看到它们被用于视觉，在视觉Transformer中非常简单地使用。
它们正在尝试解决蛋白质折叠的问题，并且很快它们可能会通过自动生成代码来取代计算机科学家。
所以，随着所有这些进步，似乎我们正在更接近于拥有一个统一的决策模型，用于人工智能。
但人工智能不仅仅是关于具有感知能力，还要利用这些感知知识来做出决策。
这就是这次演讲要讨论的内容。
但在我真正思考我们将如何将这些模型用于决策之前，我想为什么我认为提出这个问题很重要给出一些动机。
所以与强化学习的模型不同，当我们看转向感知模态的transformer，就像我在前一张幻灯片中展示的那样，我们发现这些模型非常可扩展，并且具有非常稳定的训练动力学。
所以只要你有足够的计算资源并且有越来越多的可用数据，你可以训练越来越大的模型，而且你会看到损失在非常平滑的过程中显著减少。
整体的训练动力学非常稳定，这使得从业者和研究人员能够轻松构建这些模型并学习更丰富的分布。
正如我所说，迄今为止，所有这些进展都发生在感知领域。
我们在这次演讲中感兴趣的是思考如何从感知，观察图像、观察文本等各种感官信号，然后进入实际采取行动并使我们的代理在世界中执行有趣任务的领域。
在整个演讲中，我们应该思考为什么这个视角将使我们能够进行可扩展的学习，就像我在前一张幻灯片中展示的那样，同时也为整个过程带来稳定性。
所以，顺序决策是一个非常广泛的领域。
而我今天要专门关注的是通往顺序决策的一种方式，即强化学习。
那么，简要背景是什么，什么是强化学习？
我们有一个处于当前状态的代理，代理将通过采取行动与环境进行交互。
通过采取这些行动，环境将为代理返回一个奖励，表明该行动有多好，以及代理将转移到的下一个状态，整个反馈循环将继续进行。
智能代理的目标是使用试错的方法，尝试不同的行动，查看奖励将导致什么结果，学习一个策略，将你的状态映射到行动，使得该策略在一段时间内最大化代理的累积奖励。
因此，你采取一系列的行动，然后根据你为该系列行动积累的奖励来判断你的策略有多好。
这次讲座还将专门关注一种名为离线强化学习的强化学习形式。
所以这里的想法是，与我之前谈论在线强化学习的情况不同的是，这里，现在不是主动与环境互动，而是有一系列交互的日志数据。
所以想象一下，有些机器人出去在田野上，它收集了大量的感官数据，你们都把它记录下来了。
利用这些日志数据，现在你想要训练另一个智能体。
可以是另一个机器人，然后通过查看日志数据来学习有关该环境的一些有趣信息。
所以现在没有试错组件，这是目前这个框架的一个扩展，这将是非常令人兴奋的。
所以我会在演讲结束时谈论这个，为什么考虑如何将这个框架扩展到包括一个探索组件并进行试错是令人兴奋的。
好的，现在更具体地讨论这次演讲的动机挑战，现在我们已经介绍了RL。
所以让我们看一些统计数据。
所以大型语言模型有数十亿个参数，今天的Transformer大约有100层。
它们非常稳定，可以使用监督学习风格的损失进行训练，这是自回归生成的构建模块，例如，或者作为输入的大规模语言建模。
这是一个每天都在增长的领域。
斯坦福有一门课程，我们都在上，因为它对人工智能产生了如此巨大的影响。
另一方面，RL 策略，我说的是深度 RL，它们最多可能扩展到数百万个参数或训练层。
真正令人不安的是，它们非常不稳定。
因此，当前的强化学习算法主要是基于动态规划，涉及解决一个非常不稳定的内循环优化问题。
在强化学习中，很常见看到奖励曲线是这样的。
所以我真正想让你看到的是我们在强化学习中倾向于得到的回报的方差。
这真的非常巨大，即使进行了多轮实验后。
这就是代码是如何完成的，我们的算法单独注入需要更好的改进，以便性能可以被代理在复杂环境中稳定地实现。
所以这项工作希望做的是介绍transformer。
我先用一张幻灯片展示这个模型的具体样子，然后我们将深入讨论每个组件的细节。
我有一个问题。
是的，我能快速问一个问题吗？
可以。
谢谢。
我想知道的是，为什么强化学习通常参数要少几个数量级？
这是一个很好的问题。
通常情况下，当你考虑深度强化学习算法，特别是在深度强化学习中，所使用的最常见的算法，例如，会有不同的网络扮演不同的角色来完成任务。
比如说，你会有一个网络扮演演员的角色。
所以它正在尝试找出一个策略，然后会有一个不同的网络扮演评论家的角色。
这些网络是根据自适应收集的数据进行训练的。
所以不像感知，你会有一个大量的交互数据集来训练你的模型。
在这种情况下，体系结构甚至在某种程度上环境都非常简单，因为我们试图训练非常小的组件，我们正在训练的函数，然后将它们全部组合在一起。
而且，这些函数通常在不是非常复杂的环境中进行训练。
所以这是不同问题的混合。
我不会说这纯粹是学习目标有问题，而是环境的使用，每个神经网络预测的目标的组合导致网络比我们目前看到的要大得多，往往会过拟合。
这就是为什么在强化学习中经常看到使用较少层的神经网络，而在感知中则相反。
谢谢。
你想问个问题吗？
是的。
是的，我正要问。
选择离线强化学习而不是在线强化学习是否有原因？
这是另一个很好的问题。
所以问题是为什么选择离线强化学习而不是在线强化学习？
简单的原因是因为这是第一项尝试研究强化学习的工作。
因此，离线强化学习避免了探索的问题。
给定大量的互动数据集。
不允许进一步与环境互动。
所以仅仅从这个数据集中，你正在尝试揭示出最优代理会是什么样子的政策。
所以它会 - 对，但是如果你进行在线强化学习，那岂不是就给了你这个探索的机会？
会的，确实会。
它还会做一件事，这在技术上是具有挑战性的，即探索会更难以编码。
所以离线强化学习是第一步。
我们没有理由不研究为什么在线强化学习不可行。
只是它提供了一个更为固定的设置，其中来自transformer的思想将直接推广。
好的，听起来不错。
所以让我们来看看模型，它的设计目的是非常简单的。
所以我们要做的是查看我们的离线数据，它基本上是以轨迹的形式存在的。
离线数据看起来像是一系列的状态、动作、多个时间步返回。
这是一个序列，因此将其直接作为输入馈送给transformer是很自然的想法。
在这种情况下，我们使用因果transformer，因为它在GPT中很常见。
所以我们从左到右进行。
而因为这个数据集带有时间步的概念，这里的因果关系比通常用于感知的一般含义更加切实可行。
这确实是因果关系，从时间的角度来看应该是这样的。
我们从这个转换器中预测出来的是在序列中在该标记之前的所有内容条件下的动作。
所以如果你想预测在这个 t 减 1 步的动作，我们将使用 t 减 2 步时的所有内容，以及 t 减 1 时的回报和状态。
那么我们将详细介绍每个是如何被编码的。
但基本上，这是一种单行的表达。
它正在将离线数据的轨迹数据作为一系列标记处理，通过因果transformer传递，得到一系列动作作为输出。
好的，那么我们究竟是如何通过网络进行前向传播的呢？
这项工作的一个重要方面是使用状态动作和这个被称为回报的数量。
所以这些不是直接的奖励。
这些是回报，让我们看看它们真正的含义。
这是作为输入传递给这个数字的轨迹。
而返回去的奖励是从当前时间步开始直到整个episode结束的奖励总和。
所以，实际上我们希望transformer在使用目标返回时变得更好。
这就是你应该把返回作为决定采取什么行动的输入的方式。
这种观点将会有多重优势。
它将允许我们实际上做比离线RL更多的事情，并且通过只改变返回来泛化到不同的任务。
这在这里非常重要。
所以，在时间步骤一，我们将仅有整个轨迹的奖励总和。
在时间步骤二，我们减去我们通过采取第一个行动得到的奖励，然后得到剩余轨迹的奖励总和。
好的，这就是我们称之为返回的方式。
就像在你设置的开头目标返回中，你需要获得多少更多的累积奖励来实现你的奖励目标。
输出是什么？
输出是预测行动的序列。
所以正如我在前一张幻灯片中展示的，我们使用因果transformer，因此我们将按顺序预测所需的行动。
在transformer内部计算的注意力将考虑一个重要的超参数K，它是上下文长度。
我们在这里以及接下来的讨论中看到，我将使用符号K来表示我们将关注多少个过去的标记来预测动作和当前时间步。
好的，所以再深入挖掘代码时，有一些微妙的差别，决策transformer的操作方式与普通transformer有所不同。
首先是，这里，时间步的概念会具有更大的语义，延伸到三个标记。
因此，在感知中，你只需要考虑单词的时间步，例如，对于NLP或者视觉的每个patch。
在这种情况下，我们将有一个时间步来封装三个标记，一个用于状态，一个用于动作，一个用于奖励。
然后，我们将嵌入每个标记，然后像transformer中一样添加污染嵌入。
然后，我们将这些输入馈送到transformer中。
在输出时，在这种默认设置中，我们只关心这三个标记中的一个。
我将展示一些实验，即使其他令牌可能成为目标预测的兴趣点，但现在让我们保持简单。
我们想要学习一个策略。
策略就是试图预测行动。
所以当我们尝试解码时，我们只会从预终隐藏表示中查看行动。
好的，所以这是前向传递。
现在，我们用这个网络做什么？
我们训练它。
我们如何训练它？
抱歉，在语义上有个快速问题。
如果你回到前一页，这种情况下的加号，语法表示你实际上是逐元素地相加而不是串联它们。
对吗？
是的，没错。
好的，抱歉要确认一下。
好的，那么最后一个函数是什么？
就此进行跟进。
我以为是串联。
为什么我们只是相加呢？
抱歉，你能回去吗？
是的，我觉得这是一个设计选择。
你可以串联，也可以相加。
这会导致不同的函数被编码。
在我们的情况下，是加法。
好的，你试过另一个，它就不起作用了吗？
或者为什么是这样呢？
因为我觉得直观地进行连接会更有意义。
所以我认为它们对于功能编码有不同的用途。
其中一个确实混合了状态的嵌入，并且基本上对其进行了偏移。
所以当你添加一些东西时，如果你把状态的嵌入想象成一个向量，你实际上是在对其进行偏移。
而在连接的情况下，你实际上是增加了这个空间的维度。
所以这些都是不同的选择，它们做着非常不同的事情。
我们发现这个效果更好。
我不确定我是否记得如果你连接它们，结果是否会有很大的不同，但这是我们使用的方法。
但是不是会有一个问题，因为如果你在偏移它，对吧，如果你为某个状态有一个嵌入，然后做了某些操作，最后又回到了相同的状态，你希望这些嵌入是相同的。
然而，现在你处于不同的时间步。
所以你对其进行了偏移。
那么这不会更难学习吗？
所以在这里有一个更大更有趣的问题，就是你说的基本上是，我们是否失去了马尔可夫属性？
因为正如你所说的，如果我们在不同的时间步骤回到相同的状态，我们不应该执行类似的操作吗？
而这里的答案是肯定的，实际上我们是非马尔可夫的。
这一点可能一开始看起来非常不直观，为什么非马尔可夫性在这里很重要呢？
我想引用另一篇与此密切相关的论文，试图更详细地解释其中的内容。
基本上是在说，如果你试图预测转移动态，那么你实际上可以在这里构建一个马尔可夫系统，它也能做得同样好。
然而，从实际预测行动的角度来看，看之前的时间步骤确实有所帮助，尤其是当你有缺失的观测时。
比如，如果你的观测是真实状态的子集。
所以查看之前的状态和行动有助于你更好地填补某种意义上的空白。
这通常被称为部分可观测性，通过查看先前的标记，你可以更好地预测你应该在当前时间步骤采取的行动。
所以非马尔可夫性是故意的，而且并不直观，但我认为这是区别于现有框架的一点。
所以基本上会帮助你，因为像强化学习通常在无限的、更适合无限时间跨度的问题上运作，所以从技术上讲，你想要根据历史情况采取不同的行动，因为现在你必须根据不同的时间戳采取行动。
是的，是的。
所以，如果你想要处理无限时间跨度的问题，也许像折现这样的东西能够同样有效地产生效果。
在这种情况下，我们使用了一个折现因子为1，或者基本上没有折现。
但你是对的。
如果我认为我们真的想要将其扩展到无限时间跨度，我们就需要改变折现因子。
谢谢。
好的，所以。
快速问题。
我认为刚才在聊天中已经回答了。
但我还是会问一下。
我想我可能错过了这个，或者你可能正要谈论它。
收集的离线数据，用什么策略来收集的？
所以这是一个非常重要的问题，而且这是我在实验中提到的事情。
所以我们使用的是离线强化学习的基准，基本上这些基准的构建方式是你使用在线强化学习训练一个代理，然后在某个时间步查看它的重放缓冲。
所以在它训练的过程中，就像是一个中等水平的专家，你收集它到目前为止经历过的转换，并将其作为离线数据。
这是我们的框架对使用的离线数据非常不可知的东西。
所以我到目前为止并没有讨论它，但在我们的实验中是基于传统的基准。
明白了。
所以我问的原因并不是，我相信你的框架可以适应任何离线数据，但在我看来，你即将呈现的结果似乎很大程度上取决于数据收集策略是什么。
确实，确实。
而且，我们将展示一个实验，显示数据量在我们与基线比较时会产生差异。
基本上，我们将看到这在transformer中特别在有少量离线数据时表现出色。
好的，很酷。
谢谢。
好的，很棒的问题。
那么让我们继续。
我们已经定义了我们的模型，它将会观察这些悲剧性的树。
现在让我们看看如何训练它。
非常简单，我们试图预测行动。
我们试图将它们与我们的错误中的那些匹配，如果它们是离散的，我们可以使用交叉熵。
但是这里有一些对我们的研究非常深刻的东西，那就是这些目标非常稳定，易于正则化，因为它们已经被开发用于监督学习。
相比之下，R更多地用于动态规划风格的目标，这些目标基于贝尔曼方程，这些目标最终变得更难优化和扩展，这也是你在结果中看到很多方差的原因。
好的，这就是我们训练模型的方式。
现在，我们如何使用这个模型呢？
这就是试图为模型执行预测的重点。
因此，在这里，这将类似于进行自回归生成。
这里有一个重要的标记，那就是回报。
在评估过程中，我们需要设置什么，想必我们希望达到出口级别的性能，因为那将具有最高的回报。
所以我们将初始回报设置为前进，不是基于我们的轨迹，因为现在我们没有轨迹。
你将生成一个轨迹。
所以这是在推理时期。
所以我们将其设置为导出的回报，例如。
所以编码一下整个过程会是什么样子，基本上你将这个回报设置为某个目标回报，然后将你的初始状态设置为从环境初始状态的分布中运行，然后你只是执行你的决策transformer.
所以你得到一个新的动作。
这个动作还会给你一个来自环境的状态和奖励。
你将它们附加到你的序列中，然后你得到一个新的回报，然后你只取上下文和关键因为这是transformer用于进行预测的内容，然后将其馈送回到远程transformer。
所以这是常规的自回归生成，但唯一需要注意的关键点是如何初始化RL的transformer。
抱歉，在这里我有一个问题。
那么导出目标的选择有多重要？
它必须是像专家回报的平均值，还是可以是部门中可能的最大回报？
像数字的选择真的重要吗？
这是一个非常好的问题。
所以通常我们会把它设置得比数据集中的最大回报稍高一些。
所以我认为我们使用的因子是1.1倍，但我认为我们在这个范围内进行了大量的实验，而且对于你选择的任何选择都相当健壮。
例如，对于专家级别的 Hopper 回报约为 3600，我们发现从 3500、3600 到甚至很高的数字如 5000，它都能稳定地工作。
但是，我想指出的是，这在常规 RL 中通常是不需要的，比如了解专家回报。
在这里，我们实际上是超越了常规的 RL，我们可以选择我们想要的回报。
所以我们实际上也需要这个有关测试中专家回报是多少的信息。
还有另一个问题。
是的。
所以，你只是有点超越了常规的 IRE，但我很好奇，你是否也将这个框架限制在只有离线 IRE？
因为如果你想要在在线 IRE 中运行这种框架，你必须事先确定回报。
所以我认为这种框架有点局限于只有离线 IRE。
你认为呢？
是的，我认为早些时候问这个问题也是很重要的，是的，我认为目前，这是第一组。
所以我们专注于离线强化学习，从离线数据集中可以获取这些信息。
可以考虑一下如何在线获取这些信息的策略。
你需要的是一个课程表。
所以在训练早期，当你收集数据时，你会设置，当你进行rollouts时，你会将专家的回报设置为数据集中所见的任何内容。
然后，随着你开始发现transformer实际上可以超过这个性能，就会逐步增加。
所以你可以考虑从低到高指定一个课程表，用于决策transformer的专家回报是什么。
我明白了。
好的。
谢谢。
是的，这是关于模型的。
我们讨论了这个模型是如何的，这个模型的输入是什么，输出是什么，用于训练这个模型的损失函数是什么，以及我们如何在测试时使用这个模型。
这个框架与通常被称为RL的概率推断有一种联系。
所以我们可以将强化学习表述为一个图模型问题，其中状态和动作被用来确定下一个状态。
为了编码优化的概念，通常你还会有一些额外的辅助变量，比如O1，O2等等，它们隐含地表示编码了某种奖励的概念。
在这个优化的条件下，强化学习是学习策略的任务，该策略是从状态到动作的映射，使我们获得最优行为。
如果你真的眯起眼睛看，你会发现这些优化变量和决策转换器实际上是通过回报进行编码的。
所以，如果在测试时我们给出一个足够高的值，比如专家回报，我们实际上是在说，在这个数学形式的优化量化条件下，展开你的决策转换器，希望满足这个条件。
所以是的，这就是我想谈论的模型的全部内容。
你能解释一下吗？
你说的优化变量在决策转换器中是什么意思，你是怎么理解回报的？
好的。
所以优化变量，我们可以在最简单的情境中看作是立法性或二进制的。
所以一个是如果你解决了目标，零就是如果你没有解决目标。
在这种情况下，你也可以把你的决策转换器想象成在测试时，我们对回报进行编码，我们可以将其设置为一，这基本上意味着在最优条件下。
所以这里的最优性意味着解决目标为一。
生成一个动作序列，使得这是真的。
当然，我们的学习并不完美，所以不能保证我们会得到这个结果，但我们已经以一种方式训练了transformer来解释回报，将其视为某种最优性的概念。
所以，如果我理解正确的话，这大致上就像是说，展示给我一个最优转换序列的样子，因为你的模型已经学会了成功和不成功的转换。
确切地说。
确切地说。
正如我们将在一些实验中看到的那样，我们可以...
对于二元情况，要么是最优的，要么是非最优的，但在我们的实验中，这很少是一个连续的变量。
所以我们也可以实验性地看看中间发生了什么。
好的。
那么让我们开始实验吧。
所以有很多实验，我挑选了一些我认为很有趣并在论文中给出了关键结果，但请随时参考论文，以获取有关我们模型的某些组件的更详细分析。
首先，我们可以看一下它在离线强化学习上的表现如何。
因此，有Atari套件和OpenAI gym的基准。
而我们还有另一个环境，这个环境特别困难，因为它包含稀疏奖励，并要求进行我稍后将要讨论的学分分配。
但总体而言，我们看到这个transformer在与现有最先进的无模型离线强化学习方法竞争时表现出色。
在这种情况下，这是为离线强化学习设计的Q-learning的一个版本，在存在长期配对分配的情况下，它表现出色，而传统的基于TD学习的方法会失败。
是的，所以这里的要点不应该是我们已经到了可以只用决策transformer替代现有算法的阶段，而是这是一个非常有利的证据，证明建立在transformers基础上的这种范式将使我们能够更好地迭代和改进模型，希望能够全面超越现有的算法。
在艰难的环境中已经有一些早期证据表明，这些环境确实需要进行长期的信用分配。
我可以在这里问一个关于基线的问题吗，具体是关于 TD 学习吗？
我很好奇，因为我知道很多 TD 学习代理都是前馈网络。
这些基线，它们有循环吗？
是的，是的。
所以我认为这里的保守两种学习基线或者说确实有循环，但我不是很确定。
所以我可以离线检查一下然后再回复你。
好的。
我们会没事的。
谢谢。
还有一个快速的问题。
那么在实验中，你究竟如何评估这里的决策transformer呢？
因为你需要提供返回值。
那么你是否使用最优的策略来获得最优的奖励并加以加速？
是的。
所以这里我们基本上将离线数据设置为有用的训练，并且我们将离线数据中的最大回报设定为稍高于所需的目标回报。
所以使用的系数是 1.1。
我明白了。
关于性能，抱歉，我对那个RLS并不是很了解，但这里的性能是如何定义的呢？
就像是你实际从中获得多少奖励吗？
是的，是的。
所以你可以指定一个目标回报，但不能保证你采取的实际行动会达到那个回报。
所以你根据那个来测量真实的环境回报。
是的，我明白了。
但是只是好奇，这些性能是你从实际环境中恢复多少奖励的百分比，对吗？
是的，这些不是百分比，而是一种将回报标准化的方式，使一切都在三到一百之间。
是的，我明白了，那么我就想知道，你是否大致了解决策转换器实际上恢复了多少奖励，比如说，如果你指定有时候要？
这是一个很好的问题，也是我的下一个幻灯片。
我明白了。
谢谢。
所以在这里，我们将回答这个问题，即如果输入目标回报，它可以被导出，也可能不导出。
模型在实现这一目标方面表现如何？
所以X轴是我们指定的目标回报。
而Y轴基本上是我们实际获得的回报。
作为参考，我们有这条绿线，这是预言。
这意味着无论您希望系统转换器给您什么，它都会给您。
所以这本来是理想的情况。
所以这是一个对角线。
由于离线RL，我们还有橙色，这是数据集中最佳的轨迹。
所以离线数据并不完美。
所以我们只是绘制离线数据性能的上限。
在这里，我们发现对于大多数环境，我们输入的目标回报与模型的实际性能之间有很好的匹配。
我希望您从幻灯片中得出一些其他观察结果，因为我们可以变化奖励的概念，所以在某种程度上我们可以通过回报条件来进行多任务RL。
这不是进行多任务RL的唯一方法。
您可以通过自然语言指定任务，您可以通过目标状态等方式，但这是一个概念，其中任务的概念可能是您想要多少奖励。
还有一件事要注意的是有时最小的外推。
这并不是我们一直看到的趋势，但我们确实看到了一些迹象。
所以，如果你看一下，比如说SeaQuest，在这里，数据集中最高回报轨迹相当低。
如果我们为我们的决策转换器指定一个比那更高的回报，我们确实发现模型能够实现。
因此，它能够生成比数据集中看到的回报更高的轨迹。
我相信在这个领域改进这个模型的未来工作应该思考强度如何能够在不同环境中更一致，因为这确实会实现离线RL的目标，即在给定次优行为的情况下，如何使其达到最佳行为？
如何使强度在不同环境中保持一致，这还有待观察。
我能插个问题吗？
可以。
我认为最后一点非常有趣，而且你们偶尔会看到这个很酷。
我想知道会发生什么。
所以这都是条件。
你输入你想要的回报，然后它尝试选择一系列行动来实现它。
我很好奇如果你给它荒谬的输入会发生什么。
比如说，这里的回报数量级大概是50到100。
如果你输入10,000会发生什么呢？
好问题。
这是我们早期尝试过的事情。
我不想说我们试过10,000，但我们尝试了非常高的回报，甚至连专家都无法获得。
通常我们会看到性能趋于平稳。
所以你也可以在半猎豹和乒乓球中看到这种迹象，或者在某种程度上在行走者中看到。
如果你看得很仔细，事物开始饱和。
所以如果你超过了某个特定的阈值，这个阈值通常对应着最佳轨迹的阈值，但并不总是。
在那之后，所有的回报都是相似的。
至少有一件好事是它的性能不会降低。
如果你指定了一个10,000的回报，并且它给你一个20或者其他非常低的回报，那会有点令人担忧。
它稳定下来是件好事，但并不是说它会一直增加下去。
性能会饱和到一定程度。
好的，谢谢。
我只是好奇，通常来说，对于迁移模型，你需要大量的数据。
那么你知道你需要多少数据吗？
它是如何缩放数据、性能的？
这是一个transformer？
是的，所以在这里我们使用标准数据，比如D4 RL基准用于Mijuku，我认为有数百万次过渡。
明白了。
对于Atari，我们使用了回放缓冲区的1%，这比我们用于MuJoCo基准的要小。
而且我在接下来的幻灯片中有一个结果，显示了在transformer中这一点，特别是在你拥有少量数据时特别有用。
所以，我想在最后一张幻灯片之前再问一个问题，你在多任务部分再次提到的返回调节是什么意思？
是的，所以如果你考虑这个时间的前进回报，你必须将其作为起始标记输入，这是指定你想要的策略的一种方式。
这怎么是多任务？
所以这是多任务的，因为通过改变目标返回值，你可以获得不同的策略，你实际上得到了不同的编码行为。
所以想想，例如，一个hopper，你指定了一个非常低的返回值。
所以你基本上是在说，给我一个代理，它将只停留在其初始状态，不进入未知的领域。
未知的领域，如果你给出非常非常高的话，那么你就是要求它执行传统的任务，也就是跳跃并尽可能远地前进而不摔倒。
你能否将这些多任务分类，因为基本上这意味着你的回报条件是它记忆的一个提示，这通常是多任务的一个缺陷。
所以我不确定这是否是一个任务标识符。
这就是我想说的。
所以我不确定这是否是记忆，因为我认为，我是说，拥有一个固定的离线数据集基本上意味着它非常非常特定，如果你有相同的起始状态，并且你采取相同的行动，并且你有相同的目标回报，那就可以算作是记忆。
但是在测试时，我们允许所有这些事情发生变化，事实上它们确实发生了变化。
因此，你的初始状态可能是不同的，你的目标回报可能是一个不同的标量，而不是你在训练过程中看到的一个。
因此，基本上模型必须学会生成那种行为，从一个不同的初始状态开始，也许目标回报的值与它在训练过程中看到的值不同。
如果动力学是随机的，那么即使你记住了动作，也不能保证得到相同的下一个状态。
所以，如果动力学也是随机的，你的表现就会有很差的相关性。
另外，我很好奇，训练这个transformer通常需要多长时间？
通常需要几个小时。
所以，大约需要四到五个小时，这取决于你使用的GPU的质量。
但是，这是一个合理的估计。
好的，知道了，谢谢。
好的，实际上，在进行这个实验、这个项目时，我们想到了一个基线，我们很惊讶的是，在以前的离线RL文献中没有这个基线，但这个基线非常有意义。
我们认为我们也应该思考一下，决策transformer是否实际上在做与该基线非常相似的事情。
而这个基线就是我们所谓的人的行为克隆。
所以行为克隆，它的作用基本上是忽略回报，只是通过尝试映射给定当前状态的动作来模仿智能体。
对于具有低回报和高回报的项目树都存在的离线数据集来说，这并不是一个好主意。
因此，传统的行为克隆，在离线强化学习方法中通常被视为基准。
而且，除非你有一个非常高质量的数据集，否则它并不是离线强化学习的一个很好的基准。
然而，有一个版本我们称之为BC，实际上相当有道理。
在这个版本中，我们从离线数据集中筛选出顶部轨迹。
一站式，即具有最高奖励的那些。
你知道每个转换的奖励。
你计算轨迹的回报，然后选择具有最高回报的轨迹，并保留其中的一定百分比，这将在这里成为超参数。
一旦你保留了这些顶级轨迹的一部分，你只需要求你的模型模仿它们。
因此，模仿学习也使用，特别是当它以行为克隆的形式使用时，它基本上使用监督学习，这是一个监督学习问题。
因此，如果你进行了这个筛选步骤，你实际上也可以得到监督学习的目标函数。
实际上，我们发现对于中等和高数据范围，下降transformer实际上与后NVC非常相似。
所以这是一个非常强大的基准，我认为未来离线强化学习中的所有工作都应该包括这个。
实际上，上周有一篇ICLR的投稿，对我们在这篇论文中介绍的这个基准进行了更详细的分析。
我们发现，在数据稀缺的情况下，下降transformer的性能要比个体行为克隆好得多。
这是针对Atari基准的，正如我先前提到的，与MuJoCo环境相比，我们拥有一个更小的数据集。
在这里，我们发现即使在不同的百分比超参数下变化，我们通常也无法获得决策transformer所获得的强大性能。
所以10% BC基本上意味着我们过滤掉并保留前10%的轨迹。
如果降低甚至更多，那么起始数据集将变得非常小，因此基准将变得无意义。
但即使在合理的范围内，我们也从未发现性能能与Atari基准的相同transformer相匹配。
你有什么问题吗？
所以我注意到在表三中，例如，这不是与论文中刚刚前面的那个表相同的表，有一个关于CQL性能的报告，对我来说在直觉上与百分比BC的感觉非常相似，就是你选择那些你知道表现良好的轨迹，然后尽量保持大致在相同的策略分布和状态空间分布中。
我对这个很好奇，你对这个CQL的性能相对于这里的百分比BC的性能有什么感觉吗？
所以这是一个很好的问题。
所以问题是，即使对于CQL，你也依赖于这种悲观的观念，你想选择你更有信心的轨迹，并努力确保策略保持在那个区域内。
所以我没有这个表上CQL的数据，但如果你看一下Atari的详细结果，我认为他们应该有SQL，因为这是我们在这里报告的数字。
所以我可以告诉你SQL的性能实际上相当不错。
而且在Atari上它与决策Transformer的性能相当竞争。
所以这里的TD学习基线就是SQL。
因此，自然而然地，我想它比post MDC做得更好。
如果这个被提到过，我道歉，我可能是错过了。
但你是否觉得这基本上就像是 CQL 无法很好地进行外推或者将不同轨迹的不同部分拼接在一起的失败，而决策 Transformer 可以在你拥有一个轨迹的前半段非常好、第二个轨迹的后半段非常好之间进行外推。
所以你实际上可以用决策 Transformer 将它们拼接在一起，而使用 CQL 你可能无法做到这一点，因为连接它们的路径可能不被行为策略充分覆盖。
是的，是的。
所以这实际上涉及到我没有过多强调的一种直觉，但我们在那篇论文中讨论了这个问题，基本上我们为什么期望一个 Transformer 或者任何模型来查看次优数据并得到一些东西，一个生成最佳结果的策略。
直觉是，正如 Scott 所提到的，你或许可以从次优轨迹中拼接出好的行为，而这种拼接或许可以导致一个行为，该行为比数据集中单个轨迹中所见到的任何行为都要好。
这是我们在一个小规模实验中找到的早期证据。
这也是我们的希望，因为transformer擅长处理非常长的序列。
因此，它可以识别那些行为片段，将它们拼接在一起可以给出最佳的行为。
因此，很有可能这是transformer独有的特点，而像CQL这样的方法可能无法做到，因为它在自动地过滤数据，并且受限于不能做到这一点，因为好的行为片段可能在整体上没有高回报。
但是如果你把它们过滤掉，你就会失去这些信息。
好的，所以我说有一个超参数，上下文和关键字，就像大多数感知问题一样，transformer相对于其他序列模型（如LSTM）的一个重要优势是它们可以处理非常大的序列。
乍一看，马尔可夫特性似乎对RL有所帮助，这也是之前提出的一个问题。
因此，我们进行了这个实验，比较了上下文和k等于一时的性能。
在这里，我们在环境中的上下文为30，而在Pong中为50。
我们发现增加上下文长度对于获得良好性能非常非常重要。
好的，现在到目前为止，我已经向你展示了transformer中的这个，这非常简单。我没有任何幻灯片详细介绍动态规划的细节，这是大多数强化学习的关键。
这只是在自回归框架中的纯监督学习，这为我们带来了良好的性能。
那么在这种方法实际上开始胜过一些传统方法的强化学习的情况下怎么办？
为了进一步探究一下，我们开始研究稀疏奖励环境。
基本上，我们只是采用了我们现有的MuJoCo环境，然后不是为每个转换提供奖励的信息，而是在轨迹结束时提供累积奖励。
因此，每个转换在每个时间步都会有零奖励，只有在最后一步才会一次性获得整个奖励。
因此，这是一个非常稀疏的奖励场景的原因。
在这里，我们发现与原始的密集结果相比，DT的延迟结果会稍微恶化，这是预期的，因为现在你在每个时间步都保留了一些更精细的信息，但与这里的原始DT性能相比，下降并不太显著。
就像CQL这样的情况，性能会急剧下降。
所以在稀疏奖励场景下，SQL遭受了相当大的损失，但是决策transformer却没有。
为了完整起见，您还可以查看行为克隆和人员行为克隆的性能，因为它们不考虑奖励信息，除非也许人员行为克隆只考虑用于预处理数据集的奖励信息，这些信息不知道环境是否具有稀疏奖励。
如果您正在进行在线RL，您会期望这种情况有所不同吗？
这种不同的直觉是什么？
四月，我会说不会，但也许我错过了这个问题背后的一个关键直觉。
我认为这是因为您像离线训练一样，对吧？
就像您的，下一个输入在某种意义上始终是正确的行动。
所以您不会偏离轨道，也不会偏离技术上的轨道，因为您只是不知道。
所以我可以理解在线会有一个非常艰难的，基本上是冷启动的情况，因为它只是不知道。
而且它只是在黑暗中进行，直到最终可能中大奖。
是的。
是的。
我想我同意。
这是一种很好的直觉，但是是的，我认为因为离线强化学习真的摆脱了其中的反复试验，对于稀疏奖励环境来说会更加困难。
所以在那里，DT性能的下降应该更加显著。
我不确定它与其他算法性能下降的比较如何。
但是似乎是一个有趣的设置来测试DTN。
嗯，也许我在这里是错的，但是我对决策transformer的理解也是在训练中你使用奖励进行调整，对吧？
因此，本质上对于每个从初始状态开始的轨迹，基于训练制度，模型能够知道最终结果是成功还是失败，对吧？
但这是决策transformer训练制度的独特之处。
而在CQL中，我的理解是它基于某种每个转换的训练制度。
因此，每个转换与最终奖励有些解耦。
这样说对吗？
是的。
虽然像一个困难，乍一看，我们有点想象不同的transformer可能会遇到的一个困难是，由于这是一种稀疏奖励情景，初始标记将在整个轨迹中保持不变。
除了最后一个标记突然下降到零的地方，这个标记始终保持不变。
但我认为你说得对，甚至在开始阶段，以一种看起来考虑到未来需要达到的奖励的方式来喂给它，也许是性能下降不明显的原因之一。
是的，我想，我猜这里一种消融实验是，如果你改变训练制度，使得只有最后一条轨迹有奖励。
但我在思考，这是否会被注意机制弥补，反之亦然，对吧？
如果你将奖励信息嵌入到CQL训练过程中，我很好奇那里会发生什么。
是的。
这是一个很好的经验。
好的，与此相关，还有另一个我们测试过的环境。
我在早期的幻灯片中给了你对结果的简要预览。
所以这被称为“关键-到-门”的环境。
它有三个阶段。
所以在第一阶段，代理被放置在一个有钥匙的房间里。
一个好的代理会拿起钥匙，然后在第二阶段，它会被放置在一个空房间里。
而在第三阶段，它将被放置在一个带门的房间里，在那里它将实际使用在第一阶段收集的钥匙来打开门。
所以基本上代理将会收到一个二进制奖励，表示它是否在第三阶段到达并打开了门，条件是它在第一阶段确实拿起了钥匙。
因此，这里有一个国家概念，即您希望将信用分配给发生在过去的事件。
所以如果您想要测试您的模型对长期信用分配的表现有多好，这是一个非常具有挑战性和明智的场景。
在这里，我们发现，所以我们对不同数量的项目树进行了测试。
所以这里是项目树的数量，基本上表示您实际上会看到这种行为有多频繁。
而 Listen transformer 和 人 - 行为克隆 这两个基线模型在这项任务上比其他模型做得更好。
还有一个相关的实验，也很有趣。
所以一般来说，许多其他算法都有一个演员和评论家的概念。
演员基本上是一个根据策略状态采取行动的人。
评论家基本上是评估这些行动在实现长期目标方面的优劣，以及回报和长期的累积总和。
这是一个良好的环境，因为我们可以看到，如果它被训练为评论家，远距离的transformer会表现得有多好。
所以在这里，我们所做的是，我们不是把行动作为输出目标，而是替换成了奖励？
所以这是完全可能的。
我们可以再次使用相同的颜色transformer机制，只看上一个时间步的转换，并尝试挑选奖励。
在这里，我们看到这个有趣的模式，在我们在那个开-关-门的环境中有的三个阶段中，我们确实看到奖励概率在我们预期的方式上发生了很大变化。
所以基本上有三种情景，第一种情景，让我们看看Bloom，在这种情况下，代理在第一阶段没有拾取钥匙。
所以奖励概率，它们最初都差不多，但当变得明显代理不会拾取钥匙时，奖励开始下降。
然后在整个剧集中它基本保持接近零，因为你未来阶段不可能有打开门的钥匙。
如果你拿起钥匙，有两种可能性，这两种可能性在第二阶段基本相同，在那里你有一个空房间，只是一个分散注意力的东西，使得整个剧集变得很长。
但在最后，这两种可能性是你拿起钥匙并且真正到达门口，这是我们在这里看到的橙色和棕色的那种情况，你会看到奖励概率上升，还有另一种可能性是你拿起钥匙但没有到达门口，这种情况下，你会再次看到预测的奖励概率开始下降。
所以从这个实验中得出的结论是，这些transformer不仅是出色的演员，这是我们从优化策略的结果中迄今所看到的，而且它们在完成这个长期的任务分配方面也是非常出色的评论家，其中奖励也非常稀疏。
所以只是为了确保正确，你是在预测每个时间步的奖励走向，还是你正在预测每个时间步的奖励？
所以这是奖励走向，我也可以检查。
在这个特定的实验中，我的印象是预测奖励的去向和实际奖励之间并没有真正区别，但我认为我们应该去尝试这个。
嗯，我很好奇，那么你是如何得到奖励的概率分布的呢？
是像，你只是评估了许多不同的情节然后找到了奖励，还是你在明确地预测某种分布？
哦，所以这是一个二元奖励，所以你可以有一个概率性的结果。
还有其他问题吗？
是的，通常我们会称类似于预测状态值或状态实际值的东西为批评者。
但在这种情况下，你要求决策transformer只预测奖励。
那么为什么你仍然要称它为批评者呢？
所以我觉得这里的类比变得有点清晰了，与未来的回报相比。
就像你考虑未来的回报一样，它真的捕捉到了你想要看到未来奖励的本质。
哦，我明白了。
所以你的意思是它只会预测返回而不是单步奖励，对吧？
是的，对的。
好的，所以如果我们要预测前往的回报，这对我来说有点违反直觉，因为在第一阶段当代理人还在K房间时，我认为它应该有一个很高的前往回报，如果它在K之后，但在绘图中，你不在K房间，代理人拾取了K，而没有拾取K的代理人有相同水平的前往回报。
所以这对我来说很反直觉。
我认为这反映了一个好的特性，就是你的分布，如果你正确解释前往回报，在第一阶段，你不知道这三种结果中的哪一个是真正可能发生的。
而且第一阶段我也是在说基本上的一开始。
慢慢地你会了解，但本质上在第一阶段，如果你把前往回报看作是一或零，这三种可能性都是同等可能的。
而且这三种可能性，所以如果我们试图评估这些可能性的预测奖励，它不应该是相同的，因为我们真的还不知道第三阶段会发生什么。
抱歉，这是我的错，因为我之前以为绿线是没有拾取K的代理人，但事实证明蓝线才是没有拾取K的代理人。
所以是的，这是我的错。
这对我来说是有道理的。
谢谢。
另外，我从论文中不是很清楚，但你们是否进行了实验，预测动作和奖励应该是什么？
如果你同时做这两件事，性能会有什么不同吗？
所以实际上我们做了一些初步实验，但这对我们帮助不大。
然而，我确实想再次推荐一篇同时期的论文，轨迹transformer，它试图预测状态、动作和奖励，实际上是全部三个。
他们处于一个基于模型的设置中，尝试学习每个组件，如过渡动力学、策略，甚至可能是他们设置中的评论家，也是有意义的。
我们没有发现任何显著的改进。
所以为了简单起见，保持它模型自由，我们没有尝试一起预测它们。
好的，所以总结，我们在transformer中展示了这一点，这是尝试基于序列建模来接近RL的第一个词。
我们之前方法的主要优势是它的设计简单。
希望是对于他们的扩展，我们会发现它比现有算法扩展得更好。
由于我们使用的损失函数经过测试，而我在研究和感知中进行了大量交易，因此训练是稳定的。
并且在将来，我们还希望由于与体系结构的相似性以及与感知为基础的任务训练的相似性，将它们整合到这个循环中也将变得更加容易。
因此，状态、动作，甚至感兴趣的任务，都可以基于感知的感觉进行指定，因此你可以通过自然语言指令指定目标任务。
由于这些模型可以很好地处理这些输入，希望是它们能够融入决策过程。
而且从经验上看，在离线RL设置的范围内表现强劲，特别是在需要进行长期信用分配的情景中表现出色。
所以未来还有很多工作要做。
这绝对不是终点。
这是重新思考我们如何构建能够扩展和泛化的RL代理的初步工作。
我挑选出的一些事物，我认为扩展它们会非常令人兴奋。
第一点是多模态性。
所以，我们追求这些模型的一个重要动机之一是，我们可以结合不同类型的输入，无论是在线还是离线，真正构建像人类一样工作的决策代理。
我们处理周围不同形式的许多输入，并对它们采取行动。
因此，我们做出决策，我们希望人工智能代理也能做出相同的决策。
也许这在transformer中是朝着这个目标迈出的一大步。
多任务处理，所以我在这里描述了一种非常有限的多任务处理形式，它基于期望的回报，但它可以更丰富，可以指定一个命令来控制机器人或者一个期望的目标状态，比如，甚至是视觉上的。
因此，尝试更好地探索该模型的不同多任务处理能力也将是一个有趣的扩展。
最后，多智能体。
作为人类，我们从不孤立行动。
我们总是在涉及许多其他智能体的环境中行动。
在这些场景中，事物变得部分可观察，这发挥了transformer在设计上是非马尔可夫的优势。
所以我认为有很大的可能性去探索甚至是多智体场景，在那里，transformer可以处理非常大的序列，与现有算法相比，这一事实可能会再次有助于构建环境中其他代理的更好模型并行动。
所以，是的，如果你感兴趣的话，这里有一些有用的链接。
项目网站、论文和代码都是公开的。
我很乐意回答更多的问题。
好的，感谢你的精彩发言。
非常感谢。
大家在这里度过了愉快的时光。
所以我认为我们接近班级的上限了。
通常我会给发言者提出大约五个问题，这是学生们通常知道的。
但如果有人害怕，你可以先问一般性的问题，然后我们停止录制。
所以如果有人想在这个时候提前离开，随时可以提出你的问题。
否则，我会继续下去。
那么你认为transformer在RL中的未来是什么？
你认为它们会像已经占领了语言和视觉一样占领RL吗？
那么你认为模型基和模型喂养学习，你认为你会看到更多的transformer出现在 RL 文献中吗？
是的，我认为我们会看到大量的工作。
如果还没有，我们有很好的。
在今年的 ICLR 会议上有很多使用transformer的作品。
话虽如此，我觉得需要解决的一个重要问题是探索。
这并不是微不足道的，而且我猜你将不得不放弃一些我在谈论的关于transformer在损失函数方面的优势，以实际启用探索。
因此，目前还不清楚那些修改的损失函数是否会对性能造成实质性的损害。
但只要我们无法跨越这个瓶颈，我认为，我不，我不想评论这确实是 RL 的未来。
明白了。
另外，就像你认为，像什么 - 我可以跟进一下问题吗？
当然可以。
我不确定我理解那个观点。
所以你是在说为了将transformer应用于 RL 进行探索，必须有特定的损失函数，而且由于某种原因它们很棘手？
你能解释更多，像是修改了哪些损失函数，为什么它们看起来棘手吗？
所以从本质上讲，在探索中，你必须做的是与利用相反的事情，即非加剧。
现在，transformer 中没有任何内置的东西鼓励那种寻找状态空间中陌生部分的随机行为。
这是传统强化学习算法中内置的一些东西。
所以通常会有一些熵奖励来鼓励探索。
如果要在 transformers 中进行在线强化学习，这也是人们需要考虑的修改之一。
那么如果有人，我是说，很天真地，假设我有完全相同的设置，我从中采样动作的方式是 epsilon 贪心采样，或者我创建一个 Boltzmann 分布然后从中采样。
我是说，会发生什么？
看起来这就是强化学习的做法。
那么会发生什么？
强化学习确实做了更多的事情。
它确实会做一些类似的事情，例如，更改分布，例如，后期分布，并从中采样。
但也有这些，正如我所说，细节中包含着魔鬼。
这也涉及到它如何控制探索组件和利用组件。
而且还有待确定这是否与transformers相容或者是误导的。
我不想过早下结论，但我会说，初步证据表明，将完全相同的设置直接转移到在线情况下是不可行的。
这就是我们发现的。
必须做一些调整，我们还在摸索中。
所以我问的原因是，正如你所说，细节决定成败。
如果有人像我这样天真地来尝试RL中的内容，我想听更多关于这个的信息。
所以你是在说RL中有效的内容可能对决策transformers无效吗？
你能告诉我们为什么吗，像是什么病态出现？
那些隐藏在细节中的恶魔是什么？
也提醒我，我们时间有限，所以我会尽量不着急。
随意提问。
而且，我会发送一封电子邮件进行跟进。
但对我来说，这真的很令人兴奋。
我相信这很棘手。
是的，我只会再问两个问题，然后你可以结束了。
所以一个是，就像，我告诉你的，想想，transformer中的这种东西是解决RL中的信用分配问题的一种方法，而不是使用某种像这样的接触器。
抱歉，你能重复一遍问题吗？
哦，抱歉。
所以我认为通常在RL中，我们必须学习某种这样的接触器来编码奖励去做类似的事情。
但在transformer中，transformer能够在没有这个的情况下进行信用分配。
那么你觉得像这样的书是我们应该这样做的方式吗？
就像我们应该，而不是有一些折扣，尝试直接预测奖励？
所以我想说的是，我觉得折扣因子在一般情况下是一个重要的考虑因素，它与slim transformers并不矛盾。
所以基本上会发生变化，我认为代码实际上提供了这个功能，其中返回的计算将作为奖励的折现总和。
明白了。
所以它非常兼容。
所以有些情况下我们的上下文长度不足以真正捕捉到我们需要用于信用分配的长期行为。
也许传统的技巧可以被重新引入来解决那些问题。
明白了。
是的。
我也是这样想的，当我阅读听力transformer的工作时，有趣的是你没有一个固定的，就像γ通常是一个超参数，但是你没有一个固定的γ。
那么你觉得，你也可以像学习这个东西吗？
这也可能像这样吗？可能每次都有一个不同的γ吗？
这实际上会很有趣。
我之前没有想到过，但也许学习预测距离因子可能是这项工作的另一个扩展。
而且，你认为这样，在transformer工作中是，就像，就像与Q-learning兼容吗？
所以，如果你有像CQL这样的东西，诸如此类的东西，你也可以在transformer的基础上实现这些损失函数吗？
所以我认为也许我可以想象出一些方法，你可以在这里编码悲观主义，这是CQL工作的关键。
而且实际上，我们大多数算法都是这样工作的，包括基于模型的算法。
我们这里的重点是故意追求简单，因为我们觉得我们的文献之所以如此零散，如果你考虑到每个人都试图解决的不同子问题，原因之一就是因为每个人都试图采纳对于那个狭窄问题非常合适的想法。
例如，你不管是在离线还是在线，或者你是在模仿，你在进行多任务以及所有这些不同的变种。
因此，按设计，我们不想完全采用当前算法中存在的组件，因为那样它就开始看起来更像是架构变化，而不是对RL序列建模的更一般的概念性变化的思考。
是的，听起来很有趣。
所以你认为我们可以使用某种类似于3D学习目标而不是监督学习吗？
这是可能的。
也许像我说的，对于某些情况，比如在线RL，这可能是必要的。
通常RL，我们很高兴地看到这是不必要的，但更广泛地说，对于transformer模型或任何其他涵盖RL更广泛的模型是否变得必要，还有待进一步观察。
好的，是的。
嗯，谢谢你抽出时间。
这太棒了。
