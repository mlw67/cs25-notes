今天，Erwin 和我将就通过稀疏性扩展 Transformer 进行演讲。
而我们今天要谈论的稀疏性是指每个输入可能获得不同的权重集，或者对其应用不同量的计算。
Erwin，你想开始吗？
好的。
所以我想这项工作的总体动机是，社区意识到规模可能是获取强大性能的最重要方向之一。
现在几乎有一场正在进行的竞赛，各个实验室和不同机构竞相训练最大的模型。
这或许可以追溯到 2020 年初，OpenAI 发表了一篇名为《神经语言模型的缩放定律》的论文，他们发现模型性能遵循可预测的幂律，与模型的大小在计算或参数方面都呈幂律关系。
所以这个缩放定律可以泛化到多个数量级。
这让我们有信心，如果我们要训练非常大的模型，我们可以通过推断这些缩放定律来期望某种性能。
所以在那篇论文中，他们还发现了一个有趣的观察结果，基本上更大的模型具有更好的样本效率。
因此，如果你有固定的计算预算，你可以预测在固定的计算预算下的最佳模型大小是多少。
总的观察结果是，你宁愿为较少的步骤训练非常大的模型，而不愿为更多的训练步骤训练较小的模型。
因此，这些模型主要通过密集模型进行扩展，其中你只是增加模型的维度，但他们没有考虑稀疏性。
所以稀疏性是你可以用来扩展架构的一个新维度，这也是演讲的重点。
所以我们在这里提到的稀疏性基本上是你将根据网络输入稀疏激活的权重。
所以每个输入将会经过大致相同数量的计算，但会应用不同的权重。
而这可以追溯到1991年的一篇论文，名为《自适应局部专家混合》，最近由谷歌Brain的Noam Shazier及其同事在LSTMs中重新审视，他们用一种XBus的混合替代了LSTMs中的前馈网络。
所以大致上这个工作方式是，您会有多个 XBus，每个实现一个小网络，或者在这种情况下，我认为只是一个密集的矩阵乘法。
所以您在这里有一个额外的门控网络，显示为绿色，它输出每个令牌应该发送到的专家的概率分布。
这个概率分布被计算为 softmax，一旦您有了它，就选择几个专家。
有不同的策略，也许我们稍后会谈到。
输出只是所有选择的专家输出的加权混合。
所以它们在翻译方面非常成功。
在 NLP 中，一些复杂性阻碍了它们的广泛使用。
所以 Switch Transformer 论文解决了其中一些问题，我们将讨论如何修复训练不稳定性或减少通信成本，以及简化模型复杂性。
好的，巴里，你想说些什么吗？
是的。
那么，是的。
那么我们将采用的一种方法，用于稀疏性的一种方法是开关transformer，它有点像是一个简化的专家混合变体，再加上一些其他改进的训练和微调技术，使其能够稳定训练，并且在微调后在许多下游任务上表现更好。
所以，是的，开关transformer模型的工作方式如下。
所以你有一些transformer模型，具有自注意力和前馈层，想法是我们用开关transformer层替换其中的一个，每两个或每四个前馈层。
所以你可以看到左边是一种层块，其中包括自注意力、加和、归一化，然后是一些前馈层，然后是加和、归一化。
在这种情况下，我们将正常的前馈层替换为开关层。
我们可以在右边看到这一点的说明。
所以在右边，我们可以看到该层有两个输入。
一个是令牌更多，另一个是令牌参数。
我们可以看到这些嵌入表示将被发送到一个路由器，这正是在专家混合中它是如何工作的。
所以路由器基本上只会得到所有专家的分布。
所以在这种情况下，我们可以看到最高的概率是给了第二位专家，即四位专家中的第二位。
然后，正确的令牌实际上在第一个前馈权重上具有最高的概率，这就像第一个专家一样。
所以是的，我们可以在这里看到，我们要做的是在转换开关中，这非常简单。
它只是将其发送到最高的概率专家。
所以在这里我们可以看到自适应计算的地方，我们将有四组权重。
有一些共享的权重和计算跨所有令牌。
例如，自注意层对更多的令牌和参数令牌完全相同。
但是在稀疏的层中，我们可以看到，实际上输入是，虽然应用了相同数量的浮点运算，但实际上具有不同的权重矩阵。
下一张幻灯片。
是的。
这就是开关transformer的高层次思想，与其将一个令牌发送给多个不同的专家，这可能会增加通信成本，后面我会详细介绍。
它还显著简化了算法，只需将其发送到一个专家。
因此，对于改进的培训方法，我们专注于三个不同的方面，以帮助改进稀疏模型的培训。
第一个是选择性精度，这使得稀疏模型可以以较低精度格式进行训练，这非常重要。
我们训练的大多数模型，我们真的不希望使用浮点32，因为计算速度较慢。
而且，当在不同进程和其他情况下传递张量时，它的速度是两倍的，只因为有两倍的东西。
另外，我们还有一些初始化技巧和一些培训技巧，以便让它们在模型增大的情况下更加稳定，这就像是一种新的初始化方法，还有学习率调度的变化。
第三，由于我们的模型有更多的参数，我们确实注意到了不同的过拟合动态，特别是一旦我们对这些模型进行微调，这些模型已经在互联网上进行了预训练，针对这些小任务，可能只有五万到十万个例子，它们更容易过拟合。
因此，我们还研究了一些定制的正则化方法，以帮助预防我们观察到的一些过拟合现象。
最后，我们还谈到了这种不同的可微负载平衡技术，我们使每个专家大致获得相同数量的标记。
因为这非常重要，特别是考虑到我们希望这些东西在硬件上高效，我们希望大致上每个专家都能收到相似数量的标记。
为了鼓励这一点，我们附加了额外的负载平衡损失，以及我们正在训练的交叉熵损失。
嗯，下一张幻灯片。
好的。
所以在这里，我要讲一下选择性精度。
是的，当我们训练大型模型时，能够以较低精度格式进行训练非常重要。
所以，不是每个，你知道的，权重都是一个激活是32位，我们希望将其缩小为16位，并且我们使用像B float 16表示一样。
而我们发现，从一开始，这些模型就是不稳定的。
嗯，特别是稀疏模型比密集模型更不稳定，就像，你会训练它10, 20,000步，然后损失就会发散一样。
这是我们经常遇到的问题。
所以我们发现的一个关键是基本上你需要将计算的一部分转换成float 32，这样这些模型才能稳定地训练。
而我们发现的一个关键组件是需要转换的就是路由计算。
基本上，你知道，我们稍后可以更详细地讨论技术细节，但基本上每当有这些指数函数的时候，我们需要更高的精度，因为由于舍入误差，可以从根本上改变某种指数函数的输出，这非常重要。
比如，如果你有一个指数函数，你将其改变0.1或0.2或0.3，这会极大地改变指数阴影的输出，特别是取决于输入的大小。
所以，是的，这是一个非常重要的事情，它基本上根本不改变计算，使得模型更加稳定。
下一张幻灯片。
所以我们看到的第二件事情也是初始化规模。
就像我们初始化这些模型的标准方式，我们发现这也让模型更容易不稳定或表现更差。
所以我们做的一件事情就是发现把初始化规模调小是非常有效的。
当我们这样做时，我们发现，你知道，质量就像显著提高了，而且这是一个非常简单的修复。
下一张幻灯片。
我提到的第三件事是，由于我们注意到这些模型更容易过拟合，因为它们有更多的参数，所以我们也只对专家层使用更多的辍学。
所以在这里我们可以看到，我们采用了，你知道，T5基础模型，它是一个密集模型。
然后我们有一堆不同的开关变体。
我们发现，在这四种不同的微调任务中，最有效的方法就是显著增加专家层中的辍学率。
我们发现这对抗过拟合非常有效。
下一张幻灯片。
是的。
我们有一个问题。
哦，太棒了。
好的。
好的，让我看看。
你想继续吗？
我可以问。
这只是针对以前的表格，其中包括吞吐量和精度。
我觉得令人惊讶的是，你可以匹配这个 1390 数字，我们使用的是选择性精度。
我觉得我会期望它介于两者之间。
是的。
所以本质上是因为，可能有一点噪音样本，但速度和唯一的部分我们投入的是路由器，你知道，也许它是计算中一个微不足道的部分，而且在这里没有任何通信，本质上就像网络中的一个自由操作一样。
无论您将其转换为flow 16还是float 32，实际上并不影响我们能够测量速度的精度范围内的速度。
而且，这些架构只在每四层中使用一次稀疏层。
所以，是的，实际上，float 32部分在整个架构中相当可忽略。
就像，例如，我想，从我头脑中想到的，就像是在一个稠密的、常规的稠密层中进行第一次权重矩阵相乘所需的计算的140分之一。
所以，这只是一个非常非常小的部分。
而且，是的，我们也不像每个人都提到的那样经常使用它们。
是的。
然后，你知道，关于这个的一个简短的观点，就像我不会深入一些技术细节，但是，是的，我们肯定，您知道，因为我们是在硬件上训练这些东西，而且我们真的，就像，我认为混合专家范式的一个很大的部分是，这些东西设计得非常有效地映射到硬件上。
所以我们希望进行密集矩阵乘法运算，为了使其运行得很好，我们还希望能够使每个不同专家接收到大致相等数量的标记，我认为这对于负载平衡公式并不是特别敏感。
我们尝试了一些方法，其中很多方法都行之有效。
但是，确实，在使用稀疏性时，你肯定希望添加一些负载平衡损失。
是的，下一张幻灯片。
是的，Erwin，你可以继续。
是的，我们使用的库与框架依赖于静态形状... 好的，对，所以 XLA，也就是 TensorFlow 和 Mesh TensorFlow 的编译器期望张量具有静态形状。
然而，由于路由器的存在，switch transformers 中的计算是动态的，对吧？
不同的输入将被路由到不同的专家。
因此，我们需要提前指定每个专家将发送多少标记。
因此，我们将引入此导出容量超参数来指定这一点。
这将是一个静态数字，表示每个专家可以处理多少个标记。
因此，在实践中，我们通过引入一个称为容量因子的量来参数化这一点。
我们在这里举了一个例子。
所以，底部一行是一个设备上的一堆标记，然后您需要将这些标记路由到多个设备或多个出口。
所以，如果太多的标记被路由到单个出口，一些标记将被丢弃，因为，正如我们所说，出口具有固定的容量。
所以这是左边的例子，容量因子为一，这基本上意味着总体上，没有额外的缓冲区来路由标记。
所以，我们可以使用大于一的容量因子。
所以在右边，您有一个示例，容量因子为1.5。
所以这意味着现在每个出口有三个可以处理三个标记的插槽。
所以这可以防止标记丢失，因为我们有更多的容量。
但问题是，这意味着更高的，你知道，跨设备的更昂贵的通信。
好的，所以，是的，继续。
是的，所以我们还尝试过的一件事是这种称为不丢弃标记的方法，其思想是以下。
所以，由于我们必须为每个专家确定一个固定的批次大小，并且可能存在标记丢失，我们在想，嘿，如果有标记被丢弃，它们就像有一些标记没有应用任何计算一样，可能会影响模型的性能。
那么，如果我们采用多阶段路由过程呢？
首先，您进行正常的路由，就像您将每个标记发送到其最高概率的专家一样，但然后任何丢弃的标记您将其发送到它们的第二高概率的专家，依此类推，或者您可以基本上重复此过程，以确保不会丢失任何标记。
有趣的是，实际上这种方法并没有在实践中提高模型性能。
如果说有什么，它实际上可能有点伤害。
我们觉得这实际上非常有趣。
我认为直觉是，一旦模型学到了，它希望将一个标记发送到一个专家。
它真的希望对其应用计算。
而仅仅应用一些其他计算，你知道，根本没有同样的属性，实际上可能是有害的。
所以是的，我们觉得那真的很有趣，因为我们当时非常乐观，认为这有可能，你知道的，能够提高性能，但结果并没有真正产生影响。
而且我们发现这相当令人惊讶。
我们有一个问题。
我觉得实际上有点像是你提到的地址和最后一个点。
我想当我考虑到像专家混合这样的东西时，通常是他们专业于不同的领域，对吧？
所以我认为这就像，就像很多，我只是在想，嗯，如果你把它发送给第二好的或者其他什么，嗯，如果你所有的令牌对某个专家特别好，那你只处理，比方说，20%的令牌。
那最后变得比将它们重定向到其他任何地方更好。
确切地说。
是的。
所以，即使你放弃了很多令牌，将它们发送到第二、第三或第四好的地方也是没有好处的。
而且我们实际上发现关于这些模型的一个有趣特性是，它们对于令牌的丢弃具有出奇的鲁棒性，尤其是在微调期间。
所以，是的，在标准范例中，我们要做的是预训练这个东西。
我们会有一些负载均衡损失，这实际上使得令牌非常平衡。
但是在微调期间，或者说，我们确实想要在特定任务上进行微调。
我们实际上研究了这个确切的问题，我们正在研究，在微调期间是否有负载均衡损失是否有帮助。
如果你有负载均衡损失，是的，那确实是鼓励的，你知道，对于特定的任务，我们想要尝试，你知道，所有的专家都被使用，而不是关闭它。
而且，肯定有一些，你知道，先前的专业化，实际上最好是关闭辅助损失。
即使是这样，你知道，60到70%的令牌被丢弃，实际上比，你知道，所有的令牌平衡地被使用要好得多。
但是，负载均衡损失难道不是鼓励所有专家学习非常相似的权重，然后随机分配你的令牌吗？
因为无论将东西发送到哪个专家都无所谓。
所以当我们使用负载均衡损失时，路由机制肯定是学会的。
所以模型肯定会被鼓励，你知道，选择一个专家。
它想要发送它的，好的。
对吧。
但是如果所有的专家都学习相同的权重，那么路由器基本上就会学习，“哦，我把它发送到哪里都无所谓”。
所以如果你进行负载平衡，技术上你鼓励这样做，你希望任何损失都与任何专家相匹配。
是的。
我是说，如果你有一个非常高的负载平衡损失系数，那可能是极端的行为，但在实践中，这个系数通常会被调整，我们观察到对于足够小的值，路由器仍然会学习到有意义的路由。
是的，因为这是交叉熵损失和负载平衡损失之间的一种平衡。
一方面，你确实希望鼓励模型保持平衡。
另一方面，你也希望获得良好的经验性能。
而且，模型绝对能够在一方面学习和专门化专家，使它们具有不同的权重，这样它就像是，你知道，肯定期望某些令牌被发送到某些专家，但另一方面，仍然保持合理的平衡，以便模型在现代硬件上有效运行。
确切地说。
我们也从课堂上收到一个问题。
我想问的问题是，这对我来说似乎是一个非常实验性的讨论。
我们在谈论浮点精度。
我们在谈论不同的方法，目前都运作良好。
每当我们处理一组客户时，就会有一个问题，那就是研究问题是什么？
我觉得我错过了那个。
那么，我们试图用所有这些实验来回答什么问题呢？
是的，我认为高层研究问题是，你知道，我们是否能够创建模型，你知道，从适应性计算的角度来看，我们是否能够尝试使模型更加模拟我们认为模型应该自然使用的动态，切换，不同的输入有不同数量的计算应用于它们，不同的权重应用于它们，你知道，基本上所有这些，基本上我们正在尝试研究和弄清楚如何创建一个新的框架，让这些模型得到训练，而不是它们的密集对应物。
那，你知道，对于每个输入，都始终有相同的精确计算。
所以，这很有趣，因为当你说相同的精确计算时，人们可能会想象，嗯，对我来说，立即想到的是要花多长时间来思考某件事情。
我指的是，如果我们想要进行可变长度的计算，你可以想象我可以有一小部分计算，或者我可以有更长的计算。
但是这个想法是，为什么我们要考虑不同计算的维度呢？
我是说，当然假设这些专家确实学到了不同的东西，我想你一会儿就会明白的。
那么，为什么我们立刻考虑专业专家而不是考虑可变长度的计算呢？
是的，所以实际上，我们稍后会谈到一些关于可变长度计算的内容。
我觉得它们实际上都是同样重要的方向，都应该被推动。
我认为，我想，是的，我想这有点，你知道，我并没有固定我的问题，但我想了解的是你为什么决定首先攻击这个问题？
我想了解为什么你的团队选择朝这个方向发展。
是的，绝对。
所以我认为从经验来看，稀疏性在深度学习领域的实证结果比自适应计算要好，到目前为止，我认为我们使用这些方法的方式非常适应我们现代硬件，这也是非常有前途的。
而且我认为我们看待它的方式有点像稀疏性是朝着更有趣和更普遍的自适应计算迈出的第一步，你知道，因为我认为这些东西很复杂，通常从一些已经表现良好的东西开始要比试验那些可能没有经过充分验证的东西然后试图让它真正发挥作用要好。
所以我认为我们有点从稀疏性开始，就像 Noam Shazier 和其他人在 LSTM 上取得了非常好的效果一样，我们对此很感兴趣，你知道，让我们把其中一些东西移植到transformer上，让它真正发挥作用。
然后让我们慢慢地开始向你提到的许多其他自然问题扩展。
而不是像，好吧。
而是，不同核心的不同权重，让我们也许可以每个核心有不同的计算等等。
这就是，我想我们是如何构建自然的、你知道的、研究的建设和发展的。
明白了。
好的。
谢谢。
是的。
Erwin，你觉得呢，还有其他要补充的吗？
嗯，是的。
我想我认为自适应计算和稀疏性是相关但又不同的东西。
稀疏性更像是每个例子的不同参数，而自适应计算可能是不同数量的浮点运算。
我们在标记丢弃中有一些这样的情况，但那并不是主要动机。
确实，就像Barrett提到的，我想说没有人真正弄清楚深度学习的自适应计算。
一个原因是因为我们有这些加速器，对吧？
我们需要使用数据并行ism，对吧？
因此，我们所有的加速器和框架都使用这种SPMD范式，我们应该将相同的计算应用到示例中。
所以，如果你看一下文献，你会发现像通用transformers这样的作品，他们用一个递归权重来代替transformer中的前馈。
所以这有点像每个标记上的LSTM。
而 LSTM 可以根据某些标准在不同的时间停止。
但是这些事物的实现方式只是通过掩码，因为它需要以 SPMD 编程风格实现。
所以，明显地，稀疏性是比较容易先让其运行起来的。
而且以前也有一些关于 LSTM 的先前结果。
所以就第一个问题来说，你知道，研究问题是什么，就像是，哦，我们能设计更高效的模型吗，稀疏性是一个尚未被深入探索的新维度。
是的，我认为，你知道，我对这就满意。
好的。
好的。
嗯，是的。
下一张幻灯片。
是的。
哎呀。
是的。
再次，把它全部整合在一起。
所以，开关 Transformer 层选择一个专家，就像是顶级专家。
然后将一堆一般稀疏模型改进的东西合并起来，你知道，让它更好地微调，让它更加正则化，让它能够以更低的精度格式进行训练，以及许多像是技术细节，只是为了让它们进行训练并且良好地运行起来。
嗯，是的。
所以我们还想要做的一件事情是，对顶部一和顶部二路由进行比较，因为顶部二路由是最流行的技术之一。
这里我们可以看到，我们有两个不同大小的密集模型进行了训练，我们将会观察预训练时的负对数困惑度。
所以是的，数字越大，效果越好。
下一张幻灯片。
我们将会以不同的容量因子来研究它们。
容量因子为 2.0 的意思是每个专家有足够的缓冲区可以发送两个令牌。
我们将会比较顶部一与顶部二的路由。
同时比较它们的速度以及达到某个阈值质量所需的时间。
好的。
是的。
所以在容量因子为 2.0 的情况下，我们可以看到 MOE 模型的性能优于开关transformer，这是很有道理的。
因为开关transformer只发送顶部一个令牌给每个专家，而混合专家则发送两个令牌。
这样做是有道理的，这个额外的缓冲对专家模型的混合会产生不成比例的好处。
所以我们注意到了。
下一张幻灯片或者接下来。
当我们降低容量因子时，顶部运行变得非常有趣。
高容量因子出现很多问题。
其中一个问题是它会增加更多的通信成本，用于将令牌发送给正确的专家。
它还会增加更多的计算成本，还会增加很多内存开销。
所以如果你能把这个值降低，这通常是一件非常非常好的事情。
所以我们在这里看到的是，当你有一个较低的容量因子时，转换器实际上优于专家混合。
我们可以看到，到达质量阈值的时间，我们，嗯，是的，我们，我们更快地到达那里。
所以即使在2.0和1.25的容量因子之间，像我们设置中看到的帕累托最优的情况是在较低的容量因子下使用开关转换器，仅仅是因为虽然质量在步骤基础上稍差一些，但运行速度要快得多。
所以这是一种帕累托最优的决策。
下一张幻灯片。
我们还可以看到，对于容量因子为1.0的情况，我们可以看到这实际上是非常不成比例地使开关transformer受益，甚至从帕累托的角度来看，比1.25容量因子还要好。
有趣的是，由于MOE也做了更多的计算，我们也可以在模型的其他地方增加计算量。
我们可以看到，这是一种更有效的计算分配。
总的来说，我们的结论是，是的，使用顶级路由的较低容量因子比在较高容量因子下使用顶级两个路由更符合帕累托效率。
下一张幻灯片。
或者你可以接下来。
好的。
接下来我们将看一下，开关transformer如何随着开关层中专家数量的变化而扩展。
在这里右侧，您可以看到一个图表，显示了不同开关架构的困惑度与训练步骤之间的关系，从T5基础开始，基本上没有专家或单个专家，直到128个专家。
因此，您会发现随着专家数量的增加，也增加了稀疏参数的数量，您会得到稠密基线的加速度增加。
他们就像是乘以专家数量增加时出现了收益递减的情况。
所以之前的图表是关于困惑度与训练步骤的。
而这里我们看的是困惑度与训练时间的关系。
因此，这包括了当你拥有更多专家或与密集基线比较时的所有额外通信成本。
因此，这适用于基于开关或基于T5的情况，我们观察到与T5相比高达7倍的加速。
所以，只是为了可能给这些数字加上一些背景，就像在深度学习中获得7倍的加速是相当困难的。
因此，我认为这是一个可能会激起对稀疏模型很多兴趣的结果，即使现在只是用于预训练，但仅仅是有了这个数字，也许就可以获得一些显著的东西。
好的，所以稀疏缩放定律。
所以在这里我们将看一下损失与稀疏模型参数的关系，这些参数是通过增加专家数量来增加的。
因此，与神经缩放定律论文类似，我们观察到当您增加参数时，稀疏参数并保持浮点数操作次数不变时，您会获得递减的，一致的收益，但是递减的收益。
好的，现在我们将比较专家并行性和模型并行性。
所以，我们引入了稀疏性或专家并行作为扩展模型的新维度。
但当然，对于密集模型，还有另一种方式，即简单的模型并行，其中，你知道，模型权重被分区到各个分数中。
一旦它们，嗯，超过了，在单个核心上可以输入的最大大小。
好的。
所以是的，但我假设左边是这里的专家并行。
是的。
所以，基本上我们所做的是，嗯，是的，我们在比较基于开关的模型与密集空间，我们还在与使用模型并行的更大的密集模型进行比较。
我们可以看到，你知道，基本上当我们想要扩大模型规模时，我们有两个轴可以选择。
我们可以通过模型并行通过增加flops的数量或通过稀疏性增加参数的数量来扩展。
因此，我们可以看到，你知道，即使与通过模型并行扩展的密集模型相比，稀疏性仍然是一种更有效地扩展模型的方式，仍然可以获得2.5倍的速度提升，超过了使用模型并行的较大的密集模型。
酷。
所以，嗯，基本上在这里，T5 large是使用并行处理的密集模型。
是的。
请继续。
好的。
是的。
还有一件事我们想要研究，就是，你知道的，如果你只有很少的计算机，只有很少的专家，这些专家模型是否有效。
所以通常在设计这些模型时，我们每个核心有一个专家，但是如果你没有一个大集群来运行这些东西，比如说你只有一台有两个核心的GPU，那么两个专家是否比一个密集模型更有效呢？
答案是肯定的。
所以我们甚至可以看到即使只有很少的专家，也有相当不错的扩展性能，这对于这些模型在更低计算环境中使用是非常有前景的。
下一张幻灯片。
还是你要继续吗？
好的。
是的。
所以让我们看看在使用不同类型的并行处理时的情况，即使用专家并行处理来增加专家，使用模型并行处理将模型权重传递到核心，还有数据并行处理，这在目前深度学习中是主导范式。
所以，我想在之前的幻灯片中，我们大多谈论了专家并行，但当然，密集模型和大规模密集模型使用模型并行。
所以 GP3 和其他大型模型，它们所做的就是简单地将模型权重传递到不同的因素。
是的，我们有一个问题。
哦，是的。
我只是想知道，因为我记得好像，在一篇论文中说过，拥有的专家越多，样本效率就越高。
我希望你能给我们一些关于这个的直觉，因为我不明白为什么会这样。
所以我想，也许 Erwin，我们可以处理这个问题。
是的。
我想，你知道，有很多关于更大模型更具样本效率的研究，而在缩放定律的上下文中，更大的意味着更多的参数和更多的浮点运算。
当你增加专家的数量时，会有更多的参数，但不会有更多的浮点运算，但是模型仍然像，在某种意义上，更大。
所以我想，根据更大模型更具样本效率的直觉来构建。
在我看来，这些拥有更多专家和更多参数的模型更具样本效率并不那么令人惊讶。
我想这是我的一种高层次直觉。
是的。
我会说，这是预料之中的，你知道，更多的专家会导致更好的样本效率，特别是在训练步骤中，对吧。
嗯，在训练时间内。
好的，很酷。
那我们上次说到哪了？
是的，所以我们将看一下模型权重在不同场景下是如何分配的。
所以数据并行是第一个。
这是深度学习通常使用的典型设置，尤其是对于不太大的网络，不需要模型并行的情况。
所以让我解释一下。
我将直接跳到最终的图。
我将解释如何看待这个图。
好的，所以我们有16个进程，它们被组织成一个4x4的网格，对吧？
每条虚线，这里的每个4x4虚线代表一个不同的核心。
第一行研究了模型权重是如何分配在各个核心上的，第二行说明了数据是如何分配在核心上的，也就是例子和标记。
嗯，然后理解这个图所需的最后一件事是，这里阴影方块的每种颜色都代表着一个唯一的权重矩阵。
好的，让我们从数据并行开始。
所以对于数据并行，相同的模型权重被复制到所有核心上，并且数据简单地在核心之间进行分区。
这就是这个对应的内容，就像我刚才给出的标题描述的解释。
接下来是模型并行。
那只是一个理论上的例子，因为在实践中，人们总是将模型并行与数据并行结合使用。
但是如果你只进行模型并行，现在你会有一个单一的模型权重，该权重被分区到所有核心上，并且你的数据只是在所有核心上复制而已。
所以现在我们有了模型和数据并行，这在大型密集网络中是典型的情况。
在这种情况下，模型权重被分区到一部分核心中，这些核心处理不同批次的数据。
在这个例子中，我们有四个。
所以这里的第一个子方块表示模型权重在四个核心之间进行了分区。
这在数据并行维度上被复制了四次。
在数据方面，对于模型和数据并行性，这里的数据在模型并行核心之间被复制，并在数据并行核心之间被分区。
接下来我们有专家和数据并行性。
在这种情况下，这有点类似于数据并行性，但现在每个核心将持有不同的模型权重，这由不同的颜色所示。
对于数据方面，数据只是被复制。
抱歉，数据被分区到所有核心上，就像在数据并行性场景中一样。
最后，我们有最右边的一列，这是，我猜，是用于较大模型的开关transformer论文中使用的设置。
在这里，对于模型分区，每个专家被分区到多个核心上。
在那个例子中，我们有四个专家，每个专家分区到四个核心上。
数据被复制到模型并行调用和数据并行调用之间。
这有点复杂，确实不太容易理解，但开关transformer论文有一张相同的图，有一个很好的标题来解释它。
嗯，也许我们可以，你知道的，Barrett，我们可以快速补充一些关于这个在实践中是如何实现的。
所以有一篇论文叫做网格transformer，它将批量或数据并行扩展到更一般的SPMD风格编程。
不同的实验室有不同的框架，但这篇论文奠定了通用SPMD分布式计算的基础，这对于训练大规模模型是必要的。
所以在网格抽象下，基本上我们有一个进程网格。
所以这个网格有维度，名称维度，这些名称维度指定了张量维度如何在网格维度上进行分区或复制。
所以只是这个简单的抽象支持数据并行，也支持模型并行，尤其是专家并行，一次性。
所以，你知道的，我邀请任何感兴趣的人也去查看那篇论文，因为那在某种程度上奠定了理解这些事物的基础。
好了，Barrett，你要去吗？
酷。
是的，所以下一步，我们要谈论一下，就是，我们如何将这些并行策略结合在一起，制作一个1.6万亿参数的稀疏模型。
那么，下一张幻灯片。
所以，是的，我们在这项工作中最终做的是，我们训练了两个不同的非常大的稀疏模型，并将它们与最大的 T5 模型进行了比较。
所以我们可以看到 T5 XXL，这是一个稠密模型，也是在 T5 论文中训练的最大模型。
它大约有 130 亿个参数。
在这里，我们列出了许多模型维度，比如模型 DFF，它们就像是张量的各种大小和形状，层数，头数，而且重要的是，我们还提到了在 250K 步和 500K 步时的负对数困惑度。
所以，是的，我们设计了两个稀疏模型来进行测试，我必须测试一下，比例与稀疏度与比例与稀疏度和浮点运算次数的关系。
首先让我来谈谈 switch XXL。
它的每个标记的浮点运算次数与 T five XXL 相同，但是有 64 个专家。
这导致它有大约 4000 亿个参数。
参数。
我们可以看到，基于步骤的表现实际上相当不错，并且表现优于 T5XXL 很大一部分。
有趣的是，我们设计的第三个模型 Switch C，参数达到了 1.6 万亿，但是 FLOP 数量明显较少，几乎比上述两个模型每个标记少了将近 10 个 FLOP。
所以通过减少 FLOP 来交换更多的稀疏参数。
我们可以看到，在一个步骤的基础上，Switch C 模型效果很好，但实际上并不如更高 FLOP 模型那么好，但是在我们查看 TPU 小时作为 X 轴而不是步骤的帕累托轴上，Switch C 模型的表现实际上超过了它们两个，而且优势相当明显。
所以对于预训练性能来说，我们发现实际上只要有很多稀疏性和更少的 FLOP 实际上可能会相当不错。
下一张幻灯片。
是的。
所以，是的，这是的，再次强调，这两个稀疏模型实际上正在试图验证 Noam Shazir 的一个假设，即参数对于更多的知识、推理和计算，即 FLOP，对于智能是有利的。
因此，我们将尝试通过采用这些不同的稀疏模型，然后在不同的任务上对其进行微调，其中一些任务需要更多的知识，而另一些任务则需要更多的推理，无论我们想给出怎样模糊的定义。
是的。
所以对于一个固定的 - 哦，回去。
所以是的，对于一个固定的，你能回到上一页吗？
哦，是的。
好的。
所以对于上游预训练任务的固定质量，参数独立重要吗？
所以我们这里要看两个任务。
一个是超级胶水，这是我们的推理任务。
然后另一个是类似知识任务的trivia QA，就像，你只是给它一个问题，然后让它输出一个答案。
所以在这里我们要看一下超级胶水的质量。
所以我们可以看到X轴是预训练性能，Y轴是微调后的超级胶水得分。
有趣的是，我们可以明显看到，对于固定的预训练困惑度，稀疏模型在微调时表现更差。
这在图的右上部分尤其明显，那里密集模型的微调效果明显比稀疏模型更好。
下一页。
有趣的是，当我们在更加知识密集的任务上进行研究时，对于固定的预训练困惑度，稀疏模型表现出异常良好。
所以，你知道，对于一个大致具有相同困惑度的模型，我们对于这些知识密集型任务确实取得了很大的提升。
这确实很有趣。
而且，你知道，它还展示了仅仅基于你的预训练度量进行比较的一些危险性。
所以这些模型，你知道，可以有完全相同的预训练指标，但在不同任务上微调时可能有很不同的性质。
下一张幻灯片。
有趣的是，是的，这里所有的开关模型都是，嗯，都是各种各样的模型，仍然有很多的浮点运算，但是红色模型实际上是 1.6 万亿参数的稀疏模型，拥有很少的浮点运算，但有很多很多的参数。
我们可以看到这里的红点，它在实际上相对于其他同样具有相当好的困惑度的稀疏模型表现得非常差。
所以，是的，这确实非常有趣，它表明，对于在预训练过程中具有很多稀疏性的模型，它们在一些更需要推理的度量上确实受到了影响，但对于更多这些知识密集型任务来说，它们表现得非常出色。
下一张幻灯片。
是的。
所以在这里，我们可以将其视为一个预训练困惑度的巨大离群值，在这个下游问答任务中表现得非常出色。
下一张幻灯片。
是的。
好的。
因此，我们还打算做的一件事是仅查看稀疏模型在几个尺度上的微调属性，看看它们的性能如何。
嗯，下一张幻灯片。
是的。
在这里，我们尝试了两个不同的模型。
一个是 T5 基础模型，然后我们制作了一个与之稀疏对应的 flop 匹配模型。
当他们说平坦匹配时，就像，你知道，每个标记将具有相同数量的 flop，但现在我们只是有专家。
所以我们对基础和大型模型都这样做。
我们看到实际上在除了两个弧任务之外的几乎所有任务中，稀疏模型的性能都相当不错，这绝对是令人鼓舞的。
因此，我们看到这些模型非常健壮。
它们在预训练阶段表现良好，然后在通过按照 FLOPs 和稀疏性进行适当缩放的情况下进行微调时，它们也表现良好。
而负面的结果，我们真正看到的是，当你只有大量的稀疏性而没有太多的 FLOPs 时。
下一张幻灯片。
是的。
还有一件我们想要看的事情是，嗯，多语言训练。
所以我们先前是只在英语上研究所有这些。
而且我们也想看看在多语言环境中稀疏性如何帮助，因为你知道，我们也觉得这可能是稀疏性能够很好发挥作用的一个自然场所，或者专家们可能可以跨越语言专门化。
而且我们确实看到了强烈的结果。
所以在大约100种语言中的91％上，我们看到与MT5密集模型相比，至少有4倍的速度提升。
下一张幻灯片。
Erwin，你要继续吗？
不，你继续。
好。
是的。
所以我们想谈谈的另一件事是蒸馏。
这些稀疏模型的一个缺点是它们会有更多的参数，这意味着，你知道，如果你正在提供这些东西或者其他什么，你可能需要高吞吐量的用例，或者你可能需要将其蒸馏回缩小的密集模型。
所以在这里，我们看的是T5基础和Switch基础，以及它的预训练性能。
然后我们对不同的蒸馏技术进行了一些削减实验，发现最好的技术可以保留大约30%的稀疏性改进，同时将其蒸馏回密集的对应物。
下一张幻灯片。
然后我们在多个尺度上进行了研究。
同样地，我们发现大约30%至40%的收益可以在从密集模型转变为稀疏模型，并将其蒸馏回到与密集模型相匹配的过程中保留。
所以你可以摆脱高达99%的参数，仍然保持大约30%的改进，这非常有前途。
下一张幻灯片。
等等，对不起。
好的。
抱歉。
你能再说一遍那句话吗？
你说你可以保留30%的老师的好处。
是的，基本上。
是的，所以，你，你，你，是的，完全正确。
是的，所以我们正在看，是的，你训练一个稀疏模型，然后将其蒸馏回到一个密集模型，与从头开始训练一个密集模型相比。 
就像，你看稀疏模型和稠密模型从头开始之间的差距，和稠密模型以及经过提炼的稠密模型之间的差距。
你说的提炼是什么意思？
你向前看。
哦，是的。
哦是的。
也许让我再做一个快速的高层次总结。
所以，我们将为了比较，从头开始训练一个稠密模型。
我们将从头开始训练一个稀疏模型。
然后，我们还将运行第三个实验，将那个稀疏模型提炼成一个稠密模型。
提炼是什么意思？
就像我们基本上试图匹配，嗯，就像老师的logits，就像匹配标准的东西，你知道，就像匹配每个标记的logits或者软概率之类的东西。
好的。
如果我可以插一句我的问题。
我在这个地方有困难。
我如何解释那一行说的是百分之多少是老师和性能？
是的。
好的。
所以基本上是在看稠密模型和稀疏模型之间的差距。
所以我们将会有稠密模型获得一些性能。
我们将让稀疏模型获得一些性能。
然后密集模型仍然是从稀疏模型中间某处的。
我们基本上是在说它通过这个范围的30%。
所以就像在零到一的区间内，它从密集到稀疏模型的路程是0.3。
我明白了。
所以这并不意味着教师表现的百分比并不意味着如果教师得到，如果我们将教师的猜测或预测用作基本事实，这并不意味着蒸馏模型与教师匹配的次数为33%。
不，完全不是。
基本上是说你获得了大约30%的质量改进。
是的，完全正确。
好的，很好。
然后如果我们能倒退一张幻灯片，我有一个不同的问题，但我不想打断。
当我们谈论所有这些不同的T5基础时，还有在此之前的几张幻灯片。
我对T5了解不多。
我很想知道T5在训练时，损失函数中是否有权重惩罚？
是否有权重衰减项？
不，对于任何这些稀疏或密集模型，都没有权重衰减训练。
我明白了。
那么出于好奇，如果你添加某种权重正则化来鼓励消除无用的权重，密集模型与开关模型相比表现如何呢？
哦，所以是某种类似L1项之类的东西吗？
是的。
所以我只是在想，因为这里我们在谈论稀疏性的好处。
我在想，稀疏性的这种好处有多少是因为事实上只有一部分，我的意思是，如果我理解正确的话，也许我不理解，我的理解是开关模型和前馈层，就像你，你把权重固定为零。
那就是聪明的意思。
嗯，实际上我们有点真的想要增加更多的权重。
所以我们实际上有点在尝试做，这有点，也许有点矛盾，因为我们在说开关transformer，但我们的想法是，嘿，我们实际上只想要有更多的权重，而不是更少的权重。
这有点像你会将权重归零，但是在一个更大的权重矩阵内部，如果这样说有意义的话？
我明白，是的，对我来说，似乎一个相关的基准就是问一下，如果我有密集矩阵，但我用L1或L2惩罚权重，会发生什么。
我很好奇知道这与何相比。
是的，我们没有运行这个，但也是那样，这种方式去掉了密集模型的权重，所以如果有什么...
当然，是的，是的，是的。
是的，那可能会...
最后一点是，如果你只是添加了L1惩罚损失，你不会得到结构稀疏性。
而在这里，你知道，你的巨大权重矩阵中不是随机的权重为零，对吧？
它更像是取决于每个指数的块，像块对应一次指数。
对。
因此，这种结构允许整个通信的一切，这只是你有多个调用等事实。
对。
所以我完全同意那个块结构。
我想说的是，这个开关具有非常丰富的，不仅仅是稀疏。
它还有这种丰富的结构。
我试图在我的脑海中理清的是，稀疏性是否提供了优势，还是你在其中建立的这种附加结构？
那是什么造成了性能的提升吗？
所以这就是我问的原因。
所以块结构是使得利用多次调用的事实的原因。
如果没有那个块结构，你仍然必须对所有事情进行路由。
所以你会有更多的通信成本等等。
然后你的第一个问题是什么？
抱歉。
我实际上不确定是否有问题。
我想我试图说的是我正在努力…
是的，无论如何。
但我同意。
这有点奇怪，因为稀疏性在某种程度上是有意义的，对吧？
所以就像，例如，压缩和模型剪枝是稀疏性的一种形式，但也是开关transformer和MOE也被称为稀疏性，并且那有点相关，但肯定它们旨在不同的东西。
所以这是一个非常有趣的想法，它是稀疏的，但你有更多的参数。
我得更多地考虑一下。
谢谢。
是的，就像是在这个巨大的权重矩阵内部的斑点一样，是的，是的，是的，我之前没有意识到这一点。
所以我感谢你指出这一点。
谢谢。
我有一个后续问题关于蒸馏部分。
是的，首先。
好的，如果你现在将其蒸馏回来，你基本上就回到了密集层架构，对吧？
所以现在整个专家的想法是，某些令牌会被发送到不同的专家，因为他们更擅长解决这个令牌的某些问题。
那么现在如果你回到这个密集层，你基本上只为基于这个密集层的任何专家提供服务，对吧？
就像这些令牌可能表现得很好，而所有其他令牌都有点被抛在后面。
对。
实际上，我，抱歉。
我不认为我完全理解你的问题。
所以，你是在暗示，我们只是在特定的数据集上进行训练，所以我在考虑如何使用它。
就像为什么？
是的。
是的。
是的。
所以也许具体地说，比如，就像对于超级胶水，对吧？
比如说你想为一个做超级胶水的模型提供服务。
嗯，我觉得这个想法是，你将稀疏模型蒸馏成一个在超级胶水上的密集模型。
那么你就会得到这个压缩的密集模型，比起你从头开始训练它或者从一个预训练的密集模型中训练它，现在性能更好。
那么，你使用它们了吗？
再说一遍？
你，你必须选择一个专家，对吧？
不，不，不。
你可以简单地提炼所有的，再次强调，它们只是在匹配模型的输出。
所以你可以把稀疏模型当作一种黑盒子。
我们所做的一切只是试图让密集模型匹配实际的最终，你知道，标记预测。
哦天啊。
好的。
明白了。
好的。
抱歉，我对这个解决方案的想法不是很熟悉。
所以我想那就是我的当前困惑。
好的。
是的，当然。
是的。
因为我猜这里的一个动机是，拥有专家可能会使问题的解决变得更加困难，因为它需要更大的拓扑结构，比如说你有八个专家。
我猜你可以在较少的成本上拥有多个专家，但我们就说它们可能会更难提供服务。
如果我们可以在预训练阶段从稀疏性中获益，然后使用蒸馏到一个密集模型用于服务，那将是有益的。
所以我认为那是那个实验的动机，对吧，Baris？
是的，确实。
好的，是的。
是的，我只是在说。
是的，继续。
不，你先说。
我只是说我觉得还有一个问题，所以是的。
所以是的，继续。
哦，是的，是的，听起来不错。
是的，大家到目前为止都做得很好。
只是一个快速的问题，想知道你是否认为有任何有趣的方向，围绕构建明确为并行训练而优化的模型。
我猜像 Emily 模型似乎在这里做得很好，而且在推理时，每次计算的浮点运算次数较少是非常有用的。
但我猜，你觉得在分布式训练方面有没有一些有趣的方向，你可能会喜欢像是显式地设计了很多并行头或其他类似特性的模型，你知道，有点尴尬地并行，还是只是简单地使用标准的，你知道，通过添加更多层来扩展模型，然后只是，你知道，利用模型和数据并行性就足够好了。
是的。
所以我认为，是的，所以让我确保我完全理解。
是的，我也觉得，你知道，现在，就算我们的模型肯定是与硬件和形状等密切协作设计的，你知道吗？
嗯，所以，我认为在高层面上，是的，我认为有很多有趣的研究，比如协同设计硬件、分区算法和模型。
我认为鉴于我们有这种SPMD网格样式的分区，我们已经在某种程度上设计我们的模型以很好地适应它。
所以例如，当我们想要扩展我们的模型时，我们首先去扩展的维度之一是内部隐藏维度。
扩展这个维度有一些非常好的特性吗？
它基本上就变成了一种，你知道的，独立于一些通信成本的形式。
在查看这些计算设备上的计算到内存操作时，它真的很好，你知道，嗯。
是的，确切地说。
就像，我认为当我们设计这些模型时，我们真的是在设置维度，以便它很好地映射到硬件上。
嗯，所以它几乎就像，你知道，考虑到我们有这个模型数据并行性，我们实际上是在更多地为它设计模型，但我也认为有很多新的、有趣的分布式算法之类的东西，这使得设计模型变得非常有趣。
就像，我觉得有一件很酷的事情是，比如微软的零分区。
而且，这也为设计和扩展模型提供了一些非常新颖、不错的影响。
所以，我认为这是一个非常富有成果的研究方向。
嗯，如果那个，如果那个回答了你的问题。
是的，不，那真的非常有帮助。
有趣。
谢谢。
是的，确实。
我对我们的未来非常乐观，就像设计硬件、模型、分区策略一样，因为要使其运行良好，你必须对这三者都了解，然后将它们的发展交织在一起。
是的。
是的。
听起来很棒。
酷。
是的。
所以总结一下，就像，是的。
所以，转换器transformer就像是对专家混合的一个很好的简化。
我们看到，在预训练中，我们得到了非常强大的加速改进，超过了许多T5模型，它们是非常强大的基线。
我们看到，我们可以高效地将稀疏模型精炼成密集模型，并且通过我们讨论过的一些新技术，我们可以改进预训练和微调。
我们还看到，模型在多语言数据上运行，并且我们现在可以轻松成功地训练多达1.6万亿参数模型，这是非常有希望的。
接下来的幻灯片。
所以我们也想要介绍两张幻灯片，关于一些新的工作，实际上是使用这些模型进行计算机视觉，还有一点是它们如何用于实际进行一定程度的自适应计算，不仅仅是每个输入现在都会获得不同的权重，而且有时不同的输入将会有不同数量的计算应用于它。
所以有一些非常出色的工作是由Google苏黎世团队完成的。
是的，他们只是为图像分类而做这个。
而且，他们基本上看到了类似的缩放特性，通过增加专家的数量并使用稀疏性，使它们能够在图像分类上获得良好的性能。
下一张幻灯片。
有趣的是，其中一件事情是，正如我们谈到的容量因子。
所以，他们讨论的数值是1、1.25、2.0之类的，这意味着在2.0的值下，有缓冲区，你知道，每个专家有两个令牌的位置，但实际上他们研究了小于1的情况。
这意味着在0.5的时候，只有一半的令牌的位置。
好消息是，他们在图像分类和图像中做到了这一点，图像中存在大量冗余，他们注意到你实际上可以通过只允许处理图像的部分，最多可以处理十分之一来获得非常好的性能。
图像中要处理的部分通过稀疏层。
所以，是的，我们认为这也是一个非常好的方向，结合稀疏性和自适应计算。
还有，嗯，非常感谢你们邀请我们。
这就是演讲。
太好了。
所以感谢你，巴雷特和阿里凡，来这里。
所以，你知道，非常感谢。
所以我会提出一堆问题，然后我们可以在课后为学生开放问题讨论。
有一件事是，你们尝试过使用更多线性注意机制，像改革者和其他东西，来扩展计算吗？
我个人没有。
也许吧。
我个人没有做过这个。
是的。
所以我想我们也许可以评论一下注意力图带来的成本并不是这些大型transformer中的主要成本。
所以使用线性注意力的动机就是降低注意力图的二次成本。
但到目前为止，至少在像SuperGLUE、C4等典型的NLP设置中，随着模型规模的增加，大多数内存来自于模型权重，而不是注意力图。
而且这也是因为使用非常长的上下文或序列长度并没有证明是有益的。
所以仅仅使用基本的自注意机制已经是一个非常强大的基线了。
明白。
好的。
那么另一个问题是，你认为这种机制是否更具可伸缩性？
你能否继续构建万亿参数的模型之类的东西？
你觉得呢？
是的，当然。
我认为，是的，完全是。
我认为老实说，最大的限制之一是，你知道，这甚至不一定是一个限制，只是你必须把参数放在某个地方，设备上的存储是有限的。
但如果你有足够多的设备，你知道，是的，你可以划分权重。
就像，是的，我看不到有什么能阻止它的。
明白。
那么，就个人而言，你认为是你的，就是关于方向的事情，就像，像transformer的规模化将会如何发展，会有更多的尝试只是像使用变换机制这样的电压吗，专家先生们，还是你认为社区需要其他的东西呢？
我是说，我肯定认为专家混合应该找到自己的出路，或者至少，你知道的，稀疏的玩家像开关transformer之类的东西肯定会在大模型的未来中找到自己的位置。
我认为它们确实带来了很多好处，而且在高吞吐量应用中也非常出色。
所以我认为其中一件事，就像，唯一的缺点就是稀疏性，就是说，如果你看一下每个模型重量的性能，它们总是会比较糟糕。
所以就像，如果你真的受限于，我想设计最好的模型来适应我能找到的尽可能小的设备，那么它们可能不会是最好的解决方案，因为稀疏权重只是不如用于所有东西的密集权重那么好。
我认为这实际上取决于应用程序，但我对我们在预训练期间使用大量数据并行训练这些模型，然后在中等到较高吞吐量示例中提供服务感到非常乐观。
我觉得它们实际上可能会是一个相当大的胜利。
所以这就是我对稀疏性如何在其他方面被使用的想法，是的，我想，我不知道。
有大量令人兴奋的研究，你知道，从所有线性注意力的东西到自适应计算，新的预训练目标，你知道，未来会是什么样子很难说，但是，有很多令人兴奋的事情可以期待。
好的，听起来不错，好的。
所以现在我们可以进行一轮学生提问了。
所以我们就停止录音了。
