非常感谢。
很高兴能来到这里，晚了一点，但还是祝大家万圣节快乐。
我觉得这次演讲会分为两个部分，所以我会先花大约10到15分钟聊一下普通的Transformer。
但我假设你们大多数人都对它们很熟悉，我们可以继续讨论transformer，Yannick和Neil会介绍。
那么，我会试着快速浏览一下transformer的概述，也许会多花一点时间讲讲transformer的历史，也许可以稍微讲述一下故事。
我认为那可能更有趣。
所以就在transformer架构方面，它首次引入的两种东西是多头注意力和自注意力。
然后将它们与快速自回归解码结合起来。
所以在transformer之前，几乎每个人都在使用LSTM和带有注意力机制的LSTM。
但我会尝试解释一下自注意力和多头注意力的区别。
所以最初你会有两个序列，然后会有一个注意力模块，它会从源序列到目标序列进行关注。
因此，源序列中的每个标记或每个单词都会与目标序列中的一个元素的软近似关联起来。
因此，你会得到类似于这样的东西。
但是通过自注意力机制，我们摒弃了这两个独立的序列。
我们使它们变得相同。
因此，你正在将序列中的每个元素与序列中的另一个元素相关联。
所以这里的想法是，你正在学习句子中单词与其他单词之间的关系。
所以你可以想象一下，像形容词这样的东西，它被应用于一个名词上。
所以你想要将那个形容词，比如蓝色，与球相关联。
嗯，所以我们正在学习序列内的模式，序列兴趣模式。
嗯，对不起，我在肯尼亚做了这个演讲，所以我在这里使用了和平热情。
但是使用多头注意力，思路是你有每个单词由一个嵌入表示，这在深度维度上。
然后你有你的一系列单词。
嗯，你将其分成许多不同的组。
嗯，在这里，我已经将其在深度上分成了四组。
嗯，你独立地将注意力应用于每一个这些组。
然后当你得到结果时，你可以将它们连接在一起，然后你回到了你的模型维度表示。
所以这让你能够做的是，如果每个注意力，就像每个注意力头现在可以集中于学习一个模式。
所以也许注意力头一正在学习形容词与名词的关系，而第二个注意力头可以学到不同的东西。
这样我们就可以学习一种层次结构或者不同关系的列表。
好的。
所以这就是自我注意力。
另一个部分是快速自回归解码。
嗯，我真的想详细介绍一下吗？
好的。
我会的。
所以这个重要的事情是，如果你正在进行普通的自回归解码，你所做的是生成你的第一个标记。
然后在第一个标记的条件下，生成第二个标记，在前两个的条件下，生成第三个，依此类推。
那太慢了，对吧？
就像是一遍又一遍地应用这个循环。
因此，我们可以做的是在代码中做一个假设，即我们的模型总是生成正确的内容。
我们只生成一个预测，向前推进一个标记。
这看起来是这样的，好的，所以这里是 Y hat。
抱歉，等一下。
从输入到输出。
你有你的输出，也就是 Y。
你有你的目标，也就是 Y hat。
你要做的是将这些黄金目标馈送进去，这样你就不需要实际执行这个循环。
所以，不是假设，不是要生成第一个标记，将其反馈到你的架构中，生成第二个标记，而是将整个目标序列馈送进去，你只是假装生成了到位置 K 的所有正确标记，然后预测第 K + 1 个，然后计算你的损失。
因此，实际上，你的模型可能在训练开始时生成了垃圾，但是你得到的损失就好像你的模型已经看到了所有正确的标记，现在只是在预测下一个标记。
这有点微妙，但对于训练速度来说影响巨大，因为所有这些都可以并行进行。
所以实际上这就是使得 Transformer 如此可扩展的原因。
好的，为了成功地做到这一点，如果你只是单纯地输入所有的正确标记，幼稚地，会发生的情况是你的模型只能向前看并作弊。
所以你放进了所有真实的目标，也就是你想让你的模型预测的东西。
如果你在这里计算你的损失，你可以向前看并说，好的，我只要抓住那个，我们会得到零错误，微不足道的，对吧？
因为你已经给了它所有正确的答案。
所以我们在架构内部需要做的是实际上防止注意机制能够看到它本不应该看到的标记。
这是看起来的方式，你在你的注意力上创建一个掩码。
所以，抱歉，这是做一个微不足道的注意力的例子。
如果你没有正确地对注意力进行掩码，它将会做的是，它会只是向未来看，只是抓住你告诉它要预测的标记并复制过去。
所以它学到的是一些微不足道的东西，一些它实际上不能泛化的东西。
所以我们所做的实际上是防止它关注那些标记。
我们阻止它去关注未来。
对于源序列中的每个位置，我们将它不应该看到的所有内容都屏蔽掉，一切未来的内容。
然后，随着我们的移动，我们逐渐解除屏蔽，这样它就可以开始看到过去的内容。
所以这些就像是Transformer的三个主要组成部分。
自注意力，多头注意力，然后部署这种黄金目标解码，这种快速的自回归解码。
就故事情节而言，可能会更有趣一些。
说。
所以，Transformer，我在2017年与Google的Lukasz Kaiser实习。
我坐在Noam旁边，而Ashish就在我们几排椅子的下面。
真正令人难以置信的是，这整个项目基本上是在三个月内完成的。
所以我去了Google，Noam一直在研究自回归模型，Um，同样的事情也发生在Ashish、Yakov和Nikki身上，他们一直在探索这个领域，搞清楚了。
而Lukas和我同时，一直在研究这个叫做tensor to tensor的框架，它是专门为多模态学习、自回归学习而设计的。
嗯，Lukas有点像是一个在该领域中能够追踪所有发生事情并加以采纳的大师。
在tensor to tensor内部，有一些新兴的小事物，可能只有一篇论文提到过。
嗯，人们对像层归一化这样的东西很感兴趣，但实际上它还没有真正起飞。
嗯，在学习率调度的热身阶段，所有这些小细节都默认是打开的。
所以当Noam、Ashish、Nikki和Jakob加入tensor to tensor时，所有这些东西都是默认打开的。
所以很多人看transformer论文时，似乎有很多随意的小东西被扔进去。
而现在，这些东西已经成为许多不同训练算法的标准，比如学习率热身，以及我们进行初始化的方式，所有这些细节都已经成为常态，但在当时它们只是刚刚被引入。
嗯，所以。
我们花了很多时间进行消融实验，试图弄清楚哪些是必要的部分，以及是什么让它起作用。
如果你们中的任何人实际尝试过训练transformers并尝试调整学习率的预热或更改任何这些小部分，你会发现它确实会破坏优化。
实际上，这确实会影响性能。
例如，移除层归一化之类的东西。
我总是觉得很有趣，Lukáš随意添加了所有这些东西，因为他在尝试它们时发现它们是至关重要的。
而且它们默认就是开启的。
总之，大约是三个月的时间。
我记得一切真的是在最后开始走到一起，就在NeurIPS截止日期之前。
我仍然记得坐在微小的厨房里，Ashish告诉我，就像我只是一个小实习生，他告诉我，这将是一个很大的事情。
我当时就像，是的，当然。
好的。
我完全不知道发生了什么。
我只是出现了，他就像，不，伙计，这实际上很重要。
就像，你知道，我们把蓝色提高了三个点，我就像，很酷，太好了，反正。
然后我记得在 NUR-EPS 截止日期的那个晚上，大概是凌晨 2 点吧。Ashish 是唯一留在办公室的人，而我们还在四处移动数字，调整一些东西。
然后我去睡觉了，但她还醒着，而我睡在一个小小的电话亭里。
然后对于我要提交的另一篇论文，我忘记按提交按钮，但幸运的是，一个女士打开了电话亭的门，早上在我睡觉的时候打中了我的头。
就在截止日期前，我交上了论文。
所以我要感谢那位女士，因为她帮我提交了那年的 NeurIPS 论文。
但是，总之，我觉得关于 Transformers 的疯狂之处在于它所有的东西在短短的三个月内都凝聚在一起了。
大部分的想法都发生在那段时间，就像朝着 Nerf 截止日期的冲刺。
我觉得团队中的很多其他成员，比如 Jakob、Lukasz、Shalom，他们知道它有多重要，但对我来说，我不知道。
我真的没有意识到影响有多大。
但回顾起来，整个社区如何凝聚在一起并采用它，真是令人惊讶。
嗯，我认为其中大部分归功于优化的便利性。
看起来对超参数的选择非常健壮。
所以你不需要像疯狂调整它。
花很多时间微调小细节。
嗯，另一方面是它非常适配我们运行的加速器。
嗯，所以它非常可并行化，嗯，超高效。
嗯，所以它很适合这种已经在流行中迅速发展的缩放规律的工作。
好的，除非有任何问题。
那个故事太棒了。
哦天啊。
我们都很兴奋，所以我们同时解除了静音。
太酷了。
是的，那就是我的部分。
如果有任何问题，我很乐意回答。
否则，让我们开始讲非平凡transformer。
非平凡transformer，我认为，是架构的一个很好的下一个抽象层次。
所以你可能已经看到了，transformer被应用到新领域的趋势，首先是视觉、视频和音频。
但这有点像回到了更抽象的层次。
级别。
嗯，我想是表格数据。
是的，我不知道。
我会让Yannick和Neil接下来接管，但，嗯，我觉得MPT是一个相当厉害的项目。
感谢Aiden的介绍。
感谢大家的邀请。
我们非常高兴能来到这里。
Neil和我现在要告诉大家关于我们之间数据点之间的自注意力的论文，我们在其中引入了非参数化的transformer架构。
我们将从一点动机开始，详细解释架构，然后展示实验。
这基本上是论文的一步步讲解，但也许，在某些地方加入了一些额外的见解。
好的，如约，动机和简要总结。
所以我们将从思考我们不经常思考的东西开始。
那就是，从CNN到transformer，大多数监督式深度学习都依赖于参数预测。
所以这意味着我们有一些自我训练的数据，我们想要学习从输入X预测结果Y。
为此，我们建立了一些具有可调参数theta的模型，然后我们优化这些参数以最大化在训练集上的预测似然，或者等效地，我们最小化某些损失。
然后在训练之后，我们有了这组经过优化的参数 theta。
然后在测试时，我们只需将这些参数放入模型中，并使用这些参数来对新的测试数据进行预测。
因此在这里至关重要的是，我们在测试时的预测仅取决于这些参数，对吗？
是参数化的。
此外，这意味着在给定这些参数的情况下，预测完全独立于训练数据。
那么为什么我们要进行参数化预测呢？
嗯，这真的很方便，因为我们从训练数据中学到的一切都可以用参数来总结。
因此，在预测时，我们只需要这些最终的参数，而不需要存储训练数据，这可能会非常非常大。
另一方面，我们通常有的模型已经可以同时为一堆数据进行预测，对吧？
想想现代架构中的小批量处理。
实际上，诸如批量归一化之类的东西已经使这些数据进行了交互。
因此，我们的想法是，如果我们已经在并行中拥有了所有这些数据，那就没有理由不加以利用。
所以更进一步，有点更宏大，我们挑战了作为深度学习中主导范式的参数化预测。
因此，我们希望为模型提供额外的灵活性，使其在进行预测时可以直接使用训练数据。
更具体地说，我们引入了非参数transformer架构，这将是一种通用的深度学习架构，意味着我们可以将其应用于各种场景。
NPTs将尽可能将整个数据集作为输入。
关键的是，NPTs学会了从数据点之间的相互作用中进行预测。
为了实现这一点，我们使用多头自注意力机制，正如Aiden所介绍的那样，它已经确立了自己作为一种通用的推理层。
我们还从自然语言处理社区中借鉴了另一件事，并使用了随机掩码机制。
我们使用它来告诉NPTs在哪里进行预测，也用于使学习任务规范化。
最后，当然，我们希望说服您，这种简单的想法——从输入的其他数据点、从输入的训练点中学习预测——最终效果非常好。
因此，简要总结我们已经听到的内容。
首先，我们将整个数据集输入到NPT中。
然后 B，就拿这张幻灯片来说，我们只关心在那一行绿色区域中预测橙色问号的目的。
然后我们可以比较 NPT 和参数化预测，对吧？
所以经典的深度学习模型只会根据那个单独绿色输入的特征来预测目标值。
为了做到这一点，它会使用参数 theta。
这些参数会依赖于我们所见过的任何训练数据等等。
但在测试时，我们只看那一行我们关心的预测。
相反，NPT 预测了输入中所有样本的显式依赖关系。
它们可以超越我们感兴趣的单个绿色数据，并查看所有其他样本，并考虑它们的值以进行预测。
因此，这提出了一种完全不同的思考方式，即我们如何学习预测机制。
有人在 Twitter 上将其称为 KNN 2.0，这是我们在论文中没有写过的，但也许这是一种关于 MPT 如何学习预测的不错方式。
当然，非参数模型已经存在了。
我们根本没有发明它们，我在这里将其定义为对训练数据的显式依赖进行预测，这当然是 MPT 所做的。
经典示例，如高斯过程、k-最近邻、核方法，这些可能对你来说很熟悉。
同时，也存在着将非参数方法和表示学习的优点结合起来的努力，方式类似于我们在实体中所做的方式。
然而，与实体相比，这些方法通常在某种程度上受到限制，对吧？
它们通常更多地受到统计社区的启发。
它们经常需要一些亲和力，近似推断方案在它们能够学习的相互作用方面受到限制，或者类似的东西。
因此，我们真的认为NPT可能是这些非参数预测方法中最多才多艺、最广泛适用的，但这是我们明确希望拥有的东西。
我们希望拥有一些真正易于使用的即插即用的东西，在大量场景中都能发挥作用，并且效果非常好。
因此，我将时间交给尼尔，他将告诉你有关非参数转换器架构的所有细节。
我们还有一个来自Yunfan的问题。
请讲。
是的，是的。
嗨，Jannik。
嗨。
所以你能回到上一页吗？
就是上一页。
是的，是的。
这张幻灯片，是的。
所以在问题定义方面，我认为它与一些元学习问题非常相似，基本上是学习从数据点和数据集到一些预测的映射。
那么你能否请教一下，你的问题设置与元学习问题设置之间有何区别？
我真的找不出这两个问题之间的任何区别。
嗯，我认为这实际上取决于你想要的框架，对吧？
所以我会说，元学习是当我尝试在多个数据集上进行预测时。
所以当我尝试预测一些，当我尝试学习某种预测模型时，我可以只是插入一个不同的数据集，它会自动或几乎自动地为我在这个不同的数据分布上生成新的预测。
但这完全不是我们所做的，对吧？
我们正在为一个固定的数据集训练单一模型。
所以这就是为什么我不会真的称之为元学习，因为我们正在尝试在所有监督深度学习或任何监督机器学习方法都试图在其上表现良好的相同任务上进行预测。
很酷，所以你的意思是你使用相同的测试集来测试你训练好的模型，对吧？
我是说，基本上在元学习中，我们会测试不同类型的元测试，但在你的情况下，你只想使用一个测试集，它与你的训练集的分布相似，对吧？
是的，绝对的。
所以我们稍微探讨一下数据集分布的变化。
我觉得这是一个非常有趣的场景。
我认为元学习不同数据集也是一个有趣的场景，对吧？
当你有了这个模型权重，你可以随意切换不同的数据集。
但是就这篇论文的范围而言，它主要是训练集、测试集，它们来自相同的分布，我们只是试图在标准设置中进行监督学习。
我明白了，很酷。
谢谢。
谢谢你的提问。
是的，我想补充几点，我猜。
至少根据我对元学习问题定义的理解，我认为目标更多地是能够在一个相对较小数量的额外梯度步骤上在新数据集上表现良好。
嗯，所以我认为有一些有趣的方式可以考虑在元学习类型的设置中应用 NPTs。
嗯，我们将会更深入地讨论这个问题，但是例如，你知道，可能有办法来添加一个新的数据集。
嗯，所以假设我们已经在一堆不同的数据集上进行了训练。
我们现在添加了一个新的数据集。
嗯，或许我们可以做一些类似于零点零的事情。
元学习基本上，其中没有必要进行额外的梯度步骤，因为我们基本上在预测，类似于你现在可能在NLP文献中使用提示的方式。
不管怎么说，是的，我认为我们将会详细讨论一些问题。
只是想加入其中，我认为并不是每个元学习算法，我认为你现在描述的那些都是基于优化的，但它们也是像黑匣子一样的，你不需要进一步。
我认为主要的区别似乎是有一个任务与多个任务的元学习。
是的，我也是这么认为的。
我认为主要的，是的，主要的框架问题是是否有多个数据集。
好的，太棒了。
如果没有其他问题，我会更深入地探讨架构。
太棒了。
所以NPTs有三个关键组件。
我首先会以较高层次来阐述它们，然后我们会更详细地讨论每一个。
所以首先，我们将整个数据集，所有数据点作为输入。
所以例如，在测试时，模型将同时将训练数据和测试数据作为输入。
而我们会对大数据应用多批次处理来近似这一过程。
我们在数据点之间应用自注意力。
所以例如，在测试时，我们对训练点之间的关系，测试点之间的关系，以及两个集合之间的关系进行建模。
最后，我们有基于掩码的训练目标。
这是一种类似于BERT的随机掩码，关键是我们实际上将其用于特征和训练目标。
我们将会深入探讨为什么这种方法会导致一种有趣的预测机制。
所以首先从数据集作为输入的这个想法开始，有两件事构成了NPT的输入。
这是一个以矩阵X的形式呈现的完整数据集，以及一个掩码矩阵M。
Yannick已经稍微描述了这个数据集矩阵。
我们基本上将数据点作为行。
列是属性，每个属性在所有数据点之间共享某种语义含义。
所以，例如，你只是在做单目标分类或回归。
最后一列将是目标，矩阵的其余部分将是输入特征。
所以，例如，图像的像素。
我们还有一个遮罩矩阵。
所以，比方说，我们在考虑大规模语言建模，这些大规模标记将告诉我们我们将在哪里隐藏单词以及我们将在哪里反向传播损失。
我们在这里做了类似的事情，使用这个二进制的大规模矩阵来指定哪些条目被屏蔽。
目标是从观察到的值中预测大规模值。
我看到有关处理不同长度的输入的问题。
在我们考虑的数据集中，我们会在结果部分进行讨论，但主要是表格和图像数据，其中每个数据点的长度都相同，但这就像填充一样，那将是一个合理的方法。
还有一种有趣的方式，是的，Yannick，你说吧。
只是为了补充一下，我不确定长度是指列还是行，对吧？
行，我们不关心有多少行。
长度，填充或其他什么都可以作为一个选项。
是的，我的问题是关于列的。
没错。
所以这是有道理的。
我应该思考一下。
是的。
我是说，这与整个元学习讨论是一致的，我认为如果我们想要适应具有不同数量数据点的数据集，你知道，我们可以利用自注意力对此进行处理。
所以接下来要讨论的是，基本上我们如何进行嵌入。
更明确地说，我们有这个包含n个数据点的数据矩阵。
它被称为X，还具有D个属性，并且我们有这个二进制质量矩阵M。
我们将它们堆叠起来，然后进行线性嵌入。
具体来说，我们对每个数据点独立地执行相同的线性嵌入。
我们为每个属性学习不同的嵌入。
我们在属性的索引上有一个位置编码，因为我们实际上并不关心列的等变性。
如果是表格数据，当然，你希望将所有这些异构列以不同的方式对待。
最后，我们对列的类型进行编码，因此它是连续的还是分类的。
这最终给我们提供了一个 n 乘以 d 乘以 e 的输入数据集表示形式。
NPT 的第二个关键组成部分是数据点之间的张力。
所以为了做到这一点，我们首先将我们拥有的这个表示展平为一个 n 乘以 d 乘以 e 的表示。
所以基本上我们把这些 d 乘以 e 大小的行看作是一个标记表示。
我们实际上将使用多头自注意力来完成这个操作。
你知道，我们已经多次审查了这个，但好处是我们知道从语言建模中，如果我们多次堆叠这个，我们可以建模这些更高阶的依赖关系。
这里它们是在数据点之间。
这实际上是这种架构的关键吸引点。
还有其他一些人们使用注意力进行类似事情的实例。
所以例如，像关注神经过程，很多时候他们只是使用单层作为一种表示性查找。
我们相信这实际上限制了表达能力。
通过多次堆叠，您可以学习数据点之间更复杂的关系。
Anil，你也有一些问题。
所以你可以先开始。 
哦，很酷。
谢谢。
我有一个关于你们如何做嵌入的问题。
是始终如此，这些是卷积滤波器还是线性层吗？
你们使用的嵌入类型是什么？
是的，我正在尝试回到幻灯片。
我觉得现在它对我不太满意。
但是，对于表格数据，我们实际上只做了线性嵌入。
所以，嗯，你知道的，我们，我们可以深入讨论分类和连续特征化的细节，但实际上就像，比如说对于分类，你知道，你做一个独热编码然后学习这个特定于该属性的嵌入。
嗯，然后对于数值型数据，我相信我们只是标准化了。
嗯，对于图像数据，我们最终使用了一个C410的共振18编码器。
但是，我认为，我是说，我们稍后会在结果中讨论这个，但是，嗯，这个嵌入有点随意。
嗯，你可以在数据点之间的注意力方面做任何事情。
所以，实际上你想要如何嵌入每个属性，这在很大程度上取决于你自己。
谢谢。
我认为，嗯，另一个问题，相同的问题方面。
好的。
嗯，所以，在这里我们有数据点之间的注意力。
所以我们也可以在属性之间做这种关注。
所以我们重新塑造回这个n乘d乘e的表示形式，然后我们可以独立地对每一行应用自注意力。
换句话说，对于单个数据点的直觉以及我们为什么会做这种嵌套类型的想法，我们在数据点之间的注意力和属性之间的注意力之间切换的原因只是，我们试图学习更好的数据点表示以进行数据点之间的交互。
这实际上就是正常的自我注意力，就像您在语言建模或图像分类中看到的那样。
这里的属性是令牌。
最后，我们只需重复这个过程。
那么，我们究竟得到了什么？
总结一下，我们正在学习数据点之间的高阶关系。
我们正在学习单个数据点的转换。
然后，重要的是，NPT对数据点的排列是等变的。
这基本上只是反映了对数据点之间学习关系的直觉，这种关系不应取决于你接收它们的顺序或观察数据集的顺序。
NPT的第三个关键组成部分是基于掩码的训练目标。
所以，要记住我们试图做的是预测从观察到的条目中缺失的条目，而这些质量值可以是特征或目标。
所以再说一遍，经典用途，比如质量语言建模，是对令牌序列进行自监督学习，你可以把它看作是我们设置中的特征。
我们的情况有点不同，我们通过概率p-子特征进行随机特征掩码，这与经典用途有所不同。
然后，我们还用这个概率p-子目标对训练目标进行掩码。
所以如果我们写出训练目标，我们只是对目标和特征的负对数似然损失进行加权求和。
当然，在测试时，我们只会对测试点的目标进行掩码并计算损失。
所以，让我们进一步分解并指出其中一些很酷的部分，现在右侧突出显示的是与特征相关的术语，即特征掩码。
基本上，我们发现这具有良好的正则化效果。
或多或少，模型现在可以在任何地方进行预测，使任务变得更加困难并引入了一些更多的监督。
我们在表格数据集的消融实验中发现，对于其中的八个而言，这确实有所帮助。
然后还有另一个术语，这个有点有趣。
那就是随机目标掩码。
这个想法是，实际上，您将在训练时向模型输入一些未经掩码的训练目标，这意味着 NPT 可以学习使用其他训练数据点的目标以及所有训练特征来预测某些训练数据点的掩码目标。
因此，这意味着您实际上不需要记住训练输入和输出之间的映射。
相反，您可以将模型的表示能力用于学习使用其他训练特征和目标作为输入的函数。
所以这有点涉及到这种学习KNN的概念。
嗯，你知道，显然我们可以从中学到更复杂的、关系型的查找以及类似的东西，但你可以想象一个这样的情况，我们有一堆测试数据点进来。
我们将会查看它们的特征并使用这些特征将它们分配到训练数据点的簇中。
然后我们对这些点的预测只是在相应的簇中对训练目标进行插值，这就是这种机制让NPT可以学习的一个例子。
好的。
所以如果有任何问题，我们现在可以解答。
否则，我很乐意在讨论中回答。
好的。
那么让我们讨论一下。
是的，来吧。
好奇，当你使用整个数据集时，是否会受到大小的限制而无法使用某些类型的数据集？
是的，在实践中，我们以随机小批量处理作为近似。
所以想法就是，你知道，如果你有一个相当大的小批量，你仍然会从具有这种查找能力中受益一些，因为如果你有一个合理数量的类别，你可能会能够学到一些有趣的映射，基于特征和这些类别之间的目标。
我们在实践中发现，你知道的，我们会稍微深入讨论一下，但实际上我们确实学会了利用数据点之间的关系来进行数据集的预测，其中我们正在进行小批量处理。
而且我们并没有发现你需要一个荒谬地大的批量大小才能实现这一点。
但我认为这只是一个，总的来说是一个重要的观点，这个观点引导我们去研究稀疏transformer的文献，试图在没有小批量处理假设的情况下扩展到一些更大的数据集。
太好了，谢谢。
如果我能给出一个数字，我们可以在没有小批量处理的情况下，适应大约 8,000 个数据点左右的数据集。
所以这已经占据了相当大的比例，我想说，那些表格数据集中的一部分。
但我们也处理了 1100 万个数据点的数据集，显然我们会采用小批量处理。
所以知道我们在谈论的大小是非常好的。
我对此很好奇。
这真的很令人兴奋。
我觉得你通常不会听说将transformer应用于大小为 8,000 的数据集。
我很好奇，我们可以稍后讨论这个，一旦我们涵盖了其他材料，您是否发现样本效率是这里的关键收益之一，还是在通常使用小数据集的transformers上的经验。
但我很乐意将答案推迟到讨论的一部分之后。
是的，我认为这将是一个很好的话题。
这是一般来说，我想说对我们来说有些惊讶的事情，关于NPT在小数据集上有多么强大，以及我们惊讶地并不需要调整可怕数量的参数，但我们可以稍后详细讨论。
太棒了。
那么，为了进行实验，我们主要关注表格数据，因为这是一个非常通用的设置。
而且它对深度学习来说也是非常具有挑战性的。
所以我们知道，你知道，基于树的提升方法，像extra boost之类的东西，是非常占主导地位的。
而且这也是一个与工业界非常相关的领域。
所以我们对尝试在这方面取得更好的成绩感到兴奋。
因此，我们选择了一系列不同维度的数据集，涵盖了一些不同的方面。
你知道，就像我们提到的那样，大约有数百万到数千万个实例，特征的数量范围很广，特征的构成涉及分类或连续特征，以及各种类型的任务，二元和多类分类，以及回归。
就像我说的，基线基本上是针对表格数据的通常嫌疑人，XGBoost、CatBoost、LightGBM、TuneMLPs和TabNet，TabNet是一种用于表格数据的transformer架构。
那么，让我们来看一下结果，这里我展示了各种子任务的平均排名。
在排名方面，我们在性能上表现良好，与专门为表格数据设计的方法（如CatBoost和XGBoost）相比。
事实上，我们发现NPT在这10个数据集中有4个是表现最佳的。
关于图像数据，我提到我们使用了CNN编码器，并且在CIFAR-10上表现良好。
我们也认为，总的来说，就像对小数据的图像transformer的新工作一样，这可能只需要用线性修补就可以完成。
嗯，所以这种嵌入方式可能不是关键。
尼尔，如果我可以插个话，有两个问题，你可以再回到前两张幻灯片吗？
第一个只是一个小的、次要的观点。
嗯，请往后再退一步。
谢谢。
嗯，关于这些特征，50多个，这里的“plus”是什么意思？
我得再核实一下确切的数字是多少。
我相当确定可能是大约50个。
我猜想像，哦，所以50实际上是一种排序，不是150或5000。
是的。
是的。
我是说，我，我会再为您核实，或者您可以在论文末尾查看元数据统计信息。
但是，不，这不是，你知道，任意大的。
我想说，虽然我们对是否真正需要在属性之间进行注意力的删减进行了实验，但我们发现这并没有对我们有益，但您可能可以采用一种方式，只是在那个维度上进行MLP嵌入，然后转到相对较小数量的隐藏维度，并适应任意数量的特征。
所以我认为，是的，如果你放宽了在属性之间进行注意力的必要性，你可能可以扩展至少那个维度很多。
好的。
然后我的第二个问题，如果您能前进一张幻灯片。
哦，谢谢。
嗯，在这里，我不太确定我听懂了。
什么事？
10个数据集中的四个，10个数据集中的两个，这四个中的10个是什么意思？
嗯，这是我们所有表格数据集中的一部分。
噢，我明白了。
所以，你的东西，是为了二分类，我明白了。
好的。
好的。
是的，完全正确。
太棒了。
还有其他问题吗？
这里的标准误差？
因为我的意思是，就只有10个数据集，对吧？
是的，正确。
总共有10个表格数据集。
是的。
但这些是正确的。
并且图像没有被包括在内。
是的，这些是按排名性能排列的。
正确。
好的。
我只是不明白，在这种情况下，不确定性是从哪里来的。
是的。
平均值是在10个数据集中平均排名。
那么，对于每个特定的数据集，我们有各种不同方法的排名，然后我们对每个类型的任务（二分类、多类等）中的排名进行平均，并计算出平均值和答案。
呃，如果你好奇的话，我们还在论文中提供了全部结果。
呃，我们还有几个问题。
更多问题。
Hey, yeah.
谢谢。
我想我只是觉得表现最差的是 K 和 N，这有点令人惊讶，因为它也是非参数的。
嗯，我想，你能对此发表评论吗？
是的，没错。
是不是NPT有什么内在的特点，使得它出类拔萃，远远超越了其他非参数方法，或者，嗯，是的。
为什么，为什么，为什么它在这里表现最差？
嗯，我，我想最终可以，它仍然是一种相对幼稚的预测方法，而且，嗯，你知道，它可能只是基于一种群集手段进行预测。
所以，呃，比如说，嗯，你知道，我想这对所有数据集来说可能都是普遍真理，但也可能存在某种数量。
需要对特征进行额外的推理，至少要达到基本水平。
例如，其中一个数据集是扑克牌手牌数据集，它是扑克牌中所有不同手牌之间的映射，以及人们通常所熟知的牌型，如葫芦之类。
因此，这需要对特征进行一定程度的推理，才能将事物归类。
嗯，因此，仅仅对这些不同的特征进行聚类，嗯，你知道，手很可能不会给你带来很好的预测功能。
Um, 而 NPTs 可以做一些经典的事情，比如在特征上使用 MLP 类型的东西，或者在特征上使用树型的东西，你可以学习某种复杂的嵌入，但你也可以基于嵌入的聚类进行一些非参数的预测。
是的，这很有道理。
我想，如果你使用来自编码器堆栈的预训练嵌入作为 KNN 的向量表示呢？
与其他人群相比，你认为这将会有怎样的表现？
是的，所以这就像，嗯，我的意思是，这个想法有点像深度内核学习，或者像，嗯，是的，我相信它就是深度内核学习，基本上就是你独立使用一个 MLP。
嗯，所以你在每个输入数据点上学习一个 MLP，然后在这些数据的所有表征上应用 GP。
所以，你会得到这种复杂的嵌入，然后进行查找。<EOS
那种想法与NPTs之间的关键区别在于，我们还学习了数据点之间的关系，因为我们使用这个。
参数化注意机制来学习关系。
所以我们不仅仅是独立地学习嵌入。
我们基本上是通过整个过程进行反向传播，学习我们尝试嵌入这个的方式，但也是说查找可能发生的方式，实际上也可能是更高阶的关系。
好的，很酷。
等等，我可以再提一个跟进问题吗？
哦，是的，请问。
好的，谢谢。
所以我想，如果MPT的优势与数据点之间的关系有关，那么如果你，你知道，取出，取出，比如说，编码器的表示，然后将其作为输入传递给，你知道，最近的10个邻居，以及其他一些输入表示，比如注意力样式，你可以根据这些输入数据点之间的注意力权重来加权最近邻居的向量，然后像传递它作为，你知道，最后预测层的向量，你认为这是否捕捉了某种关系，或者说有些偏离了？
所以我认为，好处在于，实际上，我们对整个事情的想法就是，在某些情况下，某些固定的内核会在任务中表现特别好，这有点恼人，最终调整这些类型的事情或者尝试推导可能对给定情况有很多意义的预测方法，有点糟糕。
理想情况下，你只想在数据集上进行反向传播，并且自己学习这些关系。
所以我实际上非常想看看我们是否能提出一些有这种类似非常特定的KNN-like预测机制的合成实验，并且看看我们是否能精确学习这些机制，并且获得，你知道的，与NPTs零误差。
事实上，像，我们将会稍微讨论一下我们所做的一些干预实验。
我们有一些NPTs最终能够学习的精确查找函数。
所以我们可以学习有趣的关系函数。
酷，是的，非常感谢。
感激不尽。
好的。
我们还有一个问题来自……
是的，当然。
我只是想澄清一下，基本上，所以在测试时，你只是取完全相同的数据集，然后就像添加你的测试示例，对吧？
然后你就像做同样类型的像掩码处理，这是它的工作方式吗？
是的，正确。
好的，明白了。
我确实还有一个问题。
呃，那只是因为像，呃，我，我认为我像误解了，像你的，呃，NPT目标的效果。
你介意回到那张幻灯片吗？
是的。
呃，你能再重复一次吗？
这有什么特别之处吗？
是的。
所以，在特征的右侧的正则化器，我会认为它与自监督学习非常相似，只是一个标准的Transformer。
或者说，你基本上只是引入了更多的监督，即使你只是在进行监督目标，这就像在特征上进行一定程度的重建，你会学到一个更有趣的表示，就像是一种正则化效果，我们认为这很有趣，但可能不像这种随机目标掩膜那样有趣，这个独特的原因是。
在标准的参数化深度学习中，你不会在训练过程中有一个实例，其中你将目标作为输入。
所以基本上发生的情况是，如果你将训练数据集作为输入，无论如何，特征上都会发生一些随机的特征掩膜。
Um, 在训练目标中，嗯，你会随机地有一些是未遮罩的，而另一些确实被遮罩了，你将对那些被遮罩的进行反向传播损失，当然，因为，你知道，如果你要实际尝试对其进行反向传播损失，你就不希望你的模型在输入时有这些可用的。
但你可以将其他的用作输入。
这意味着你可以学习这些一种类似于插值函数。
所以这就像是能够学习KNN的整个想法。
但这样不是又让模型能够作弊了吗？
是的，这是一个有趣的观点，实际上是微妙的。
所以我认为值得提出来。
首先，我们实际上从未对模型在输入时可见的东西进行反向传播损失。
所以，例如，如果模型实际上最终基本上过度拟合了训练标签，我们就不会观察到模型对测试数据的泛化能力。
我们不会观察到这一点。
所以，显然，似乎这种在输入到NPT的标签上阻止反向传播的做法是有帮助的。
这也可能是鸟式随机遮罩中的一种可能性，您还将随机地将一些标签翻转为不同的类别。
所以这就像是在鸟遮罩文本中引入的一种随机的小字。
我们也这样做，所以有可能这在某种程度上对此有所贡献，但很可能只是因为我们没有在可见的东西上反向传播损失。
好的。
谢谢。
有道理。
如果可以，我有两个问题。
当然。
抱歉。
我们能看一下指标、性能、结果幻灯片吗？
我觉得我可能漏掉了其他的一些东西。
对不起。
所以在二元分类上，AUROC，您能澄清这些数字的含义吗？
它们是AUROC吗？
所以这是...
所以在每个数据集上，比如对于特定的二元分类数据集，我们将得到方法的排名。
嗯，我们会重复这个过程。
是的。
去吧。
那么，这里的这些数字是相对于在这个特定情况下的四个数据集中的排名。
正确。
是的。
啊，我明白了。
所以，这些数值并不是数据集平均AUROC。
不是。
是的。
它们并不是，我是说，像平均我们的，可能有道理，但是像准确率和RMSE这样的平均似乎是个坏主意，对吧？
因为你可能有一些数据集，在那里一切都有很高的准确性，或者RMSE需要一些截然不同的东西。
我明白了。
所以，这里的这些数字只告诉我们不同方法之间的相对排名，而不是它们实际执行得有多好。
我是说，它告诉我们它们相对于彼此的表现如何，但不告诉我们它们执行得有多好。
我明白了。
好的。
但这一切都在附录中。
我们都有，我们有那些信息。
我明白了。
好的。
我坐在这里感到困惑，想着为什么AUROC最好的是最小的呢？
嗯，准确率，2.5的准确率是什么意思？
无论如何，好的，现在理解多了。
谢谢你们两位。
太好了。
所以我会尽快通过这个，只是为了节省时间。
但基本上，你在看了所有这些结果之后可能会想的是，我们在这些真实数据集上甚至是否学到了任何数据点之间的相互作用呢？
所以基本上，我们设计了一个实验来弄清楚这一点。
这个想法是，当我们在一个数据点上进行预测时，我们将禁止NPT使用其他数据点。
如果我们这样做了，并且观察到NPT实际上预测或表现明显更差，那么它确实是在利用数据点之间的这些相互作用。
一个微妙的挑战，或者说这是一个我们可以从中获得的额外好处，理想情况下，我们实际上不会破坏批次统计数据。
所以，比如每个特定属性的平均值，如果我们能找到一种方法来做这个实验，使得我们不破坏这些东西，我们就可以排除我们学到了类似批次规范化的东西的可能性。
我们这样做的方式是，我们基本上按顺序查看每个数据点的预测。
所以，比如，在这种情况下，我们正在看模型对这个特定的绿色行的预测。
它将会在最后一列进行预测，这一列有一个问号，表示被掩盖。
我们要做的是，我们将独立地对除了那个属性之外的所有其他数据点对其进行排列。
所以，如果仅仅是预测经典的参数深度模型，那么该行的信息仍然完整。
但是所有其他行的信息都消失了。
这就是为什么我们称之为这种类型的损坏实验。
所以我们通常发现，当我们执行这个实验时，大多数方法的性能都会急剧下降。
我要注意的是，这些方法在很多情况下的性能都相当接近。
所以这实际上确实相当显著。
例如，在蛋白质方面，我们从所有方法中表现最佳的变成了最差的表现者，甚至比KNN之类的方法还要差。
我还要注意，呃，在这些数据集上，比如森林、踢球和乳腺癌，我们实际上观察到性能几乎没有下降。
而我们基本上将其视为模型的一个有趣特性，而不一定是一个错误。如果我们在给定的数据集上进行反向传播，模型可能会发现尝试使用某种数据点之间的关系预测实际上并不那么值得，反而可以学会使用参数进行预测，并在对其中任何一个进行预测时基本上忽略其他数据点。
嗯，所以这可能导致一些有趣的想法，也许你可以进行事后修剪之类的操作。
嗯，消除数据点之间的紧张关系并进行微调，比如说。
好的。
那么现在我将把话筒交给 Yannick，让他谈一谈一些有趣的关系学习。
是的。
但你会吗？
嗯，我看到我们已经到了时间的尽头，但是，嗯，我知道有计划中的缓冲时间等等。
你能过一下这个实验吗？
我们可以进行一些讨论。
那么，你们更喜欢什么呢？
是的，我认为我们通常会在这一点上停止录制并进行不公开的讨论。
嗯，我想要问的问题是，这个时候有没有人有任何问题？
但我认为我们基本上一直都希望能随时提问。
所以我个人觉得很好，就是，是的，随时提出问题。
是的，我觉得这听起来不错。
是的，我认为你可以按计划进行视频。
而且，我们稍后可以看一下时间的问题。
我觉得这只会再花四五分钟。
是的，是的，是的。
你应该去做。
是的，当然。
好的。
所以尼尔现在告诉我们，NPT在真实数据中的表现如何，以及它们确实利用了来自输入的其他样本的信息。
但我们现在要更进一步，提出一些玩具实验，测试一下NPT能够学习从其他行中查找信息的程度，比如它们能学习这种非参数化预测机制的程度。
具体来说，我们将创建以下半合成数据集。
所以我现在希望你专注于A。
所以我们拿取了我们之前使用过的表格数据集之一，具体来说是蛋白质数据集，但这并不重要。
重要的是这是一个回归数据集。
现在我们所做的是，这里的上半部分是原始数据集，但下半部分是原始数据集的一个副本，在这个副本中我们揭示了真实的目标值。
所以现在 MPTs 可以学习使用数据点之间的注意力来实现任意好的性能。
它们可以学习在这些匹配的重复行中查找目标值，然后将它们粘贴回到被掩码的目标值中。
然后在测试时间，当然，我们输入一个新颖的测试数据输入，在这个机制下，确保它没有学习记忆任何东西，而是确实学习了这个正确的关联机制，这也是可能的。
所以我们看到的确实是，MPTs 成功学会了执行这种查找。
我在这里可视化的是注意力图，它们非常清楚地显示出，当为这里的绿色行进行预测时，这个第一行绿色行时，MPTs 只关注的是那里的另一行绿色行。
所以这真的很好。
我们可以进一步观察NPTs应该预测的与它们实际预测的之间的那种穿透性相关性。
因此，这是99.9%。
这比你用参数预测能达到的任何东西都要好。
因此，看起来NPTs在这里实际上可以发现这个机制。
而且在这里发现，我觉得这是个正确的词，因为NPTs可能，正如我们所见，也只是继续以参数方式预测，对吧，从每一行独立地。
这确实在向我们展示，模型中存在这种从其他行学习预测的偏差。
当然，这在这个设置中也是非常吸引人的，因为它允许你在这个设置中实现任意的负载损失，或者尽可能低地优化它。
因此，我们有点将此理解为，我们的基于梯度的发现，非参数哲学似乎有些道理。
因此，我们可以通过进行某种干预实验来进一步探讨NPTs实际上在多大程度上学习到了一个稳健的因果机制，该机制是这个半合成数据集的基础。
所以，仅仅依赖，你知道的，这个，嗯，嗯，这个，这个额外的，嗯，嗯，这个额外的测试数据列已经相当不错了，但我认为我们可以再深入研究一下，看看这是否具有泛化性，超越了我们在训练集中看到的数据，或者超越了来自这个特定分布的数据。
所以现在我们在测试时对单个重复数据点进行干预，改变它们的目标值。
所以现在我们只关心特定行的预测。
我们在所有行上都这样做，但每次我们只关心单个行。
我们在这里改变目标值。
我们希望看到的是NPT也会相应调整预测。
这对我们来说是一个非常简单的干预实验，用来测试NPT是否实际学会了这种机制，同时在某种程度上也测试了鲁棒性，因为现在我们将目标值与不属于训练分布的特征关联起来。
所以我们看到的是，当我们调整这些值时，这是重复值的一种情况。
然后我们在这里看到目标值。
当我们调整它们时，我们可以看到相关性保持得非常好。
它并不完全是99.9%，就平均而言，我们现在是99.6%，但仍然非常、非常好。
此时，你可能会对我有些恼怒，因为你知道，标准的非参数模型也可以解决这个任务。
但这是一个我可以通过最近邻解决的任务。
当然，也许，你知道，我可能需要稍微改变输入格式，因为这有点像批处理设置，我可以使用掩码，但最普遍的最近邻也可以，你知道，它也会根据它们的特征查找不同的输入点。
最近邻没有学会做这个。
我仍然认为需要学习这个很酷，因为它确实需要我们学习相当多的计算序列，比如匹配所有的特征，查找目标值，将其复制回来等等。
但是，实际上，我们很容易将这个任务复杂化到一个程度，以至于我们知道的几乎没有其他模型可以轻松解决。
所以，一个非常简单的事情就是给所有重复的值加上一个加一。
现在最近邻会查找正确的颜色正确的行，当然，但它总是会预测出带有加一的错误目标。
实际上，我们知道的许多模型，并没有对特征和目标的联合分布进行建模。
它们建模的是给定输入特征的目标的条件分布。
因此，它们也无法做到这一点。
对我们来说，这实际上一点问题都不是。
嗯，MPTs 将学会减去另一个，嗯，没有问题。
当然，这仍然是一个非常合成的设置，但是，我认为，我向你挑战，想出 MPTs 无法解决但其他模型能够解决的问题。
我认为总体来说，这种掩码机制和方法的非参数性在一般情况下都非常好，导致在各种情境下都有很好的行为。
因此，有了这个，我想我们可以进入结论，尼尔会给你。
是的，我认为，我是说，我们可以剪辑主要部分。
我将快进。
只是看着它们。
是的，是的，我本来要说的，我认为你会领会要点的。
NPTs 将整个数据集作为输入，并使用自注意力来建模数据点之间的复杂关系。
你知道，它们在对表格数据和图像数据的实验中表现得很好。
我们展示了一些干预性实验，以展示它们能够解决复杂的推理任务。
在论文中还有一些其他实验。
我会说，有趣的未来工作类型是扩展类型的事物，这样我们就不会有这种小批量近似。
然后，还要试图将其扩展到一些更有趣的应用需求。
所以我们谈到了一点关于元学习，但也可能是一些类似的东西，比如少数样本的泛化，领域自适应，半监督学习等。
所以我认为如果还有一些问题，也许我们可以进行更多的讨论。
是的，我觉得听起来不错。
好的，谢谢你的演讲。
我觉得每个人都玩得很开心。
我只会问一些一般性的问题，然后我们可以在此之后与大家进行讨论。
所以我认为我注意到的一件事情就像这样，就像你说的，这与KNNs类似。
而且我认为这似乎类似于图神经网络，所以我可以想象每个数据点都像一个节点，然后你可以把一切都看作是一个全连接的图，你在这个图中学习一些注意力率。
所以这就像是一个在这种图结构上进行的节点预测任务。
那么对此有何评论吗？
像是类似于图神经网络还是有其他的区别？
是的，这是一个很好的观察。
是的，我认为在图神经网络的研究中有很多相似之处。
如果我们想谈论区别，那么区别可能在于我们假设了一个完全连接的图，对吧？
所以你可能还可以表达为我们正在发现关系结构，而图神经网络通常假设它是已知的。
但这也并不总是真实的。
因此有很多相似之处。
我不知道，Neil，如果有什么特别的事情你想提到，就说吧。
但这是一个很好的观察。
而且我们确实认为是这样的。
我们在论文的更新版本中添加了一个关于图神经网络的相关工作的额外部分，很快将在线上。
请继续。
是的，我同意你说的一切。
我认为在GNN文献中，我们关注的最接近的工作是这篇神经关系推理论文，它使用消息传递神经网络尝试学习可能存在或不存在的边，以及帮助进行外推，我认为，多粒子系统中的粒子位置等等，这与我们的想法相似。
如果没有这些边作为给定的关注机制，可能会近似出现一些相互作用的东西之间的有趣关系。
我明白了。
知道了。
是的，这真的很酷。
另一件事是，你主要关注表格数据，但你也可以有其他形式吗？
比如，如果你想处理语言之类的东西，你仍然可以使用非参数化的transformers吗？
是的。
所以我们做表格的动机部分是因为我们觉得表格数据在某种意义上是语言数据的泛化，例如，我想，有人提出了像填充这样的其他概念。
但最终你可以把它看作是一堆分类属性。
所以它绝对可以推广到像句子和我们所做的图像之类的东西。
所以，是的。
我觉得实际上，我总是在考虑我们是否认为更小或更大的数据对我们更有趣。
所以我认为小数据非常有趣，因为我们可以将整个数据集放入其中。
而且所有这些都可以直接使用，无需额外考虑。
但是大数据实际上也非常有趣，因为当然，您可能需要引入一些近似机制或一些查找机制，因为您不能始终将整个数据集放入其中。
但与此同时，您实际上是在明确地权衡您用于查找的计算与您需要用于存储的计算。
比如 GPT 中有多少参数用于存储数据，对吧？
这些模型中发生了大量的记忆，我们知道这一点，因此也许我们可以更有效地使用参数来学习查找类似的行为，对吧？
这更接近于这种，你知道的，神经 KNM 或者其他什么。
所以我认为这些都是非常令人兴奋的问题。
是的，我也期待未来的工作，因为这似乎是一种非常好的方式，可以像进行一次性学习一样。
所以是的，真的很有趣看到这一点。
好的，我将停止录制，我们可以像处理其他问题一样。
