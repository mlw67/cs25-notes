Thank you all for having me.
It's exciting to be here.
One of my favorite things is talking about what is going on inside neural networks, or at least what we're trying to figure out is going on inside neural networks.
So it's always fun to chat about that.
Oh my gosh, I have to figure out how to do things.
Okay, what?
I won't, okay, there we go.
Now we are advancing slides, that seems promising.
So I think interpretally means lots of different things to different people.
It's a very broad term and people mean all sorts of different things by it.
And so I wanted to talk just briefly about the kind of interpretability that I spend my time thinking about, which is what I'd call mechanistic interpretability.
So most of my work actually has not been on language models or on RNNs or transformers, but on understanding vision confidence and trying to understand how do the parameters in those models actually map to algorithms.
So you can think of the parameters of a neural network as being like a compiled computer program.
And the neurons are kind of like variables or registers.
And somehow there are these complex computer programs that are embedded in those weights.
And we'd like to turn them back in to computer programs that humans can understand.
It's a kind of reverse engineering problem.
And so this is kind of a fun example that we found where there was a car neuron and you could actually see that we have the car neuron and it's constructed from like a wheel neuron and it looks for, in the case of the wheel neuron, it's looking for the wheels on the bottom.
Those are positive weights and it doesn't want to see them on top.
So it has negative weights there.
And there's also a window neuron.
It's looking for the windows on the top and not on the bottom.
And so what we're actually seeing there is it's an algorithm.
It's an algorithm that goes and turns.
You know, it's just saying, you know, well a car has wheels on the bottom and windows on the top and Chrome in the middle.
And that's actually like just the strongest neurons.
And so we're actually seeing a meaningful algorithm.
That's not an exception.
That's sort of the general story that if you're willing to go and look at neural neural network weights and you're willing to invest a lot of energy and try to presentiate them, there's meaningful algorithms written in the weights waiting for you to find them.
And there's a bunch of reasons I think that's an interesting thing to think about.
One is, you know, just no one knows how to go and do the things that neural networks can do.
Like no one knows how to write a computer program that can accurately classify image net, let alone, you know, the language modeling tasks that we're doing.
No one knows how to like directly write a computer program that can do the things that GPT -3 does.
And yet somehow breaking descent is able to go and discover a way to do this.
And I want to know what's going on.
I want to know, you know, how, what is it discovered that it can do in these systems?
There's another reason why I think this is important, which is safety.
So, you know, if we want to go and use these systems in places where they have a big effect on the world, and I think a question we need to ask ourselves is, you know, what happens when these models have unanticipated failure modes?
Failure modes we didn't know to go and test for or to look for or to check for.
How can we discover those things?
Especially if they're really pathological failure modes.
So the models in some sense, deliberately doing something that we don't want.
Well, the only way that I really see that we can do that is if we can get to a point where we really understand what's going on inside these systems.
So that's another reason that I'm interested in this.
Now, actually doing interpretively on language models and transformers, it's new to me.
Before this year, I spent like eight years working on trying to reverse engineer continents and vision models.
And so the ideas in this talk are new things that I've been thinking about with my collaborators.
And we're still probably a month or two out, maybe longer from publishing them.
And this was also the first public talk that I've given on it.
The things that I'm gonna talk about, they're, I think, honestly still a little bit confused for me and definitely are gonna be confused in my articulation of them.
So if I say things that are confusing, please feel free to ask me questions.
There might be some points for me to go quickly because there's a lot of content, but definitely at the end, I will be available for a while to chat about this stuff.
And yeah, also I apologize if I'm unfamiliar with Zoom and make mistakes.
But yeah, so with that said, let's dive in.
So I wanted to start with a mystery.
Before we go and try to actually dig into, what's going on inside these models, I wanted to motivate it by a really strange piece of behavior that we discovered and wanted to understand.
And by the way, I should say all this work is done with my colleagues in anthropic and especially my colleagues, Catherine and Nelson.
Okay, so onto the mystery.
I think probably the most interesting and most exciting thing about transformers is their ability to do in -context learning, or sometimes people will call it meta -learning.
The GPT -3 paper goes and describes things as, language models are few shot learners.
Like there's lots of impressive things about GPT -3, but they choose to focus on that.
And now everyone's talking about prompt engineering and Andre Caprothy was joking about how, software 3 .0 is designing the prompt.
And so the ability of language models of these large transformers to respond to their context and learn from their context and change their behavior and response to their context, really seems like probably the most surprising and striking and remarkable thing about them.
And some of my colleagues previously published a paper that has a trick in it that I really love, which is, so we're all used to looking at learning curves.
You train your model and as your model trains, the loss goes down.
Sometimes it's a little bit discontinuous, it goes down.
Another thing that you can do is you can go and take a fully trained model and you can go and ask, as we go through the context, as we go and we predict the first token and then the second token and the third token, we get better at predicting each token because we have more information to go and predict it on.
So the first token, the loss should be the entropy of the unigrams and then the next token should be the entropy of the bigrams and it falls, but it keeps falling and it keeps getting better.
And in some sense, that's the model's ability to go and predict, to go and do in -context learning.
The ability to go and predict, to be better at predicting later tokens than you are at predicting early tokens, that is in some sense a mathematical definition of what it means to be good at this magical in -context learning or meta -learning that these models can do.
So that's kind of cool because that gives us a way to go and look at whether models are good at in -context learning.
If I could just ask the question, like a clarification question might be my best.
When you say learning, there are no actual parameter updates happening here.
That is the remarkable thing about in -context learning.
So yeah, indeed, we traditionally think about neural networks as learning over the course of training by going and modifying their parameters, but somehow models appear to also be able to learn in some sense.
If you give them a couple of examples in their context, they can then go and do that later in their context, even though no parameters changed.
And so it's some kind of quite different notion of learning as you're gesturing at.
Okay, I think that's making more sense.
So, I mean, could you also just describe in -context learning in this case as conditioning, as in like conditioning on the first five tokens of a 10 token sentence?
Predicting the next five tokens?
Yeah, I think the reason that people sometimes think about this as in -context learning or meta -learning is that you can do things where you like actually take a training set and you embed the training set in your context, like just two or three examples.
And then suddenly your model can go and do this task.
And so you can do few shot learning by embedding things in the context.
Yeah, the formal setup is that you're just conditioning on this context.
And it's just that somehow this ability, like this thing, like there's some sense, you know, for a long time, people were, I mean, I guess really the history of this is we started to get good at neural networks learning, right?
And we could go and train language, train vision models and language models that could do all these remarkable things.
But then people started to be like, well, you know, these systems are, they take so many more examples than humans do to go and learn.
How can we go and fix this?
And we had all these ideas about meta -learning develop where we wanted to go and train models explicitly to be able to learn from a few examples and people develop all of these complicated schemes.
And then the like truly like absurd thing about transformer language models is without any effort at all.
We get this for free, but you can go and just give them a couple of examples in their context and they can learn in their context to go and do new things.
I think that was like, like that was in some sense, the like most striking thing about the GPT -3 paper.
And so this ability to go and have the just conditioning on a context going and give you, you know, new abilities for free and the ability to generalize to new things is in some sense, the most, yeah, and to me the most striking and shocking thing about transformer language models.
That makes sense.
I mean, I guess from my perspective, I'm trying to square like the notion of learning in this case with, you know, if you or I were given a prompt of like one plus one equals two, two plus three equals five as the sort of few shot set up, and then somebody else put, you know, like five plus three equals and we had to fill it out.
In that case, I wouldn't say that we've learned arithmetic because we already sort of knew it, but rather we're just sort of conditioning on the prompt to know what it is that we should then generate, right?
But it seems to me like that's.
Yeah, I think that's on a spectrum though, because you can also go and give like completely nonsensical problems where the model would never have seen, see like mimic this function and give a couple of examples of the function and the model's never seen it before.
And it can go and do that later in the context.
And I think what you did learn in a lot of these cases, so you might not have learned arithmetic, like you might've had some innate faculty for arithmetic that you're using, but you might've learned, oh, okay, right now we're doing arithmetic problems.
Got it.
In any case, I agree that there's like an element of semantics here.
Yeah, no, this is helpful just to clarify exactly sort of what you meant when you cut us learning.
Thank you for walking through that.
Of course.
So something that's I think really striking about all of this is, well, okay, so we've talked about how we can sort of look at the learning curve and we can also look at this in context learning curve, but really those are just two slices of a two dimensional space.
So like in some sense, the more fundamental thing is how good are we at producing the nth token at a given point in training?
And something that you'll notice if you look at this, so when we talk about the loss curve, we're just talking about if you average over this dimension, if you like average like this and project onto the training step, that's your loss curve.
And if you, the thing that we are calling the in context learning curve is just this line, yeah, this line down the end here.
And something that's kind of striking is there's this discontinuity in it.
Like there's this point where the model seems to get radically better in a very, very short time span and going and predicting late tokens.
So it's not that different in early time steps, but in late time steps, suddenly you get better.
And a way that you can make this more striking is you can take the difference in your ability to predict the 50th token and your ability to predict the 500th token, you can subtract from the 500th token, the 50th token loss.
And what you see is that over the course of training, you're not very good at this and you got a little bit better, and then suddenly you have this cliff and then you never get better, the difference between these at least never gets better.
So the model gets better at predicting things, but its ability to go and predict late tokens over early tokens never gets better.
And so there's, in the span of just a few hundred steps in training, the model has gotten radically better at its ability to go and do this kind of in context learning.
And so you might ask, what's going on at that point?
And this is just one model, but well, so first of all, it's worth noting that this isn't a small change.
So that you can, we don't think about this very often, but often we just look at law schools and we're like, did the model do better than another model or worse than another model?
But you can think about this as in terms of Nats and it's just the information theoretic quantity in that, and you can convert that into bits.
And so one way you can interpret this is it's something roughly like the model 0 .4 Nats is about 0 .5 bits is about like every other token and the model gets to go and sample twice and pick the better one.
It's actually, it's even stronger than that.
That's a sort of an underestimate of how big a deal going and getting better by 0 .4 Nats.
So this is like a real big difference in the model's ability to go and predict late tokens.
And we can visualize this in different ways.
We can also go and ask, how much better are we getting at going and predicting later tokens and look at the derivative.
And then we can see very clearly that there's some kind of discontinuity in that derivative at this point.
And we can take the second derivative then, and we can, well, derivative with respect to training.
And now we see that there's like, there's very clearly this line here.
So something in just the span of a few steps, a few hundred steps is causing some big change.
We have some kind of phase change going on.
And this is true across model sizes.
You can actually see it a little bit in the loss curve and there's this little bump here and that corresponds to the point where you have this, you have this change.
We actually could have seen in the loss curve earlier too.
It's this bump here.
Excuse me.
So we have this phase change going on and there's, I think, a really tempting theory to have, which is that somehow whatever, there's some, this change in the model's output and its behaviors and it's in these sort of outward facing properties corresponds, presumably, to some kind of change in the algorithms that are running inside the model.
So if we observe this big phase change, especially in a very small window in the model's behavior, presumably there's some change in the circuits inside the model that is driving it.
At least that's a natural hypothesis.
So if we want to ask that though, we need to go and be able to understand what are the algorithms that's running inside the model?
How can we turn the parameters in the model back into those algorithms?
And so that's going to be our goal.
Now it's going to require us to cover a lot of ground in a relatively short amount of time.
So I'm going to go a little bit quickly through the next section and I will highlight sort of the key takeaways and then I will be very happy to go and explore any of this in as much depth.
I'm free for another hour after this call and just happy to talk in as much depth as people want about the details of this.
So it turns out the phase change doesn't happen in a one -layer attention -only transformer.
And it does happen in a two -layer attention -only transformer.
So if we could understand a one -layer attention -only transformer and a two -layer only attention -only transformer, that might give us a pretty big clue as to what's going on.
So we're attention -only, we're also going to leave out layer norm and biases to simplify things.
So one way you could describe a attention -only transformer is we're going to embed our tokens and then we're going to apply a bunch of attention heads and add them into the residual stream and then apply our unembedding and that'll give us our logics.
And we could go and write that out as equations if we want, multiply by an embedding matrix, apply attention heads and then compute the logics for the unembedding.
And the part here that's a little tricky is understanding the attention heads.
And this might be a somewhat conventional way of describing attention heads.
And it actually kind of obscures a lot of the structure of attention heads.
I think that oftentimes we make attention heads more complex than they are.
We sort of hide the interesting structure.
So what is this saying?
Well, it's saying for every token compute a value vector and then go and mix the value vectors according to the attention matrix and then project them with the output matrix back into the residual stream.
So there's another notation which you could think of this as using tensor products or using, well, I guess there's a few left and right multiplying this.
There's a few ways you can interpret this but I'll just sort of try to explain what this notation means.
So this means for every X, our residual stream, we have a vector for every single token.
And this means go and multiply independently the vector for each token by WV.
So compute the value vector for every token.
This one on the other hand means, notice that it's now on the, A is on the left -hand side.
It means go and multiply the attention matrix or go and do linear combinations of the values, value vectors.
So don't change the value vectors point -wise but go and mix them together according to the attention pattern, create a weighted sum.
And then again, independently for every position, go and apply the output matrix.
And you can apply the distributive property to this.
And it just reveals that actually didn't matter that you did the attention sort of in the middle.
You could have done the attention at the beginning, you could have done it at the end.
That's independent.
And the thing that actually matters is there's this WV WO matrix that describes, what it's really saying is, WV WO describes what information the attention head reads from each position and how it writes it to its destination.
Whereas A describes which tokens we read from and write to.
And that's kind of getting more at the fundamental structure of an attention head.
An attention head goes and moves information from one position to another.
And the process of which position gets moved from and to is independent from what information gets moved.
And if you rewrite your transformer that way, well, first we can go and write the sum of attention heads just as it's in this form.
And then we can go in and write that as the entire layer by going and adding in an identity.
And if we go and plug that all in to our transformer and go and expand, we have to go in and multiply everything through.
We get this interesting equation.
And so we get this one term, this corresponds to just the path directly through the residual stream.
And it's gonna wanna store bigram statistics.
It's just, all it gets is the previous token and tries to predict the next token.
And so it gets to try and predict, try to store bigram statistics.
And then for every attention head, we get this matrix that says, okay, well, we have the attention pattern.
So it looks, that describes which token looks at which token.
And we have this matrix here, which describes how for every possible token you can attend to, how it affects the logics.
And that's just a table that you can look at.
And it just says, for this attention head, it looks at this token, it's gonna increase the probability of these tokens.
In a one layer attention only transformer, that's all there is.
Yeah, so this is just the interpretation I was describing.
And another thing that's worth noting is, according to this, the attention only transformer is linear if you fix the attention pattern.
Now, of course, the attention pattern isn't fixed, but whenever you have the opportunity to go and make something linear, linear functions are really easy to understand.
And so if you can fix a small number of things and make something linear, that's actually, it's a lot of leverage.
Okay.
And yeah, we can talk about how the attention pattern is computed as well.
If you expand it out, you'll get an equation like this.
And notice, well, I think it'll be easier.
Okay.
I think the core story that I take away from all of these is we have these two matrices that actually look kind of similar.
So this one here tells you, if you attend to a token, how are the logits affected?
And you can just think of it as a giant matrix of, for every possible input token, how are the logits affected by that token?
Are they made more likely or less likely?
And we have this one, which sort of says, how much does every token want to attend to every other token?
One way that you can picture this is, okay, there's really three tokens involved when we're thinking about an attention net.
We have the token that we're gonna move information to, and that's attending backwards.
We have the source token that's gonna get attended to, and we have the output token whose logits are gonna be affected.
And you can just trace through this.
So you can ask what happens, how does attending to this token affect the output?
Well, first we embed the token.
Then we multiply by WV to get the value vector.
The information gets moved by the attention pattern.
We multiply by WO to add it back into the residual stream, we get hit by the unembedding, and we affect the logits.
And that's where that one matrix comes from.
And we can also ask, what decides whether a token gets a high score when we're computing the attention pattern?
And it just says, embed the token, turn it into a query, embed the other token, turn it into a key, and dot product them.
And so that's where those two matrices come from.
So I know that I'm going quite quickly.
Maybe I'll just briefly pause here.
And if anyone wants to ask for clarifications, this would be a good time.
And then we'll actually go and reverse engineer and say, everything that's going on in a one -layer attention -only transformer is now in the palm of our hands.
It's a very toy model.
You know, one actually uses one -layer attention -only transformers, but we'll be able to understand the one -layer attention -only transformer.
So just to be clear, so you're saying that the quick key circuit is learning the attention weights.
And essentially, is it sponsored running the attention between different tokens?
Yeah, yeah.
So this matrix, when it, yeah, all three of those parts are learned, but that's what expresses whether a attention pattern is, yeah, that's what generates the attention patterns.
It's run for every pair of tokens.
And you can think of values in that matrix as just being how much every token wants to attend to every other token, if it was in the context.
We're doing positional embeddings here.
So there's a little bit that we're sort of aligning over there as well.
It's sort of in a global sense, how much does every token want to attend to every other token?
And the other circuit, like the output value circuit, is using the attention that's calculated to, yes, like affect the final outputs.
It's sort of saying, if the attention head, assume that the attention head attends to some token.
So let's set aside the question of how that gets computed.
Just assume that it attends to some token.
How would it affect the outputs if it attended to that token?
And you can just calculate that.
It's just a big table of values that says, for this token, it's gonna make this token more likely.
This token will make this token less likely.
Right, okay, that's interesting.
And it's completely independent.
It's just two separate matrices.
They're not, the formulas that might make them seem entangled, but they're actually separate.
Right, to me, it seems like the lecture supervision is coming from the output value circuit and the query key separate seems to be more like unsupervised kind of thing.
So there's no.
I mean, there are just, I think in the sense that every, in a model, like every neuron is in some sense, like signals is somehow downstream from the ultimate signal.
And so the output value circuit is getting more direct, is perhaps getting more direct signal.
But yeah.
Interesting.
We will be able to dig into this in lots of detail, in as much detail as you want in a little bit.
So we can, maybe I'll push forward.
And I think also actually an example of how to use this to reverse engineer a one layer model will maybe make it a little bit more motivated.
Okay, so just to emphasize this, there's three different tokens that we can talk about.
There's the token that gets attended to.
There's the token that does the attention, which are called the destination.
And then there's the token that gets affected, gets the next token, which its probabilities are affected.
And so something we can do is notice that the only token that connects to both of these is the token that gets attended to.
So these two are sort of, they're bridged by their interaction with the source token.
So something that's kind of natural is to ask for a given source token, how does it interact with both of these?
So let's take for instance, the token perfect.
Which tokens, one thing we can ask is which tokens want to attend to perfect?
Well, apparently the tokens that most want to attend to perfect are R and looks and is and provides.
So R is the most, looks is the next most and so on.
And then when we attend to perfect, and this is with one single attention edge, so it'd be different if we did a different attention edge, it wants to really increase the probability of perfect and then to a lesser extent, super and absolute and pure.
And we can ask you, what sequences of tokens are made more likely by this particular set of things, wanting to attend to each other and becoming more likely?
Well, things are the form, we have our token that we attended back to, and we have some skip of some number of tokens, they don't have to be adjacent, but then later on we see the token R and it attends back to perfect and increases the probability of perfect.
So you can think of these as being like, we're sort of creating, changing the probability of what we might call skip trigrams, where we skip over a bunch of tokens in the middle, but we're affecting the probability really of trigrams.
So perfect, R perfect, perfect, looks super.
We can look at another one.
So we have the token large, these tokens contains using specify, wanna go and look back to it and an increase of probability of large and small and the skip trigrams that are affected are things like large, using large, large contains small and things like this.
If we see the number two, we increase the probability of other numbers and we affect probably tokens or skip diagrams like two, one, two, two has three.
Now you're all in a technical field, so you'll probably recognize this one, we have Lambda and then we see backslash and then we wanna increase the probability of Lambda and sorted and Lambda and operator, so it's all, it's all LaTeX.
And it wants to, if it sees Lambda, it thinks that, you know, maybe next time I use a backslash, I should go and put in some LaTeX math symbol.
Also same thing for HTML, we see NBSP for non -breaking space and then we see an ampersand, we wanna go and make that more likely.
The takeaway from all of this is that a one layer attention only transformer is totally acting on these skip trigrams.
Everything that it does, I mean, I guess it also has this pathway by which it affects bigrams, but mostly it's just affecting these skip trigrams.
And there's lots of them, it's just like these giant tables of skip trigrams that are made more or less likely.
There's lots of other fun things it does, sometimes the tokenization will split up a word in multiple ways, so like we have Indy, well, that's not a good example, we have like the word Pike and then we see the token P and then we predict Ike and we predict spikes and stuff like that.
Or these ones are kind of fun, maybe they're actually worth talking about for a second, so we see the token Lloyd and then we see an L and maybe we predict Lloyd or R and we predict Ralph and C, Catherine.
But we'll see in a second, well, yeah, we'll come back to that in a sec.
So we increase the probability of things like Lloyd, Lloyd and Lloyd, Catherine or PixMap.
If anyone's worked with QT, we see PixMap and we increase the probability of P, PixMap again, but also QCanvas.
But of course there's a problem with this, which is it doesn't get to pick which one of these goes with which one.
So if you want to go and make PixMap, PixMap and PixMap QCanvas more probable, you also have to go and create, make PixMap PCanvas more probable.
And if you wanna make Lloyd, Lloyd and Lloyd, Catherine more probable, you also have to make Lloyd, Cloyd and Lloyd, Lathrin more probable.
And so there's actually like bugs that transformers have, like weird, at least in these really tiny one layer attention only transformers, there's these bugs that they seem weird until you realize that it's this giant table of skip trigrams that's operating.
And the nature of that is that you're going to be, it sort of forces you if you want to go and do this, to go in and also make some weird predictions.
Chris, is there a reason why the source tokens here have a space before the first character?
Yes, that's just the, I was giving examples where the tokenization breaks in a particular way.
And because spaces get included in the tokenization, when there's a space in front of something and then there's an example where the space isn't in front of it, they can get tokenized in different ways.
Got it, cool, thanks.
Yeah, great question.
Okay, so just to abstract away some common patterns that we're seeing, I think one pretty common thing is what you might describe as like B, A, B.
So you go and you see some token and then you see another token that might proceed that token and then you're like, ah, probably the token that I saw earlier is going to occur again.
Or sometimes you predict a slightly different token.
So like maybe an example, the first one is two, one, two, but you could also do two has three.
And so three isn't the same as two, but it's kind of similar.
So that's one thing.
Another one is this example where you have a token that something that's tokenized together one time and that's split apart.
So you see the token and then you see something that might be the first part of the token and then you predict the second part.
I think the thing that's really striking about this is these are all in some ways a really crude kind of in -context learning.
And in particular, these models get about 0 .1 nats rather than about 0 .4 nats of in -context learning and they never go through the phase change.
So they're doing some kind of really crude in -context learning and also they're dedicating almost all of their attention heads to this kind of crude in -context learning.
So they're not very good at it, but they're dedicating their capacity to it.
I'm noticing that it's 1037.
I want to just check how long I can go because maybe I should like super accelerate if this is.
Oh, Chris, I think it's fine because like students are also asking questions in between.
So you should be good.
Okay, so maybe my plan will be that I'll talk until like 1055 or 11 and then if you want, I can go and answer questions for a while after that.
Yeah, it works.
Fantastic.
So you can see this as a very crude kind of in -context learning.
Like basically what we're saying is it's sort of all this flavor of, okay, well, I saw this token, probably these other tokens, the same token or similar tokens are more likely to go and accrual later and look, this is an opportunity that sort of looks like I could inject the token that I saw earlier.
I'm going to inject it here and say that it's more likely.
That's like, that's basically what it's doing.
And it's dedicating almost all of its capacity to that.
So, you know, these, it's sort of the opposite of what we thought with RNNs in the past.
Like it used to be that everyone was like, oh, you know, RNNs, it's so hard to get the care about long distance contacts.
You know, maybe we need to go and like use dams or something.
No, if you train a transformer, it dedicates and you give it a long enough context.
It's dedicating almost all its capacity to this type of stuff.
Just kind of interesting.
There are some attention is, which are more primarily positional.
Usually we, you know, the model that I've been training that has two layer, it's only a one layer model, has 12 attention is.
And usually around two or three of those will become these more positional sort of shorter term things that do something more like, like local trigram statistics.
And then everything else becomes these skip trigrams.
Yeah, so some takeaways from this.
Yeah, you can, you can understand one layer and internally transformers in terms of these OV and QK circuits.
Transformers desperately want to do in context learning.
They desperately, desperately, desperately want to go and look at these long distance contacts single and predict things.
There's just so much entropy that they can go and reduce out of that.
The constraints of a one layer attention on the transformer force it to make certain bugs that wants to do the right thing.
And if you freeze the attention patterns, these models are linear.
A quick aside, because so far this type of work has required us to do a lot of very manual inspection.
Like we're looking through these giant matrices, but there's a way that we can escape that.
We don't have to use, look at these giant matrices if we don't want to.
We can use eigenvalues and eigenvectors.
So recall that an eigenvalue and an eigenvector just means that if you multiply that vector by the matrix, it's equivalent to just scaling.
And often in my experiences, those haven't been very useful for interpretability because we're usually mapping between different spaces.
But if you're mapping onto the same space, eigenvalues and eigenvectors are a beautiful way to think about this.
So we're going to draw them on a radial plot and we're going to have a log radial scale because they're kind of vary, their magnitude is going to vary by many orders of magnitude.
Okay, so we can just go and our OB circuit maps from tokens to tokens, that's the same vector space on the input and the output.
And we can ask, what does it mean if we see eigenvalues of a particular kind?
Well, positive eigenvalues, and this is really the most important part, mean copying.
So if you have a positive eigenvalue, it means that there's some set of tokens where if you see them, you increase their probability.
And if you have a lot of positive eigenvalues, you're doing a lot of copying.
If you only have positive eigenvalues, everything you do is copying.
Now, imaginary eigenvalues mean that you see a token and then you want to go and increase the probability of unrelated tokens.
And finally, negative eigenvalues are anti -copying.
They're like, if you see this token, you make it less probable in the future.
Well, that's really nice, because now we don't have to go and dig through these giant matrices that are vocab size by vocab size.
We can just look at the eigenvalues.
And so these are the eigenvalues for our one layer attention -only transformer.
And we can see that for many of these, they're almost entirely positive.
These ones are sort of entirely positive.
These ones are almost entirely positive.
And really these ones are even almost entirely positive.
And there's only two that have a significant number of imaginary and negative eigenvalues.
And so what this is telling us is it's just in one picture.
We can see, okay, they're really 10 out of 12 of these attention heads are just doing copying.
They just are doing this long distance, well, I saw a token, probably it's gonna occur again type stuff.
That's kind of cool.
We can summarize it really quickly.
Now, the other thing that you can, yes, this is for a second.
We're gonna look at a two layer model in a second and we'll see that also a lot of its heads are doing this kind of copying or stuff, they have large positive eigenvalues.
You can do a histogram.
One thing that's cool is you can just add up the eigenvalues and divide them by their absolute values.
And you've got a number between zero and one, which is like how copying is just the head or between negative one and one, how copying is just the head.
You can just do a histogram and you can see, oh yeah, almost all of the heads are doing lots of copying.
It's nice to be able to go and summarize our model.
And I think this is sort of like, we've gone from a very bottom up way and we didn't start with assumptions about what the model is doing.
We tried to understand its structure and then we were able to summarize it in useful ways.
And now we're able to go and say something about it.
Now, another thing you might ask is what do the eigenvalues of the QK circuit mean?
And in our example so far, they wouldn't have been that interesting, but in a minute they will be.
And so I'll briefly describe what they mean.
A positive eigenvalue would mean you want to attend to the same tokens.
And imagine your eigenvalue, and this is what you would mostly see in the models that we've seen so far, means you wanna go in and attend to a unrelated or different token.
And a negative eigenvalue would mean you want to avoid attending to the same token.
So that will be relevant in a second.
Yeah, so those are gonna mostly be useful to think about in multilayer attentional and transformers when we can have chains of attention heads.
And so we can ask, well, I'll get to that in a second.
So that's a table summarizing that.
Unfortunately, this approach completely breaks down once you have MLP layers.
MLP layers, now you have these non -linearity since you don't get this property where your model is mostly linear and you can just look at a matrix.
But if you're working with only attentional transformers, this is a very nice way to think about this.
Okay, so recall that one -layer attentional transformers don't undergo this phase change that we talked about in the beginning.
Well, right now we're on a hunt.
We're trying to go and answer this mystery of what the hell is going on in that phase change where models suddenly get good at in -context learning.
We wanna answer that.
And one -layer attentional transformers don't undergo that phase change, but two -layer attentional transformers do.
So we'd like to know what's different about two -layer attentional transformers.
Okay, well, so in our previous, when we were dealing with one -layer attentional transformers, we were able to go and rewrite them in this form.
And it gave us a lot of ability to go and understand the model because we could go and say, well, this is bigrams and then each one of these is looking somewhere and we have this matrix that describes how it affects things.
And yeah, so that gave us a lot of ability to think about these things.
And we can also just write in this factored form where we have the embedding and then we have the attention heads and then we have the un -embedding.
Well, oh, and for simplicity, we often go and write WOV for WOWV because they always come together.
It's always the case, like it's in some sense an illusion that WO and WV are different matrices.
They're just one low -rank matrix.
They're never, they're always used together.
And similarly, WQ and WK, it's sort of an illusion that they're different matrices.
They're always just used together.
And keys and queries are just sort of, they're just an artifact of these low -rank matrices.
So in any case, it's useful to go and write this together.
Okay, great.
So a two -layer attention -only transformer, what we do is we go through the embedding matrix, then we go through the layer one attention heads, then we go through the layer two attention heads, and then we go through the un -embedding.
And for the attention heads, we always have this identity as well, which corresponds to just going down the residual stream.
So we can go down the residual stream or we can go through an attention head.
Next up, we can also go down the residual stream or we can go through an attention head.
And there's this useful identity, the mixed product identity that any tensor product or other ways of interpreting this obey, which is that if you have an attention head and we have the weights and the attention pattern and the WOV matrix and the attention pattern, the attention patterns multiply together and the OV circuits multiply together and they behave nicely.
Okay, great.
So we can just expand out that equation.
We can just take that big product we had at the beginning and we can just expand it out and we get three different kinds of terms.
So one thing we do is we get this path that just goes directly through the residual stream where we embed and un -embed and that's gonna wanna represent some bigram statistics.
Then we get things that look like the attention head terms that we had previously.
And finally, we get these terms that correspond to going through two attention heads.
Now it's worth noting that these terms are not actually the same as they're, because the attention head, the attention patterns in the second layer can be computed from the outputs of the first layer, those are also gonna be more expressive, but at a high level, you can think of there as being these three different kinds of terms.
And we sometimes call these terms virtual attention heads because they don't exist in the sense, like they aren't sort of explicitly represented in the model but they in fact, they have an attention pattern, they have no these are out there.
They're sort of in almost all functional ways like a tiny little attention head and there's exponentially many of them.
Turns out they're not gonna be that important in this model, but in other models they can be important.
Right, so one thing that I said is it allows us to think about attention heads in a really principled way.
We don't have to go and think about, I think there's like people look at attention patterns all the time and I think a concern you have as well, there's multiple attention patterns, like the information that's being moved by one attention head, it might've been moved there by another attention head and not originated there.
It might still be moved somewhere else.
But in fact, this gives us a way to avoid all of those concerns and just think about things in a single principled way.
Okay, in any case, an important question to ask is how important are these different terms?
Like we could study all of them, how important are they?
And it turns out, there's an algorithm you can use where you knock out attention, knock out these terms and you go and you ask how important are they?
And it turns out that by far the most important thing is these individual attention head terms in this model, by far the most important thing.
The virtual attention heads basically don't matter that much.
They only have an effective 0 .3 nats using to the above ones and the bigrams are still pretty useful.
So if we want to try and understand this model, we should probably go and focus our attention on, you know, the virtual attention heads are not gonna be the best way to go in and focus our attention especially since there's a lot of them.
There's 144 of them for 0 .3 nats, very little that you would understand per studying one of those terms.
So the thing that we probably wanna do, we know that these are bigram statistics.
So what we really wanna do is we wanna understand the individual attention head terms.
This is the algorithm, I'm gonna skip over it for time.
We can ignore that term because it's small.
And it turns out also that the layer two attention heads are doing way more than layer one attention heads.
And that's not that surprising.
Like the layer two attention heads are more expressive because they can use the layer one attention heads to construct their attention patterns.
Okay, so if we could just go and understand the layer two attention heads, we probably understand a lot of what's going on in this model.
And the trick is that the attention heads are now constructed from the previous layer rather than just from the tokens.
So this is still the same, but the attention head, the attention pattern is more complex.
And if you write it out, you get this complex equation that says, you know, you embed the tokens and you go and you shuffle things around using the attention ends for the keys, then you multiply it by WQK, then you multiply, shuffle things around again for the queries and then you go and multiply by the embedding again because they were embedded and then you get back to the tokens.
But let's actually look at them.
So one thing that's, remember that when we see positive eigenvalues in the OB circuit, we're doing copying.
So one thing we can say is, well, seven out of 12, and in fact, the ones with the largest eigenvalues are doing copying.
So we still have a lot of attention heads that are doing copying.
And yeah, the QK circuit, so one thing you could do is you could try to understand things in terms of this more complex QK equation, but also just try to understand what the attention patterns are doing empirically.
So let's look at one of these copying ones.
I've given it the first paragraph of Harry Potter and we can just look at where it attends.
And something really happened, interesting happens.
So almost all the time, we just attend back to the first token.
We have this special token at the beginning of the sequence.
And we usually think of that as just being a null attention operation.
It's a way for it to not do anything.
In fact, if you look, the value vector is basically zero.
It's just not copying any information from that.
But whenever we see repeated texts, something interesting happens.
So when we get to Mr., tries to look at and, it's a little bit weak.
Then we get to D and it attends to Urs.
That's interesting.
And then we get to Urs and it attends to Lee.
And so it's not attending to the same token.
It's attending to the same token shifted one forward.
Well, that's really interesting.
And there's actually a lot of attention heads that are doing this.
So here we have one where now we hit the potters, pot and we attended to Urs.
Maybe that's the same attention head.
I don't remember when I was constructing this example.
It turns out this is a super common thing.
So you go and you look for the previous example, you shift one forward and you're like, okay, well, last time I saw this, this is what happened.
Probably the same thing is gonna happen.
And we can go and look at the effect that the attention head has on the logits.
Most of the time it's not affecting things, but in these cases, it's able to go and predict when it's doing this thing of going and looking one forward, it's able to go and predict the next token.
So we call this an induction head.
An induction head looks for the previous copy, looks forward and says, ah, probably the same thing that happened last time is gonna happen.
You can think of this as being a nearest neighbors.
It's like an in context nearest neighbors algorithm.
It's going and searching through your context, finding similar things and then predicting that's what's gonna happen next.
The way that these actually work is, I mean, there's actually two ways, but in a model that uses rotary attention or something like this, you only have one.
You shift your key.
First you have an earlier attention hand shifts your key forward one.
So you would like take the value of the previous token and you embed it in your present token.
And then you have your query and your key go and look at, yeah, try to go and match.
So you look for the same thing and then you go and you predict that whatever you saw is gonna be the next token.
So that's the high level algorithm.
Sometimes you can do clever things where actually it'll care about multiple earlier tokens and it'll look for like short phrases and so on.
So induction heads can really vary in how much of the previous context they care about or what aspects of the previous context they care about.
But this general trick of looking for the same thing, shift forward, predict that is what induction heads do.
Lots of examples of this.
And the cool thing is you can now, you can use the QK eigenvalues to characterize this.
You can say, well, we were looking for the same thing shifted by one, but looking for the same thing if you expand through the attention nodes in the right way, that'll work out.
And we're copying.
And so an induction head is one which has both positive OV eigenvalues and also positive QK eigenvalues.
And so you can just put that on a plot and you have your induction heads in the corner.
So your OV eigenvalues, your QK eigenvalues, I think actually OV is this access, QK is this one access, doesn't matter.
And in the corner, you have your eigenvalues or your induction heads.
Yeah, and so this seems to be, well, okay, we now have an actual hypothesis.
The hypothesis is the way that that phase change we were seeing, the phase change is the discovery of these induction heads.
That would be the hypothesis.
And these are way more effective than regular, than this first algorithm we had, which was just sort of blindly copy things wherever it could be plausible.
Now we can go and like actually recognize patterns and look at what happened and predict that similar things are gonna happen again.
That's a way better algorithm.
Yeah, so there's other attention heads that are doing more local things.
I'm gonna go and skip over that and return to our mystery, because I am running out of time.
I have five more minutes.
Okay, so what is going on with this in context learning?
Well, now we have hypothesis, let's check it.
So we think it might be induction heads.
And there's a few reasons we believe this.
So one thing is gonna be that induction heads, well, okay, I'll just go over to the end.
So one thing you can do is you can just ablate the attention heads and it turns out you can color.
Here we have attention heads colored by how much they are an induction head.
And this is the start of the bump.
This is the end of the bump here.
And we can see that they, first of all, induction heads are forming.
Like previously, we didn't have induction heads here.
Now they're just starting to form here.
And then we have really intense induction heads here and here.
And the attention heads, where if you ablate them, you get a loss.
So we're looking not at the loss, but at this meta learning score, the difference between, or in context learning store, the difference between the 500th token and the 50th token.
That's all explained by induction heads.
Now we actually have one induction head that doesn't contribute to it.
Actually, it does the opposite.
So that's kind of interesting.
Maybe it's doing something shorter distance.
And there's also this interesting thing where they all rush to be induction heads and then they discover only a few went out in the end.
So there's some interesting dynamics going on there.
But it really seems like in these small models, all of in context learning is explained by these induction heads.
Okay, what about large models?
Well, in large models, it's gonna be harder to go and ask this, but one thing you can do is you can ask, okay, we can look at our in context learning score over time, we get this sharp phase change.
Oh, look, induction heads form at exactly the same point in time.
So that's only correlational evidence, but it's pretty suggestive correlational evidence, especially given that we have an obvious, like the obvious effect that induction heads should have is this.
I guess it could be that there's other mechanisms being discovered at the same time in large models, but it has to be in a very small window.
So really suggests the thing that's driving that change is in context learning.
Okay, so obviously induction heads can go and copy text, but a question you might ask is, can they do translation?
Like there's all these amazing things that models can do that it's not obvious in context learning or the sort of copying mechanism could do.
So I just wanna very quickly look at a few fun examples.
So here we have an attention pattern.
Oh, I guess I need to open Lexiscope.
Let me try doing that again.
Sorry, I should have thought this through a bit more before this talk.
Chris, could you zoom in a little please?
Yeah, yeah, thank you.
So I'm not, my French isn't that great, but my name is Christopher, I'm from Canada.
What we can do here is we can look at where this attention head attends as we go and we do this.
And it'll become especially clear on the second sentence.
So here we're on the period and we tend to show.
Now we're on and just is I in French.
Now we're on the I and we attend to sweet.
Now we're on the am and we attend to do, which is from, and then from to Canada.
And so we're doing a cross -lingual induction head which we can use for translation.
And indeed, if you look at examples, this is where it seems to be a major driving force in the model's ability to go and correctly do translation.
Another fun example is I think maybe the most impressive thing about in context learning to me has been the model's ability to go and learn arbitrary functions.
Like you just show the model a function and can start mimicking that function.
Well, okay.
I have a question.
Yes.
So do these induction heads only do kind of a look ahead copy or like, can they also do some sort of like a complex structure recognition?
Yeah, yeah.
So they can both use a larger context, previous context and they can copy more abstract things.
So like the translation one is showing you that they can copy rather than the literal token, a translated version.
So it's what we might call soft induction head.
And yeah, you can have them copy similar words.
You can have them look at longer contexts.
It can look for more structural things.
The way that we usually characterize them is whether in large models, just whether they impurely behave like an induction head.
So the definition gets a little bit blurry when you try to encompass these more, this sort of a blurry boundary.
But yeah, there seem to be a lot of attention heads that are doing sort of more and more abstract versions.
And yeah, my favorite version is this one that I'm about to show you, which is used, let's isolate a single one of these, which can do pattern recognition.
So it can learn functions in the context and learn how to do it.
So I've just made up a nonsense function here.
We're gonna encode one binary variable with the choice of whether to do a color or a month as the first word.
Then we're gonna say we have green or June here.
Let's zoom in more.
So we have color or month and animal or fruit, and then we have to map it to either true or false.
So that's our goal.
And it's gonna be an XOR.
So we have the binary variable represented in this way.
We do an XOR.
I'm pretty confident this was never in the training set because I just made it up and it seems like a nonsense problem.
Okay, so then we can go and ask, can the model go and push that?
Well, it can, and it uses induction heads to do it.
And what we can do is we can look at the, so we look at a colon where it's gonna go and try and predict the next word.
And for instance, here, we have April dog.
So it's a month and then an animal, and it should be true.
And what it does is it looks for a previous cases where there was an animal, a month and then an animal, especially one where the month was the same and goes and looks and says that it's true.
And so the model can go and learn a function, a completely arbitrary function by going and doing this kind of pattern recognition induction head.
And so this to me made it a lot more plausible that these models actually can do in context learning.
Like the generality of all these amazing things we see these large language models do can be explained by induction heads.
We don't know that.
It could be that there's other things going on.
It's very possible that there's lots of other things going on, but it seems a lot more plausible to me than it did when we started.
I'm conscious that I am actually over time, I'm gonna just quickly go through these last few slides.
Yeah, so I think thinking of this as like an in context nearest neighbors, I think is a really useful way to think about this.
Other things could absolutely be contributing.
This might explain why transformers do in context learning over long context better than LSTMs.
And LSTM can't do this because it's not linear in the amount of compute it needs.
It's like quadratic or N log N if it was really clever.
So transformers are LSTMs impossible to do this, transformers do do this.
And actually they diverge at the same point that if you look, well, I can go into this in more detail after if you want.
There's a really nice paper by Marcus Hutter explaining, trying to predict and explain why we observe scaling laws and models.
It's worth noting that the arguments in this paper go exactly through to this example, this theory.
In fact, they sort of work better for the case of thinking about this in context learning with essentially a nearest neighbors algorithm than they do in the regular case.
So yeah, I'm happy to answer questions.
I can go into as much detail as people want about any of this.
And I can also, if you send me an email, send me more information about all of this.
And yeah, and again, this work is not yet published and you don't have to keep it secret, but just if you could be thoughtful about the fact that it's unpublished work and probably is a month or two away from coming out, I'd be really grateful for that.
Thank you so much for your time.
Yeah, thanks a lot Chris.
This was a great talk.
Thanks Chris.
So I'll just open with some general questions and then we can do like a round of questions from the students.
Sure.
So I was very excited to know like, so what is the line of work that you're currently working on?
Is it like extending this?
So what do you think is like the next things you try to do to make it more interpretable?
What are the next?
I mean, I want to just reverse engineer language models.
I want to figure out the entirety of what's going on in these language models.
And you know, like one thing that we totally don't understand is MLP layers, more, we understand some things about them, but we don't really understand MLP layers very well.
There's a lot of stuff going on in large models that we don't understand.
I want to know how models do arithmetic.
I want to know, I think that I'm very interested in is what's going on when you have multiple speakers, the model can clearly represent, like it has like a basic theory of mind, multiple speakers in a dialogue.
I want to understand what's going on with that.
But honestly, there's just so much we don't understand.
It's really, it's sort of hard to answer the question because there's just so much to figure out.
And we have a lot of different threads of research in doing this, but yeah.
The interpretability team at Anthropic is just sort of has a bunch of threads trying to go and figure out what's going on inside these models and sort of a similar flavor to this of just trying to figure out how do the parameters actually encode algorithms and can we reverse engineer those into meaningful computer programs that we can understand.
Well, another question I had is like, so you were talking about like how like the transformers are planning to do metal learning in that change.
So it's like, and you spend a lot of time talking about like the induction heads and like that was like very interesting, but like, can you formalize those sort of metal learning algorithm they might be learning?
Is it possible to say like, oh, maybe this is a sort of like internal algorithm that's going that's making them like good metal learners or something like that?
I don't know.
I mean, I think, so I think that there's roughly two algorithms.
One is this algorithm we saw in the one layer model and we see it in other models too, especially early on, which is just try to copy, you saw a word, probably a similar word is gonna happen later.
Look for places that it might fit in and increase the probability.
So that's one thing that we see.
And the other thing we see is induction heads, which you can just summarize as in context nearest neighbors basically.
And it seems possibly there's other things, but it seems like those two algorithms and the specific instantiations that we are looking at seem to be what's driving in context learning.
That would be my present theory.
Yeah, it sounds very interesting.
Yeah, okay.
So let's open like a round of first three questions.
So yeah, please go ahead for questions.
