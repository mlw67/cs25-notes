谢谢大家邀请我来。
在这里真是令人兴奋。
我最喜欢的事情之一就是讨论神经网络内部发生的事情，或者至少是我们试图弄清楚神经网络内部发生了什么。
所以聊起这个总是很有趣的。
天哪，我必须弄清楚如何做事情。
好的，怎么办？
我不会，好的，我们开始了。
现在我们正在切换幻灯片，这看起来很有希望。
所以我认为“可解释性”对不同的人来说意味着许多不同的事情。
这是一个非常广泛的术语，人们对此有各种不同的理解。
因此，我想简要谈谈我花时间思考的解释性类型，我称之为机械解释性。
所以我的大部分工作实际上并不是关于语言模型、RNN或transformer，而是关于理解视觉信心，并试图理解这些模型中的参数如何实际映射到算法。
因此，您可以将神经网络的参数视为编译后的计算机程序。
神经元有点像变量或寄存器。
而一些复杂的计算机程序嵌入在这些权重中。
我们想要将它们转换成人类可以理解的计算机程序。
这是一种逆向工程问题。
这是一个有趣的例子，我们发现了一个车辆神经元，你可以实际上看到我们有车辆神经元，它是由轮子神经元构成的，并且它在轮子神经元的情况下寻找底部的车轮。
这些是正权重，它不希望在顶部看到它们。
所以它在那里有负权重。
还有一个窗户神经元。
它寻找顶部的窗户而不是底部的窗户。
所以我们实际上看到的是一个算法。
这是一个算法，它进行转换。
你知道，它只是说，你知道，一辆车在底部有轮子，在顶部有窗户，在中间有Chrome。
这实际上就像是最强大的神经元。
所以我们实际上看到了一个有意义的算法。
这不是一个例外。
这差不多是一个常规的故事，如果你愿意去查看神经网络的权重，并且你愿意投入大量的精力去尝试表征它们，那么等待你发现的权重中就有一些有意义的算法。
而且我认为有很多原因值得思考。
其中一个原因是，你知道的，就是没有人知道如何去做神经网络可以做的事情。
就像没有人知道如何编写一个可以准确分类图像网的计算机程序，更不用说我们正在做的语言建模任务了。
没有人知道如何直接编写一个可以像 GPT-3 一样完成任务的计算机程序。
但是某种程度上，破解梯度下降算法能够发现一种方法来做到这一点。
我想知道发生了什么。
我想知道，你知道，它是如何在这些系统中发现自己能做什么的？
还有另一个我认为这很重要的原因，那就是安全性。
所以，你知道，如果我们想要在那些对世界产生重大影响的地方使用这些系统，我认为我们需要问自己的一个问题是，当这些模型出现意外故障时会发生什么？ 
我们不知道要去测试或寻找或检查的故障模式。
我们如何发现这些事情呢？
特别是如果它们是非常病态的故障模式。
因此，在某种意义上，模型故意做着我们不想要的事情。
那么，我真正看到的唯一方法是，如果我们能够真正理解这些系统内部发生了什么。
这也是我对此感兴趣的另一个原因。
现在，对语言模型和transformer进行解释性分析对我来说是新的。
在今年之前，我花了大约八年的时间试图逆向工程大陆和视觉模型。
因此，这次演讲中的想法是我与合作者们一起思考的新事物。
而且，我们可能还需要一个月或两个月，甚至更长时间才能将它们发布出来。
这也是我第一次公开演讲。
我要谈论的事情，我觉得，说实话，对我来说仍然有点困惑，而且在我表达它们时肯定会感到困惑。
所以如果我说的话让人困惑，请随时问我问题。
可能有一些要快速处理的地方，因为内容很多，但肯定在最后，我会有一段时间来聊这些事情。
是的，如果我对Zoom不熟悉并且犯错，我也要道歉。
但是，话虽如此，让我们深入讨论。
所以，我想从一个谜题开始。
在我们尝试深入了解这些模型内部发生了什么之前，我想通过一个非常奇怪的行为来激发一下兴趣，并希望能够理解。
顺便说一句，我要说所有这些工作都是与我的同事在Anthropic一起完成的，特别是我的同事Catherine和Nelson。
好的，接下来是这个谜题。
我认为transformer最有趣、最令人兴奋的事情可能是它们具有的上下文学习的能力，有时人们会称其为元学习。
GPT-3的论文描述语言模型为少样本学习者。
GPT-3有很多令人印象深刻的地方，但他们选择专注于这一点。
现在大家都在谈论提示工程，Andre Caprothy开玩笑说，软件3.0就是设计提示。
因此，这些大型transformer语言模型对其上下文的响应能力以及从上下文中学习和改变其行为与响应的能力，实际上似乎是它们最令人惊讶、引人注目和卓越的特点。
而我的一些同事之前发表了一篇论文，其中有一个我非常喜欢的技巧，那就是，我们都习惯于查看学习曲线。
你训练你的模型，随着模型的训练，损失下降。
有时它有点不连续，它下降了。
另一件事情是，你可以拿一个完全训练好的模型，然后你可以去问，当我们通过上下文、当我们预测第一个标记、然后第二个标记和第三个标记时，我们在预测每个标记时变得更加熟练，因为我们有更多的信息来进行预测。
因此，第一个标记，损失应该是单字的熵，然后下一个标记应该是双字的熵，它下降了，但它不断下降并且变得更好。
从某种意义上说，这就是模型进行上下文学习的能力。
能够预测并且在预测后面的词语方面比在预测前面的词语方面表现更好，从某种意义上说，这是对这种神奇的上下文学习或元学习在数学上的定义，这些模型能够做到这一点，这相当于对这些模型在上下文学习方面表现良好的数学定义。
所以这很酷，因为这给了我们一种方法去检验模型在上下文学习方面是否表现良好。
如果我只是问一下，像是一个澄清问题可能是我最好的选择。
当你说学习时，这里并没有发生实际的参数更新。
这就是上下文学习的显著之处。
是的，实际上，我们传统上认为神经网络通过在训练过程中修改其参数来学习，但不知何故，模型似乎也能在某种程度上学习。
如果你给它们一些上下文中的例子，它们就能在后续上下文中做同样的事情，即使参数没有改变。
所以这是某种不同于你所暗示的学习概念。
好的，我觉得这样更有意义了。
那么，我的意思是，你也可以在这种情况下将上下文学习描述为条件，就像是在一个10个词的句子中对前五个词进行条件设置一样吗？
预测接下来的五个标记？
是的，我认为人们有时会将这看作是上下文学习或元学习的原因是，您可以执行一些操作，例如将训练集嵌入到上下文中，例如只是两到三个示例。
然后，您的模型突然可以执行此任务。
因此，您可以通过将事物嵌入上下文中来进行少量学习。
是的，正式的设置是您只是在这个上下文中进行条件设置。
而且就是这种能力，就像这种东西，有一些感觉，你知道，很长一段时间，人们一直在，我的意思是，我猜真正的历史是我们开始擅长神经网络学习，对吧？
然后我们可以训练语言，训练视觉模型和能够执行所有这些引人注目的事情的语言模型。
但后来人们开始说，嗯，你知道，这些系统所需的例子比人类多得多，以便学习。
我们如何解决这个问题呢？
于是我们产生了关于元学习的所有这些想法，我们想要明确地训练模型，以能够从少量示例中学习，并且人们制定了所有这些复杂的方案。
然后，关于transformer语言模型真正荒谬的一点是，毫不费力就能做到。
我们可以免费得到这个，但你可以只是给他们一些例子，并在它们的上下文中学习，它们可以在自己的上下文中学习并学会做新事物。
我认为，那就像，就在某种意义上，GPT-3论文中最引人注目的地方。
因此，这种能力，仅仅基于上下文的条件，就能给你，你知道，新的能力，而且对新事物进行泛化的能力，在某种意义上，是最，是的，对我来说，transformer语言模型最引人注目和令人震惊的事情。
这是有道理的。
我是说，我想从我的角度来看，我试图将学习的概念与这种情况联系起来，你知道，如果给你或我一个类似的提示，比如一加一等于二，二加三等于五，作为几次尝试的设置，然后其他人提出了五加三等于，我们必须填写它。
在这种情况下，我不会说我们已经学会了算术，因为我们已经在某种程度上知道了，而是我们只是在提示上进行条件设置，以知道我们应该生成什么，对吧？
但在我看来，好像是这样的。
是的，我觉得这是一个谱，因为你也可以给出完全荒谬的问题，模型从未见过，比如模仿这个函数，并给出一些函数的例子，而模型以前从未见过它。
它可以在后续上下文中做到这一点。
我认为在很多情况下，你所学到的东西可能并不是算术，比如你可能有一些天生的算术能力，但你可能学到了，哦，好的，现在我们在做算术问题。
知道了。
无论如何，我同意这里有一个语义的元素。
是的，不，这很有帮助，可以澄清一下你在谈论学习时的具体意思。
谢谢你详细解释。
当然。
我认为真正引人注目的一点是，好吧，所以我们已经谈论了我们如何看待学习曲线，我们也可以将其视为上下文学习曲线，但实际上这只是二维空间中的两个切片。
所以在某种意义上，更基本的问题是，在训练的某一点上，我们有多擅长生成第n个标记？
而如果你看这个，你会注意到的一点是，当我们谈论损失曲线时，我们只是在谈论如果你在这个维度上取平均，如果你像这样取平均然后投射到训练步骤上，那就是你的损失曲线。
而我们所称之为上下文学习曲线的东西只是这条线，是的，就是这条在这里的线。
有点引人注目的是，有这种不连续性。
就像有这个点，模型似乎在很短的时间内变得更好，并且能够预测后面的标记。
因此，在早期时间步骤中并没有太大的区别，但在后期时间步骤中，突然间你变得更好了。
你可以使这一点更引人注目的一种方法是，你可以取你在预测第50个标记的能力和你在预测第500个标记的能力之间的差异，你可以从第500个标记的损失中减去第50个标记的损失。
你会看到，在训练过程中，你在这方面并不是很擅长，你有点进步，然后突然间你遇到了这个悬崖，然后你再也没有变得更好，至少在这两者之间的差异永远不会变得更好。
因此，模型在预测事物方面变得更好了，但它在预测晚期标记与早期标记之间的能力永远不会变得更好。
所以，在训练的几百个步骤中，模型在进行这种上下文学习方面的能力得到了显著提升。
所以你可能会问，那时发生了什么？
这只是一个模型，但首先值得注意的是，这不是一个小改变。
因此，你可以，我们经常不会考虑到这一点，但我们经常只是看法学院，然后我们会想，这个模型比另一个模型做得好还是差？
但你可以把这个想象成Nats，这只是信息论中的一个量，你可以把它转换成比特。
所以你可以解释这是一种大概如下的情况，模型0 .4 Nats大约是0 .5比特，大约是每个其他的令牌，而模型可以去取样两次并选择更好的一个。
事实上，它甚至更加强大。
这是一个低估了模型提高0 .4 Nats的重要性的一种方式。
所以这就像是模型预测后面的令牌的能力的一个真正的大差异。
我们可以用不同的方式来可视化这一点。
我们也可以去问，我们在预测后面的标记时，我们变得更好了多少，然后看看导数。
然后我们可以非常清楚地看到，在这一点上，导数中有某种不连续性。
然后我们可以取二阶导数，然后，我们可以，相对于训练，导数。
现在我们看到，这里有一条线。
所以在几步之内，几百步之内，有一些东西引起了一些大的变化。
我们有某种相变正在发生。
这在模型大小上也是正确的。
你实际上可以在损失曲线中看到一点点，这里有一个小突起，这对应于你有这个，你有这个变化的点。
我们实际上也可以在损失曲线中更早地看到。
这里有一个突起。
对不起。
所以我们有这种相变正在发生，有，我认为，一个非常诱人的理论，即无论如何，有一些，这个模型的输出和它的行为中的变化，以及这些外向的特性对应，大概，到模型内部运行的算法的一些变化。
所以，如果我们观察到这个巨大的相变，尤其是在模型行为的一个非常小的窗口中，可以推测模型内部电路发生了变化，驱使着这种变化。
至少这是一个自然的假设。
所以，如果我们想要提出这个问题，我们需要去理解模型内部正在运行的算法是什么？
我们如何将模型中的参数转化回那些算法呢？
这将是我们的目标。
现在，这将需要我们在相对较短的时间内涵盖很多内容。
因此，在接下来的部分，我将会比较快速地进行，然后我将强调一些关键的要点，然后我将非常乐意深入探讨其中的任何内容。
在这通话之后，我还有一个小时的空闲时间，很乐意与大家讨论这方面的细节。
结果表明，相变并不会发生在单层注意力-唯一transformer中。
但是，在双层注意力-唯一transformer中却发生了相变。
因此，如果我们能理解单层注意力-唯一transformer和双层注意力-唯一transformer，这可能会给我们提供一个相当大的线索，关于发生了什么。
所以我们只关注注意力机制，我们还会省略层归一化和偏置，以简化事务。
描述注意力机制的一种方式是嵌入我们的标记，然后应用一系列的注意力头并将它们添加到残差流中，然后应用我们的去嵌入，这将给我们带来逻辑。
我们可以写出方程来表达这一点，乘以一个嵌入矩阵，应用注意力头，然后计算去嵌入的逻辑。
这里有一点棘手的部分是理解注意力头。
这可能是描述注意力头的一种比较传统的方式。
实际上，它有点掩盖了注意力头的很多结构。
我认为我们经常把注意力头弄得比它们实际上更复杂。
我们有点隐藏了有趣的结构。
那么这是什么意思呢？
嗯，它是说对于每个标记，计算一个值向量，然后根据注意力矩阵混合值向量，然后用输出矩阵将它们投影回残差流中。
所以还有另一种表示法，你可以将其视为使用张量积或使用，嗯，我想还有一些左右乘以它。
有几种解释方式，但我只是试图解释这个符号意味着什么。
这意味着对于每个 X，我们的残差流，我们都有一个向量与每个标记相关。
这意味着独立地计算每个标记的向量与 WV 相乘。
所以计算每个标记的值向量。
另一方面，这个意味着注意力矩阵现在位于左侧。
这意味着去乘以注意力矩阵或进行值、值向量的线性组合。
所以不要逐点改变值向量，而是根据注意力模式混合它们，创建加权和。
然后再次，独立地对每个位置应用输出矩阵。
你可以对此应用分配性质。
这只是揭示了实际上在中间做注意力并不重要。
你可以在开头做注意力，也可以在结尾做。
这是独立的。
而真正重要的是有一个描述 WV WO 矩阵的事物，它实际上表达的是，WV WO 描述了注意力头从每个位置读取和将信息写入其目标的方式。
而 A 则描述了我们从哪些标记读取和写入。
这更深入地涉及到注意头的基本结构。
一个注意头会将信息从一个位置移动到另一个位置。
而确定从哪个位置移动以及移动的信息是独立的。
如果按照这种方式重写您的transformer，首先，我们可以按照这个形式编写注意头的总和。
然后，我们可以通过添加一个身份来将其写成整个层。
如果我们将所有这些都插入到我们的transformer中并展开，我们必须逐步进行所有计算。
我们得到了这个有趣的方程。
因此，我们得到了一个术语，这对应于直接通过残差流的路径。
它会想要存储二元统计信息。
它只会得到前一个标记，并试图预测下一个标记。
于是它开始尝试预测，尝试存储二元统计数据。
然后对于每一个注意力头，我们得到一个矩阵，表明，好的，我们有了注意力模式。
因此，它看起来，描述了哪个标记看哪个标记。
我们有这个矩阵在这里，它描述了对于每一个可能的标记你可以关注，它如何影响逻辑。
那只是一个你可以查看的表格。
它只是说，对于这个注意力头，它关注这个标记，它会增加这些标记的概率。
在一个单层仅有注意力的transformer中，就只有这些了。
是的，所以这只是我描述的解释。
另一个值得注意的事情是，根据这个，如果你固定注意力模式，仅有注意力的transformer是线性的。
现在，当然，注意力模式并不是固定的，但是每当你有机会使某些东西变成线性时，线性函数就非常容易理解。
因此，如果你可以固定少量的东西并使其线性，那实际上，这是很大的杠杆。
好的。
是的，我们也可以谈谈注意力模式是如何计算的。
如果你将其展开，你会得到这样一个方程。
请注意，嗯，我认为这样会更容易。
好的。
我认为我从所有这些中汲取的核心故事是，我们有这两个看起来有点相似的矩阵。
这个告诉你，如果你关注一个标记，logits 会受到什么影响？
你可以把它想象成一个巨大的矩阵，对于每个可能的输入标记，logits 是如何受到该标记的影响的？
它们会更可能还是更不可能？
我们有这个，它有点像是说，每个标记有多想关注每个其他标记？
你可以这样想象它，好的，当我们考虑一个注意力网络时，实际上涉及到三个标记。
我们有一个我们将要向其传递信息的标记，它是向后关注的。
我们有一个源标记将被关注，以及一个输出标记，其 logit 将受到影响。
你可以跟踪这个过程。
所以你可以问，关注这个标记会对输出产生什么影响？
嗯，首先我们嵌入标记。
然后我们乘以 WV 得到值向量。
信息是通过注意力模式移动的。
我们乘以WO，将其添加回残差流，我们受到去嵌入的影响，我们影响到logit。
这就是那个矩阵的来历。
我们还可以问，是什么决定了在计算注意力模式时一个记号得到高分？
它就这么说，嵌入记号，将其转换为查询，嵌入另一个记号，将其转换为关键词，然后进行点积运算。
所以这就是那两个矩阵的来历。
我知道我说得很快。
也许我会在这里简短地停顿一下。
如果有人想要求解释，这是个好时机。
然后我们将实际进行逆向工程，说，一个单层注意力的transformer中正在发生的一切现在掌握在我们手中。
这是一个非常简单的模型。
你知道，实际上没有人使用单层注意力的transformer，但我们将能够理解这种单层注意力的transformer。
所以，就是为了明确一下，你是说快速关键电路正在学习注意力权重。
而且基本上，它是在各种记号之间运行注意力的赞助商吗？
是的，是的。
所以这个矩阵，当它，是的，所有这三部分都是学习的，但这就是表达了注意力模式是否，是的，这就是生成注意力模式的方式。
对于每对标记，都要运行它。
您可以将该矩阵中的值视为每个标记希望在上下文中参与每个其他标记的程度。
我们在这里做的是位置嵌入。
所以在那里有一点我们也在对齐的。
从某种全局意义上讲，每个标记想要参与到每个其他标记的程度有多大？
而另一个电路，就像输出值电路，正在使用计算出的注意力来，是的，影响最终的输出。
它有点在说，如果注意力头部，假设注意力头部关注某个标记。
所以让我们搁置如何计算这一问题。
只是假设它关注某个标记。
如果它关注该标记，它会如何影响输出？
您可以简单地计算出来。
这只是一个巨大的值表，表明了对于这个标记，它会使这个标记更有可能。
这个标记会使这个标记更不可能。
好的，好的，这很有趣。
而且它完全独立。
它只是两个独立的矩阵。
它们并不是，可能会让它们看起来相互纠缠的公式，但它们实际上是独立的。
对，对我来说，看起来好像是讲座监督来自于输出值电路，而查询键的分离似乎更像是一种无监督的事情。
所以没有。
我的意思是，我认为在某种意义上，就像每个神经元都在某种程度上，信号都是从最终信号的下游某种程度上传递的模型中一样。
因此，输出值电路可能获得的信号更直接。
但是，是的。
有趣。
我们将能够深入研究这个问题，以你想要的任何程度。
所以我们可以，也许我会继续前进。
而且我认为实际上如何使用这个来逆向工程一个单层模型的示例可能会使它更有动力。
好的，所以只是强调一下，我们可以讨论三种不同的标记。
有一个被关注的标记。
有一个进行关注的标记，被称为目的地。
然后有一个受影响的令牌，得到下一个令牌，其概率受到影响。
所以我们可以做的一件事是注意到，连接这两者的唯一令牌是被关注到的令牌。
所以这两者有点，它们通过与源令牌的交互而被桥接。
那么一个很自然的问题是，对于给定的源令牌，它如何与这两者互动？
比如说，让我们来看看令牌perfect。
我们可以问的一个问题是，哪些令牌想要关注perfect？
嗯，显然最想关注perfect的令牌是R和looks和is和provides。
R是最想关注的，looks是其次，依此类推。
然后当我们关注perfect时，这是通过单一的注意力边缘，如果我们使用不同的注意力边缘，情况会有所不同，它想真的增加perfect的概率，然后在较小程度上是super和absolute和pure。
我们可以问，通过这一组特定的事物，它们想要互相关注并变得更有可能，会使哪些令牌序列变得更可能？
好，事情是这样的，我们有我们回到的令牌，我们有一些跳过一些令牌的跳过，它们不必相邻，但后来我们看到令牌R，它回到了完美，并增加了完美的概率。
所以你可以把这些想象成是，我们有点像在创建，改变我们可能称之为跳过三元组的概率，在中间跳过了一堆令牌，但我们实际上影响的是三元组的概率。
所以完美，R完美，完美，看起来很棒。
我们可以看看另一个。
所以我们有令牌大，这些令牌包含使用指定，想要回头看看它，并增加大和小的概率以及受影响的跳过三元组，例如大，使用大，大包含小之类的东西。
如果我们看到数字二，我们增加其他数字的概率，并影响可能的令牌或跳过图表，例如二，一，二，二有三。
现在你们都在一个技术领域，所以你们可能会认出这个，我们有Lambda然后我们看到反斜杠然后我们想要增加Lambda和sorted和Lambda和operator的概率，所以这全是LaTeX。
它希望，如果它看到 Lambda，它会认为，也许下次我使用反斜杠时，我应该放入一些 LaTeX 数学符号。
同样对于 HTML，我们看到 NBSP 表示不间断空格，然后我们看到一个与号，我们希望使其更有可能。
从所有这些中得出的要点是，一个仅具有一层注意力的transformer完全是在处理这些跳过三元组。
它所做的一切，我是说，我猜它还有这种方式，通过它影响 bigrams，但主要是影响这些跳过三元组。
而且有很多，就像这些巨大的跳过三元组表，它们更有或更不可能出现。
它还有很多其他有趣的事情，有时标记化会以多种方式拆分一个词，就像我们有 Indy，好吧，这不是一个好的例子，我们有像单词 Pike 然后我们看到标记 P 然后我们预测 Ike 和我们预测 spikes 等等。
或者这些可能有趣，也许实际上值得稍微谈一谈，所以我们看到标记 Lloyd 然后我们看到一个 L 可能我们预测 Lloyd 或 R 然后我们预测 Ralph 和 C，Catherine。
但我们马上会看到，嗯，我们马上就回来。
所以我们增加了像劳埃德、劳埃德和劳埃德、凯瑟琳或者PixMap这样的事情的概率。
如果有人使用QT，我们会看到PixMap，然后增加P、再次出现PixMap的概率，但同时也增加了QCanvas的概率。
但是这当然存在问题，就是它无法确定哪一个对应哪一个。
所以，如果你想让PixMap、PixMap和PixMap、QCanvas更有可能，你还必须去创建，让PixMap、PCanvas更有可能。
如果你想让劳埃德、劳埃德和劳埃德、凯瑟琳更有可能，你还必须让劳埃德、Cloyd和劳埃德、Lathrin更有可能。
因此，实际上transformer存在一些bug，至少在这些非常小的仅有一个层注意力的transformer中存在这些奇怪的bug，直到你意识到这是一个巨大的跳过三元组表在起作用。
而这个现象的本质是，如果你想这样做，它会迫使你去做一些奇怪的预测。
克里斯，这里的源标记为什么在第一个字符之前有一个空格呢？
是的，这只是我举了一些例子，其中标记化以特定的方式中断。
因为空格被包含在标记化中，当某物前面有空格，然后有一个例子，其中该空格不在其前面时，它们可以以不同的方式进行标记化。
知道了，很酷，谢谢。
是的，很好的问题。
好的，所以只是为了抽象出一些我们正在看到的常见模式，我认为一个相当常见的事情是你可能描述为 B、A、B。
所以你去看到一些标记，然后你看到另一个可能在该标记之前的标记，然后你会想，可能之前看到的标记会再次出现。
或者有时你会预测一个略有不同的标记。
所以像一个例子，第一个是两，一，两，但你也可以做两有三。
所以三不同于两，但有点相似。
所以那是一件事。
另一个是这个例子，其中你有一个标记，有一次被一起标记化的东西，然后被分开。
所以你看到该标记，然后你看到可能是该标记的第一部分，然后你预测第二部分。
我认为真正引人注目的是，这些在某种程度上都是一种相当粗糙的上下文学习。
特别是，这些模型获得了大约 0.1 个 nats，而不是大约 0.4 个 nats 的上下文学习，它们从未经历过相变。
所以它们正在进行某种非常粗糙的上下文学习，而且它们几乎把所有的注意力都放在了这种粗糙的上下文学习上。
所以它们在这方面做得并不好，但它们正在将它们的容量投入其中。
我注意到现在是 1037。
我想检查一下我能讲多久，因为也许我应该超快速度进行，如果是的话。
哦，克里斯，我觉得没问题，因为学生也在中间提问。
所以你应该没问题。
好的，也许我的计划是，我会讲到 1055 或 11 点，然后如果你愿意，我可以在那之后回答一段时间的问题。
是的，可以。
太棒了。
所以你可以把这看作是一种非常粗糙的上下文学习。
基本上我们所说的是，这是一种所有的口味，好吧，我看到了这个令牌，可能其他令牌，相同的令牌或类似的令牌更有可能稍后出现，看，这是一个机会，看起来我可以注入我之前看到的令牌。
我要在这里插一句，说这更有可能。
就像，基本上它正在做的就是这样。
它几乎将所有的容量都用于此。
所以，你知道，这些，这有点是我们过去对于循环神经网络的看法的反义。
以前大家都认为，哦，你知道，循环神经网络，很难关心长距离联系。
也许我们需要去使用水坝或者其他什么。
不，如果你训练一个Transformer，它会专注于你给它的足够长的上下文。
它几乎将所有的容量都用于这种类型的事情。
挺有趣的。
有一些注意力是更加基于位置的。
通常我们，你知道，我一直在训练的模型只有一个层，它只有一个层，有12个注意力头。
而且通常有大约两到三个会变成这些更多基于位置的、更短期的东西，更像是，像是本地的三元统计。
然后其他的都会成为这些跳过三元组。
是的，所以从中可以得到一些要点。
是的，你可以，你可以从内部把一个层次的Transformer理解为这些OV和QK电路。
Transformers 极其渴望进行上下文学习。
它们极其、极其、极其想要去查看这些远距离的联系并预测事物。
它们可以从中减少的信息量非常之大。
在transformer上一层注意力的约束下，迫使其产生一些想要做正确事情的错误。
如果你冻结注意力模式，这些模型就是线性的。
一个快速的旁注，因为到目前为止，这种类型的工作要求我们进行大量的手工检查。
就像我们正在查看这些巨大的矩阵一样，但是我们可以摆脱它。
如果我们不想要，我们不必查看这些巨大的矩阵。
我们可以使用特征值和特征向量。
所以回忆一下，特征值和特征向量就意味着，如果你将该向量乘以矩阵，它相当于只是缩放。
在我的经验中，它们通常对于可解释性并不是很有用，因为我们通常在不同的空间之间进行映射。
但是如果你在同一个空间进行映射，特征值和特征向量是一种很好的思考方式。
那么我们将在一个径向图上绘制它们，而且我们将使用对数径向刻度，因为它们的变化幅度很大，它们的数量级将变化多个数量级。
好的，所以我们可以直接从令牌到令牌映射我们的OB电路，这是输入和输出上相同的向量空间。
我们可以问，如果我们看到特定类型的特征值，那意味着什么？
嗯，正特征值，这实际上是最重要的部分，意味着复制。
所以如果你有一个正特征值，这意味着有一些令牌集，如果你看到它们，你会增加它们的概率。
如果你有很多正特征值，你就在进行大量复制。
如果你只有正特征值，你所做的一切都是复制。
现在，虚部特征值意味着你看到一个令牌，然后你想增加与其无关的令牌的概率。
最后，负特征值是反复制。
它们就像是，如果你看到这个令牌，你会使它在未来变得不太可能。
那真是太好了，因为现在我们不必查看这些巨大的矩阵，其大小为词汇量乘以词汇量。
我们只需查看特征值。
所以这些是我们的单层注意力 -only transformer 的特征值。
我们可以看到，对于许多这些，它们几乎完全是正的。
这些是完全是正的。
这些几乎完全是正的。
实际上，这些几乎完全是正的。
只有两个具有大量虚数和负特征值。
这告诉我们的是它只是在一个图片中。
我们可以看到，好吧，这真的是 12 个注意力头中有 10 个只是在复制。
他们只是在做这种长距离，嗯，我看到一个标记，可能会再次出现这种事情。
这有点酷。
我们可以很快地总结一下。
现在，另一件事情，你可以，是的，这是第二个。
我们马上要看一个两层模型，我们会看到它的许多注意力头也在做这种复制或其他事情，它们有较大的正特征值。
你可以做一个直方图。
一个酷的事情是你可以把特征值加起来，然后除以它们的绝对值。
你有一个介于零和一之间的数字，就像复制只是头部或者在负一和一之间，复制只是头部。
你可以做一个直方图，你会看到，哦，是的，几乎所有的头部都在大量复制。
能够去总结我们的模型是件好事。
我认为这有点像，我们从一个非常自下而上的方式开始，我们没有假设模型在做什么。
我们试图理解它的结构，然后我们能够以有用的方式对其进行总结。
现在我们能够对此发表一些看法。
现在，你可能会问的另一件事是QK电路的特征值是什么意思？
到目前为止，根据我们的例子，它们可能不是那么有趣，但一分钟后它们将变得有趣起来。
因此，我将简要描述它们的含义。
正特征值意味着你想要关注相同的标记。
想象一下你的特征值，这是我们到目前为止所见过的模型中大多数情况下会看到的，意味着你想要去关注一个不相关或不同的标记。
负特征值意味着你想要避免关注相同的标记。
所以这一点将在一秒钟内变得相关。
是的，所以这些在多层注意力和transformer中思考时会非常有用，我们可以在注意力头之间形成链条。
所以我们可以问，嗯，我马上就会讲到。
这是一个总结的表格。
不幸的是，一旦有MLP层，这种方法就完全失效了。
MLP层，现在你有了这些非线性，因为你不再具有模型大部分是线性且可以只看矩阵的属性。
但是如果你只使用注意力transformer，这是一种非常好的思考方式。
好的，所以要记住，单层注意力transformer在开始时不会经历我们谈论过的这种阶段性变化。
嗯，现在我们正在寻找。
我们正在努力回答这个谜团，就是在那个阶段性变化中，模型突然变得擅长上下文学习的是什么。
我们想要解答这个问题。
单层注意力transformer不经历那种阶段性变化，但是双层注意力transformer会经历。
所以我们想知道双层注意力transformer有什么不同。
好的, 那么在我们之前, 当我们处理单层注意力transformer时, 我们能够以这种形式进行重写。
这给了我们很多能力去理解模型，因为我们可以说，这是二元组，然后每个都在某个地方查找，我们有一个描述它如何影响事物的矩阵。
是的，这给了我们很多思考这些事情的能力。
我们还可以以这种分解形式进行书写，其中我们有嵌入，然后有注意力头，然后有非嵌入。
嗯，哦，并且为了简单起见，我们经常将 WOV 写成 WOWV，因为它们总是一起出现。
这总是情况，就像 WO 和 WV 是不同矩阵的一种幻觉。
它们只是一个低秩矩阵。
它们从不，它们总是一起使用。
同样，WQ 和 WK，它们是不同矩阵的一种幻觉。
它们总是一起使用。
而密钥和查询只是某种程度上这些低秩矩阵的产物。
总之，以这种方式进行书写是有用的。
好的，很好。
所以一个仅有两层注意力的transformer，我们要做的是先通过嵌入矩阵，然后经过第一层注意力头，然后经过第二层注意力头，然后再经过去嵌入。
而对于注意力头，我们始终有这个身份，它对应着沿着残差流向下走。
所以我们可以沿着残差流向下走，或者我们可以通过一个注意力头。
接下来，我们也可以沿着残差流向下走，或者我们可以通过一个注意力头。
还有这个有用的身份，混合乘积身份，任何张量积或其他解释方式都遵循的规则，即如果你有一个注意力头，我们有权重和注意力模式以及WOV矩阵和注意力模式，注意力模式相乘，OV电路相乘，它们表现得很好。
好的，很好。
所以我们可以展开那个方程。
我们可以展开一开始的那个大乘积，我们可以把它展开，然后得到三种不同类型的项。
所以我们做的一件事是获得这条直接通过残差流的路径，我们嵌入和去嵌入，这将表示一些二元统计信息。
然后我们得到类似于先前的注意力头术语的东西。
最后，我们得到这些术语，对应于经过两个注意力头。
现在值得注意的是，这些术语实际上并不相同，因为注意力头，第二层的注意力模式可以从第一层的输出中计算得到，它们也可能更具表现力，但在高层次上，你可以认为有这三种不同类型的术语。
有时我们称这些术语为虚拟注意力头，因为它们在模型中并不存在，就像它们在模型中并没有明确地表示一样，但实际上，它们有一个注意力模式，它们确实存在。
它们在几乎所有功能方面都像一个微小的注意力头，而且有指数级别的数量。
事实证明，在这个模型中它们并不会太重要，但在其他模型中它们可能很重要。
对，所以我说的一件事是，它让我们以一种非常原则性的方式思考注意力头。
我们不必去考虑，我认为人们一直在关注注意力模式，我认为你也有这样的担忧，有多种注意力模式，例如，一个注意力头传递的信息可能是由另一个注意力头传递的，而不是由它原始生成的。
它可能仍然会被移动到其他地方。
但事实上，这给了我们一种方法来避开所有这些担忧，只是以一种单一的原则方式来考虑事物。
好吧，在任何情况下，一个重要的问题是要问这些不同术语有多重要？
我们可以研究所有这些，它们有多重要？
事实证明，有一种算法可以使用，你可以排除注意力，排除这些术语，然后询问它们有多重要？
事实证明，在这个模型中，个别注意力头术语是最重要的，远比其他任何东西都重要。
虚拟注意力头基本上并不那么重要。
它们只有0.3个nats的有效性与以上的那些相比，并且双字母词仍然非常有用。
所以如果我们想要尝试理解这个模型，我们应该专注于，你知道的，虚拟注意力头不太可能是我们集中注意力的最佳方式，特别是因为它们有很多。
有144个0.3 nats的，每个学习其中一个术语，你能理解的很少。
所以我们可能想做的事情是，我们知道这些是二元统计数据。
所以我们真正想做的是理解各个注意力头术语。
这就是算法，我会因时间原因跳过它。
我们可以忽略那个术语，因为它很小。
而且事实证明，第二层的注意力头做的比第一层的多得多。
这并不奇怪。
就像第二层的注意力头更具表现力，因为它们可以利用第一层的注意力头来构建它们的注意力模式。
好的，所以如果我们能够理解第二层的注意力头，我们可能就能理解这个模型中发生的大部分情况。
而诀窍在于，注意力头现在是从前一层构建而来的，而不仅仅是从标记构建而来。
所以这仍然是相同的，但是注意力头，注意力模式更加复杂。
如果你写出来，你会得到这个复杂的方程，它说，你知道，你嵌入了标记然后你去，你用关键字重新排列事物，然后你将其乘以WQK，然后你乘以，再次重新排列查询的事物然后你去乘以嵌入，因为它们被嵌入了然后你回到标记。
但是让我们实际看一看它们。
所以有一件事情是，记住，当我们在OB电路中看到正的特征值时，我们正在复制。
所以我们可以说的一件事是，好吧，12个中有7个，事实上，特征值最大的那些正在复制。
所以我们仍然有很多注意力头在做复制。
是的，QK电路，所以你可以做的一件事是，你可以试图用这更复杂的QK方程来理解事物，但也试图从经验上理解注意力模式在做什么。
所以让我们看看其中一个复制的。
我已经给它了《哈利·波特》的第一段，我们可以看一看它关注的地方。
而且确实发生了一些有趣的事情。
所以几乎所有的时间，我们只是回到第一个标记。
我们在序列开头有这个特殊的标记。
我们通常把它看作是一个空的注意力操作。
这是一种让它什么都不做的方式。
事实上，如果你看一下，数值向量基本上是零。
它只是不复制任何来自那个位置的信息。
但是每当我们看到重复的文本时，有趣的事情发生了。
所以当我们遇到Mr.时，试图查看和，它有点弱。
然后我们到达D，它关注Urs。
这很有趣。
然后我们到达Urs，它关注Lee。
所以它不是关注相同的标记。
它关注的是相同标记向前移动了一个。
嗯，这真的很有趣。
实际上有很多注意力头在做这件事。
在这里，我们有一个例子，现在我们碰到了potters，锅炉，我们关注了Urs。
也许这是同一个注意力头。
我不记得我构建这个例子时是什么时候。
事实证明这是一种非常常见的事情。
所以你去找之前的例子，你往前挪一个，然后你会说，好吧，上次我看到这个，发生了这个。
很可能同样的事情会发生。
然后我们可以看看注意力头对logits的影响。
大多数时候它不会影响事情，但在这些情况下，它能够预测到当它这样往前看一个时，它能够预测到下一个标记。
所以我们称之为感应头。
感应头寻找先前的副本，向前看并说，啊，很可能上次发生的事情会再次发生。
你可以把这想象成是最近邻的一种。
就像是上下文最近邻算法。
它在上下文中搜索，找到相似的事物，然后预测接下来会发生什么。
这实际上有两种工作方式，但在使用旋转注意力或类似方法的模型中，你只有一种。
你挪动你的关键。
首先，你有一个较早的注意力头，将你的关键向前挪动一个。
所以你会取先前标记的值，并嵌入到你的当前标记中。
然后你有你的查询和你的关键词去查找，是的，尝试去匹配。
所以你寻找相同的东西，然后你去预测你所看到的东西会是下一个标记。
所以这就是高级算法。
有时候你可以做一些聪明的事情，实际上它会关注多个较早的标记，它会寻找像短语等等。
所以归纳头部在关心前面的上下文或前面的上下文的哪些方面上真的可以变化很大。
但是这种寻找相同的东西，向前移动，预测的一般技巧就是归纳头部所做的。
有很多这样的例子。
而酷炫的是你现在可以，你可以用QK特征值来表征这个。
你可以说，嗯，我们在寻找相同的东西，向前移动了一个，但是如果你通过正确的方式扩展注意力节点，寻找相同的东西，那么就会奏效。
而我们正在复制。
所以一个归纳头部是一个既具有正OV特征值又具有正QK特征值的头部。
所以你只需把它放在一个图上，你的归纳头部就在角落里。
所以您的OV特征值，您的QK特征值，我认为实际上OV是这个轴，QK是这个轴，都无关紧要。
在角落里，您有您的特征值或感应头。
是的，所以这似乎是，嗯，好吧，我们现在有一个实际的假设。
假设是我们看到的那种相变，相变就是发现这些感应头。
那将是假设。
而且这些比我们之前的第一个算法更有效，那个算法只是在可能的地方盲目地复制东西。
现在我们可以去真正地识别模式，并查看发生了什么，并预测类似的事情将再次发生。
这是一个更好的算法。
是的，所以还有其他一些关注头在做更多的本地事情。
我要跳过那个，回到我们的谜题，因为时间不多了。
我还有五分钟。
好的，那么这个上下文学习到底是怎么回事？
嗯，现在我们有了假设，让我们来检查一下。
所以我们认为可能是感应头。
有几个原因我们相信这一点。
所以一个问题是归纳头，嗯，好吧，我就直接跳到最后。
所以你可以做的一件事是只消除注意力头，结果你可以着色。
这里我们有按归纳头程度着色的注意力头。
这是凸起的开始。
这是这里凸起的结束。
我们可以看到，首先，归纳头正在形成。
就像之前，我们这里没有归纳头。
现在它们刚开始在这里形成。
然后我们在这里和这里有非常强烈的归纳头。
而注意力头，如果你消除它们，你会得到一个损失。
所以我们不是在看损失，而是在看这个元学习分数，第500个标记和第50个标记之间的差异，或者在上下文学习存储中的差异。
这一切都可以通过归纳头来解释。
现在我们实际上有一个归纳头不对其产生影响。
事实上，它做的是相反的。
所以这有点有趣。
也许它在做一些较短的距离的事情。
还有一件有趣的事情，他们都争先恐后地成为感应头，然后他们发现最终只有少数人成功了。
所以那里有一些有趣的动态在发生。
但实际上，在这些小型模型中，所有上下文学习都可以通过这些感应头来解释。
好的，那么大型模型呢？
嗯，在大型模型中，要去询问这个会更加困难，但你可以做的一件事是，你可以询问，好的，我们可以看一下我们随时间变化的上下文学习得分，我们得到这个明显的相变。
哦，看，感应头确实是在同一时间点形成的。
所以这只是相关的证据，但这是相当有启发性的相关证据，特别是考虑到我们有一个明显的，像感应头应该产生的明显效果。
我想可能是在大型模型中同时发现了其他机制，但这必须在一个非常狭窄的时间窗口内。
所以真的暗示着引起这种变化的东西是上下文学习。
好的，显然感应头可以复制文本，但你可能会问的一个问题是，它们能进行翻译吗？
就像模型可以做的所有这些令人惊奇的事情一样，在上下文学习或复制机制中并不明显。
所以我只是想快速看几个有趣的例子。
这里我们有一个注意力模式。
哦，我猜我需要打开Lexiscope。
让我再试一次。
抱歉，在这次演讲之前我应该更深思熟虑一些。
克里斯，你能放大一点吗？
是的，谢谢你。
所以我的法语不是很好，但我的名字是克里斯托弗，我来自加拿大。
我们在这里可以看到当我们进行这样做时，这个注意力头关注的地方。
而且这将在第二个句子上变得特别清楚。
所以在这里我们在句号上，我们倾向于展示。
现在我们在and上，只是我在法语中。
现在我们在I上，我们注意到甜。
现在我们在am上，我们关注做，这是从，然后从加拿大。
所以我们正在进行跨语言归纳头部，我们可以用于翻译。
而且，如果你看一些例子，这似乎是模型正确进行翻译的主要驱动力。
另一个有趣的例子是我认为对我来说在上下文学习中最令人印象深刻的事情可能是模型学习任意函数的能力。
就像你只是向模型展示一个函数，它就可以开始模仿那个函数。
好吧。
我有一个问题。
是的。
那么这些归纳头部只是做一种向前复制，还是像，它们还能做一些复杂的结构识别之类的吗？
是的，是的。
所以它们既可以使用更大的上下文，之前的上下文，又可以复制更抽象的东西。
就像翻译一个，它向您展示它们可以复制而不是文字令牌，一个翻译版本。
所以这就是我们可能称之为软归纳头部的东西。
而且，是的，你可以让它们复制类似的词。
你可以让它们查看更长的上下文。
它可以寻找更多结构上的东西。
我们通常是通过是否在大型模型中，它们是否像归纳头部那样行为不纯来表征它们。
所以当你试图涵盖这些更多，这种模糊边界时，定义会变得有些模糊。
但是，似乎有很多注意力头在做更抽象的版本。
而且，我最喜欢的版本是我即将展示给你的这个，它被使用，让我们孤立一个这样的，它可以进行模式识别。
所以它可以在上下文中学习函数并学会如何做到这一点。
所以我在这里编了一个无意义的函数。
我们将用选择是做颜色还是月份作为第一个词来对一个二进制变量进行编码。
然后我们说这里是绿色或六月。
让我们再放大一些。
所以我们有颜色或月份和动物或水果，然后我们必须将它映射到true或false。
所以这是我们的目标。
而且它将是一个异或。
所以我们用这种方式表示二进制变量。
我们做一个异或。
我相当确信这从未出现在训练集中，因为我只是编造了它，而且看起来像是一个无意义的问题。
好的，那么我们可以问，模型是否可以推动这个？
嗯，它可以，并且它使用归纳头来做到这一点。
而且我们可以做的是，我们可以看一下，所以我们看一个冒号，它将会去尝试预测下一个词。
例如，在这里，我们有四月狗。
所以这是一个月份然后是一个动物，应该是真的。
它的作用是寻找以前有动物、一个月和一个动物的案例，特别是其中月份相同的案例，并查找并声明它是真的。
因此，模型可以通过执行这种模式识别归纳来学习一个完全任意的函数。
因此，对我来说，这使得这些模型实际上能够进行上下文学习的可能性大大增加。
像我们看到这些大型语言模型做的所有这些惊人的事情的一般性可以通过归纳头来解释。
我们不知道那个。
可能有其他事情正在发生。
很可能有很多其他事情正在发生，但对我来说，这似乎比我们开始时更可信。
我意识到我实际上已经超时了，我会快速浏览一下这些最后几张幻灯片。
是的，所以我认为把这个看作是上下文中最近的邻居是一个非常有用的思考方式。
其他事情绝对可能会起作用。
这可能解释了为什么transformer在处理长上下文的情况下比LSTM更好。
LSTM无法做到这一点，因为它在计算量上不是线性的。
就像是二次的或者如果它真的很聪明可能是N log N。
所以transformer对于LSTM来说是不可能做到这一点的，transformer确实能够做到。
实际上，它们在同一点上出现分歧，如果你看，嗯，如果你想的话，我可以在之后详细介绍。
Marcus Hutter有一篇非常好的论文，试图预测和解释我们为什么观察到缩放定律和模型的原因。
值得注意的是，这篇论文中的论点恰好适用于这个例子，这个理论。
事实上，对于在上下文学习中考虑这种情况而言，它们在本质上是通过最近邻算法更好地处理的，而不是在常规情况下。
所以是的，我愿意回答问题。
我可以根据人们的需求提供关于这一切的更多细节。
而且，如果您给我发送电子邮件，我还可以提供有关所有这些的更多信息。
是的，再说一遍，这项工作还没有发表，你不必保密，但如果你能考虑到它尚未发表并且可能还有一个月或两个月才会发布，我会非常感激的。
非常感谢你的时间。
是的，非常感谢你，克里斯。
这是一个很棒的交谈。
谢谢你，克里斯。
所以我将首先提出一些一般性问题，然后我们可以进行学生提问环节。
当然。
所以我非常兴奋地想知道，你目前正在从事什么样的工作？
是在扩展这个工作吗？
那你认为下一步应该尝试做些什么来使其更具可解释性？
接下来是什么？
我的意思是，我想要逆向工程语言模型。
我想要弄清楚这些语言模型的全部内容。
你知道的，有一件事情我们完全不理解是多层感知器层，我们对它们的一些情况有所了解，但我们并不是非常了解多层感知器层。
大型模型中有很多我们不理解的东西。
我想知道模型是如何进行算术运算的。
我想知道，我认为我非常感兴趣的是当有多个发言者时发生了什么，模型能否清楚地表示，就像它有一种基本的心灵理论，多个发言者在对话中。
我想了解那是怎么回事。
但说实话，有很多我们不理解的地方。
实际上，很难回答这个问题，因为有很多需要弄清楚的地方。
我们在做这方面有很多不同的研究线索，但是是的。
Anthropic的可解释性团队正在试图弄清楚模型内部发生了什么，与此类似，他们正试图弄清楚参数实际上是如何编码算法的，我们是否可以将其逆向工程为我们能理解的有意义的计算机程序。
还有一个我想问的问题是，你在谈论transformers如何计划在那个变化中进行金属学习。
所以，你花了很多时间谈论感应头，那很有趣，但是，你能否将这种金属学习算法形式化呢？
可能可以说，噢，也许这是一种像是内部算法在起作用，使它们成为优秀的金属学习者之类的东西吗？
我不知道。
我的意思是，我认为，我认为大致有两种算法。
一种是我们在单层模型中看到的算法，我们在其他模型中也看到了，特别是在早期阶段，那就是尝试复制，你看到一个词，可能稍后会出现类似的词。
寻找可能适合的地方并增加概率。
所以这是我们看到的一件事。
另一件我们看到的事是归纳头部，你可以简单地总结为上下文中最近的邻居。
而且可能还有其他事情，但似乎这两种算法以及我们正在研究的具体实例似乎是推动上下文学习的原因。
这将是我的目前理论。
是的，听起来很有趣。
好的，行。
那么，让我们开始第一轮的前三个问题。
好的，请开始提问。
