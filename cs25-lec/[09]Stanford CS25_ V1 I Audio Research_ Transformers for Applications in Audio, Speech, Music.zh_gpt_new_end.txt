感谢今天邀请我参与讨论。
今天我将主要谈论有关音乐和音频的transformer，这与我们在过去的课程中所做的完全不同。
我也是斯坦福大学唯一的发言人，所以我必须做得很好。
所以你看到很好的幻灯片，因为我在某种意义上代表着大学。
是的，今天的演讲基本上是这样的，我会抛出很多东西。
这有点像自助餐式的，然后你可以随意喜欢或不喜欢任何你想要的。
我将主要讨论我正在进行的三篇论文。
让我们从一个不同的角度介绍一下transformer，以及音频表示是什么。
谈论关于音频的生成模型，这只是在样本级别进行语言建模。
然后我会谈论如何进行语音和音频的语言建模，这与人们为文本所做的不同。
当前文献中有哪些趋势呢？
最后，我将简要提及与计算机视觉中的视觉transformer相关的类似情况。
我们如何将类似的思想应用于音频transformer，并加入一些信号处理来提高性能。
我被告知演讲大约是35到40分钟，其中约有15分钟的问答时间。
我还应该说，所有的观点都是我的，斯坦福大学或任何其他教授对我所犯的任何错误概不负责。
因此，transformer在某种程度上已经彻底改变了所有人对深度学习的看法。
在那之前，一切都是关于CNN的。
而且大多数突出的模型都是阶段性出现的。
有一段时间，每个人都在使用CNN。
然后，人们开始采用CNN和某种形式的扩张卷积，慢慢地，递归网络正在退出潮流。
现在似乎transformer一直很流行。
因此，它们似乎几乎解决了被提出的每一个问题。
那么它们有什么特别之处呢？
其中一个让我印象深刻的事实是它们的简单性，如果你考虑一下的话，它也非常受欢迎。
所以它在2018年刚发布，三年之内就有大约30,000次引用，而且它在每个领域几乎解决了每一个问题。
当然，它也有一些局限性。
但是如果你想一想，从某种程度上来说，transformers基本上就是一种通过特征学习来不断进行自注意力级联的方式。
如果我们不断地重复这个过程，那么模型就会学到输入的哪些部分是重要的，并继续转换它们，去除那些不重要的内容，只保留与特定任务相关的有限信息。
而要跟上文献的步伐非常非常困难。
我在这里开个玩笑，但甚至Twitter的推荐引擎也开始有点失控，就好像他们在疯狂地搜索transformers一样，不知道克里斯·曼宁在搞什么。
这是2020年的报应。
所以，研究人员也很难跟上发展的步伐。
就在transformers出现之前，整个自然语言处理社区都在疯狂地讨论带有注意力的双向MSDMs。
因此，在2017年之前的每一篇论文中，你都会看到编码的MSDM层。
你继续添加多个层。
然后在此之后，你有了注意机制，它只是学习什么是重要的，然后一次又一次地顺序解码。
但这并不是一种理想的做法，因为事实证明，当我们开始投入更长的序列时，连接不再以应该的方式存储梯度更新。
所以谷歌的研究人员说，与其只在最后一个编码层中有一个注意力层，我们可以在每个单独的层中都有这些注意机制，这样在某种程度上就会学习对于特定问题在该特定层中什么是重要的。
我们一遍又一遍地这样做。
于是，transformer和注意机制一个接一个地串联了起来的整个想法就出现了。
我不会详细介绍，因为这是课程的最后一堂课。
但是在神经网络文献中，通常的技巧确实有帮助，比如拥有多头注意力，拥有跳跃连接和层归一化。
所以所有这些东西，它们不仅仅是为了transformer本身而提供收益，而且可以应用到任何其他的架构中。 
另一件有助于这项研究的事情基本上是计算能力不断提升。
所以所有这些大公司都在将大量的计算资源投入到解决非常简单和琐碎的任务上。
山顶是像课程中讨论的开关transformer这样的东西。
但我认为引发所有这一趋势的事情之一是ELMO，它只是为自然语言处理学习了这些上下文化的表示。
而这个模型在这里也许是第一个0 .0或者0 .1之类的模型，或者在整个革命中起着推波助澜的作用。
你可以看到这些模型看起来有多相似。
BERT基本上是受到ELMO的启发，其中他们只是用transformer模块替换了一些LSTM层。
所以值得注意的一点是，不管是自然语言处理还是其他领域，这些都可以在各种领域中得到应用。
而今天的讨论，我将只是将它们用于音频。
所以我基本上会开始介绍人们，音频表示是什么，为了完整起见，讨论一下频谱图。
所以你可以取任何时间域信号，然后将该信号分解为各种基本函数。
如果你进行傅里叶变换，你就相当于将实际的时间域信号分解为其正弦基本分量。
所以如果你有这样一个波形，它是三个纯正弦波的和，那么基本上就是这样。
而且你可以看到，当你进行傅里叶变换并取其幅值时，你可以看到这里显示了各个分量的强度。
所以你可以取另一个波形，比如说一个方波。
而你所拥有的基本上是一个更丰富的正弦波分解，因为它是一种不连续的信号。
所以你需要更多的正弦波来尽可能地表示该特定信号。
在这里，你也可以看到，如果这是一个方波，那么它实际上由许多正弦波组成，这里的每个条代表特定正弦波的强度。
从优化的角度来看，我是说，这立即就是次优的，对吧？
因为你在表示方波时，你会固定使用的正弦波的数量。
我宁愿使用一个基函数，它本身是一个方波，而不是一个正弦信号，对吧？
第二件事就像，即使你拿到一个正弦信号，我们也只是把它们放在等距的空间里。
所以你在某种程度上把整个频率访问分成了等距的箱子，每个箱子都负责处理特定的正弦波。
这就是传统傅里叶表示法来表示任何信号的方式。
那么我们为什么要用什么是频谱图呢？
但实际上，所有这些信号都是不连续的。
所有这些信号变化相当大，对吧？
所以你可以有像我说话时的信号，它在一段时间内是一个方波，然后变成正弦波，然后变成其他东西。
所以我们真正需要的是一种批量处理输入信号并对这些单独批次进行傅里叶变换的方法。
我故意使用了批处理这个词，但你可以像传统术语那样，对信号进行窗口处理。
所以在这里，你可以看到你有一个连续的信号，你不断地对它进行窗口处理，你应用傅里叶变换，然后你得到的基本上是信号的频谱图表示。
所以在这里，你实际上看到的是对于每个切片，信号看起来基本上是这样的。
在对波形进行傅里叶变换之后，如下所示。
你所做的是对于频谱图的表示，不断堆叠这些傅里叶变换切片，保留傅里叶变换切片的幅度。
这样，你就可以获得音频信号的二维表示。
如果你来自视觉背景，基本上在视觉中所做的一切都会很好地适用于这些二维谱表示。
我会快速演示这些频谱图是如何看起来的，涵盖各种常见的声音。
所以你可以看到，对于频谱图，你在 x 轴上有一个时间轴，然后在 y 轴上有一个频率轴。
然后，对于你感兴趣的信号，你基本上是将这些切片放在一起，不同的声音给你不同的谱表示。
所以这在某种程度上是一个在傅里叶空间中的视觉问题。
所以也可以有不同类型的表示。
首先，你可以只取这些傅立叶变换的片段，然后对它们进行线性映射，这样你就可以使它们尽可能地接近人类听觉。
所以，你可以将频率的对数作为y轴，而不是常见的频率。
然后，你会得到类似于常数Q的表示。
这样做的好处是，你可以看到对于不同的频率，谐波之间的间距基本保持不变。
所以，如果你在训练卷积滤波器，那么这是一个巨大的优势，因为信号中的一个不变分量已经消失了，你只需学习这些捕捉傅立叶片段常数模板的滤波器。
你可以有金属滤波器组系数，或者你也可以有原始波形。
对于原始波形，基本上有两件事情我们必须牢记。
一个是采样率。
所以我们基本上是取连续信号然后将连续信号离散化。
所以一个参数就是我们采样连续信号的速度有多快。
所以，如果你在进行电话语音通信，通常每秒会有大约16,000或8,000次的采样。
另一件事情，我们也要考虑的是我们将垂直轴划分为多少个级别。
所以在这种情况下，你可以看到每个点基本上就是一个级别。
通常人们使用八位量化器或十六位量化器。
这样，你可以考虑，对于我们听到的每一秒音频，你会有16,000个样本。
然后在这16,000个样本中，可以选择介于零到255之间的一个级别。
这就像，如果我能够将连续音频的问题转化为这种离散的空间，那基本上我就是在进行语言建模的领域。
所以我首先要讨论的论文是，我们如何像使用transformer的WaveNet那样，对原始音频进行生成建模。
如果你喜欢我所做的事情，如果你认为这与你相关，请引用或请查看QR码。
好的，我会从今天的讲座的第一个子主题开始，那就是，什么是WaveNets，以及我们如何进行原始音频的生成建模呢？
所以简而言之，你可以把这看作是在这 255 个音频状态上进行语言建模。
所以你可以投入你喜欢的transformer模型，比如transformer Excel 或者 GPT，或者任何你想叫它的东西，然后就把问题看作是你试图预测出 255 中的哪个水平，你必须在给定一定上下文的情况下预测下一个水平。
这就是 WaveNet 在做的事情。
所以你对连续空间的概率分布建模的方式基本上是在试图预测在某种解析上下文给定的情况下，下一个样本的概率是多少。
WaveNet 受欢迎的原因在于它已经被引用了 3000 多次，而且几乎所有与语音和音频相关的问题都将它作为核心构建模块。
你可以想象一下，比如语音转文本，文本转语音合成，乐器转换，互联网上的数据包丢失隐藏，语音降噪等等。
所以无论在哪里涉及到修改音频的要素，人们都一直在使用 WaveNet 作为核心构建模块。
而原始波形合成之所以困难，是因为问题的规模太大了，如果我只是尝试合成 10 秒的音频，那么我需要对 160,000 个样本进行概率分布。
而这本身就很困难，因为我们的耳朵对细微的变化非常敏感。
如果图像中我偏移了一个像素，那么图像就像我的眼睛一样不容易注意到这种影响，与如果我在音频中偏移了几个样本，它就会很快被我们的耳朵捕捉到。
人们过去一直在尝试原始音频合成。
在所有 WaveNet 和基于 transformer 的方法之前，类似 WaveRNN 和 sample RNN 一直是最先进的模型。
右边我展示了一个 sample RNN 模型，它在多个级别上模拟了接下来会出现什么的概率分布。
这是由 Yoshua Bengio 在 Nila 完成的一项工作，但你可以看到，如果你仅仅看到这个架构与 transformer 架构相比，它们开始变得非常相似，因为你尝试做的是对这里的概率分布进行建模，你尝试看到很多像是局部子结构，然后你不断地重复这样做。
你可以进行类比，比如，注意机制也应该在某种程度上做同样的事情。
所以这在过去有点像文学，我们尝试做的是，我们只是像 WaveNet 模型一样，试图看看 transformer 是否能够击败它们。
我们的直觉是，它应该能够击败它们，因为它们在其他领域都很成功，比如在语言建模中。
所以它也应该能够做到对原始波形进行建模。
我们还尝试看看是否可以通过条件或上下文本身来规避顺序和平方约束。
而且，我们并没有针对特定应用进行研究，我们只是说，好吧，就模拟行为而言，它们会表现如何？
因此，这个数据集就像是真实世界的录音一样。
所以实际上声音不应该有影响，因为模型对被输入的内容是不可知的。
而且设置完全相同，就像你提供了某种上下文，我必须预测下一个样本一样。
你用 WaveNets 做同样的事情，你用基于 transform 的模型，像 GPT 这样的模型做同样的事情，看它们会怎么样。
我会简要介绍一下 WaveNet 模型是什么。
所以 WaveNet 在某种程度上类似于基于卷积的模型，通过将序列问题视为由卷积模型学习的方式，消除了所有消失梯度问题。
所以他们基本上做的是使用这种扩张层或带有扩张的卷积，基本上就是在每个后续层中每隔一个样本进行跳跃。
所以你可以看到，如果我有一个扩张因子为二，核大小为二，我会得到这样一种拓扑结构，其中我的卷积滤波器在第一层中就像是合并了前两个样本。
然后在下一层中每隔一个样本进行跳跃，然后在下一层中每隔三个样本进行跳跃，这样我就会看到下一层的第四个样本，依此类推。
损失仍然是相同的。
所以我有了这个网络，学到了一个潜在空间，然后有一个分类交叉熵损失，基本上是我必须预测给定前一个样本的下一个样本。
然后我也用transformer做了完全相同的事情。
但是我必须确保以因果方式进行。
所以我有了与 GPT 非常相似的东西，在我的注意机制中有因果掩码，我一遍又一遍地进行这个过程。
所以你有自我关注，之后你有前馈层。
你只需要一堆这些转换块，然后看它们的表现。
所以我说直觉上应该有效。
所以就像它应该比我们基础的 WaveNet 模型做得更好，对吧？
因为如果你看拓扑图，我们有点在自己定义拓扑。
所以如果当前预测在第一层是依赖于很久远的样本，比如第二个样本，而不是第十个样本，那会怎样。
所以我们有点忽略了所有这些拓扑，而这些拓扑对于这个特定任务的预测是重要的。
而我们的带有自我关注机制的 Transformer 可以学习，哪些样本部分重要哪些不重要，并且你可以不断地进行迭代。
所以对我们来说有道理，转换层应该比 WaveNet 模型做得更好。
我们遇到的第二个问题是，我们不能有太多的上下文。
例如，注意机制需要存储所有的 n 阶方阵。
在这种情况下，如果我像以100毫秒的间隔存储数据，那么我大约有1600个样本，我需要在多个层面上存储1600乘以1600。
而且这只是在数据上变成一个巨大的问题，与内存约束有关的问题。
所以我们说的是，好吧，如果我们只是将上下文本身用作潜在代码会怎样呢？
所以为了在每一层都有更好的表示，我们不能有巨大的注意力矩阵。
所以我们说我们将只对每个样本进行条件处理，通过CNN层来理解潜在代码是什么。
所以你仍然有一个关注机制或过去的上下文，但是我也在每个样本上进行条件处理，好的，基于这个上下文嵌入，下一个样本应该是什么。
如果你考虑一下，这是这样的，如果在钢琴上演奏了五六个音符，那么如果我只是加入一个CNN层，我就有点确定会播放哪些音符。
所以我将使用这些信息以及我的模型正在学习的内容，然后我会进行条件处理，我将使用它来预测下一个样本。
所以在评估标准方面，我们没有寻找负对数似然分数。
我们刚刚看了一下我们的预测任务表现如何。
所以我们接触了像 DeepMind 实现的堆栈 WaveNet 这样的东西，并看到，好吧，使用他们的基准测试，甚至是更大的堆栈 WaveNet 的性能如何。
然后，我们开始增加transformers的复杂性，并开始看看我们提出的像在普通transformer架构上进行条件建模的东西，看看它们的表现如何。
我们并没有寻找特定于应用的问题，基本上就是说，我们不看看感知任务在文本到语音合成或语音降噪方面表现如何。
我们只是看一下，好吧，如果我们尝试使用交叉熵损失来对这个建模，那么在相同的模型，相同的损失函数下，它们在类似的参数上会表现如何？
所以这是第一种，我们如何使用我们的transformers进行生成建模的子块。
对于第二个问题，我将迅速介绍一下我们如何使用transformers进行语言建模，这在现在变成了一个非常时髦的术语。
这项工作是由 Julius Smith 在2020年完成的。
这项工作的目标是，我们是否可以以某种方式对连续的音频序列进行语言建模？
我在文档的这个子块中简要提到了这一点。
这是关于解决声音场景理解的，基本上就是，如果我有一段音频，那么我想要理解其中的内容。
如果我们能做到这一点，那么我们可以做很多花哨、很棒的应用程序。
所以，例如，如果你考虑像是自动驾驶汽车，VMO已经开始将麦克风整合到自动驾驶汽车中。
为什么呢？
因为比如说如果有救护车过来了或者有消防车过来了，那么那种声音会被捕捉到，比激光雷达甚至他们的传感器早得多。
所以他们想要理解这一点，并根据此采取行动。
在COVID期间，苹果在他们的Apple手表上做了一个洗手检测，因为如果你能检测到什么时候有人在洗手，那么你可以告诉人们，哦，你需要洗手20秒钟。
然后这可以被用作一个很酷的应用程序。
它可以用于音乐推荐。
所以Spotify、YouTube音乐等都会给出非常好的歌曲，这些歌曲在内容上与你可能喜欢的相似。
它也可以提供非常酷的应用程序。
比如说人们尝试过从音频中检测抑郁，或者我可以检测我是否在咳嗽，或者我是否在打喷嚏。
这些可以成为良好的医疗设备，医疗应用，可以与医生提供的当前诊断一起使用。
所以基本上对我们来说，问题就是，我们如何在连续的音频领域进行语言建模？
其次，我们如何训练模型或者我们应该如何解决这个问题？
这种类似食谱的方法在当今变得非常非常流行，你应该如何解决这个问题？
它始于像 OpenAI 这样的公司，一定程度上是 DeepMind 提出了关于 VQV 模型的想法。
但事实证明，就目前而言，transformers 喜欢在离散空间中操作。
而它们所做的事情是，只要你的表示是离散的，它们就非常擅长建模接下来会发生什么。
人们一直在提出的一种解决方法是，你可以以某种方式采取你喜欢的嵌入。
你可以采用 VQVE 嵌入，或者你可以采用 Wave to Wek，或者在视频方面，你可以只是做经典的 VGG 或 ResNet 嵌入。
你可以对其应用 k-means 聚类。
而 k -means 聚类会给你类似于离散的代码。
你用这些离散的代码做语言建模，预测下一个代码。
在某种程度上，如果你这样做，那么你有点在做语言建模或音频。
如果你需要回到音频，那么你已经看到了 WaveNet 可以调节 WaveNet 模型以给出连续输出。
所以你可以使用这些代码来恢复到类似于 Jukebox 和 OpenAI 所做的音频。
所以我会简单地提一下向量量化是什么。
说实话，它是最被低估的算法之一。
它的基本原理是在连续的嵌入空间中提供离散的代码。
那么它是如何做到的呢？
基本上，你有一个嵌入空间，在这里我们假设是 2D，你定义了你想要将它们中的每一个放入的簇的数量。
你运行 k -means 然后你肯定会得到这些像是这样的补丁，其中所有这些嵌入或连续嵌入的代表性嵌入会出现。
你可以取所有这些补丁，然后你可以对它们进行编号，或者你可以简单地列出它们。
所以在这种情况下，你可以有大约25个数字或20个数字，它们在某种程度上是从连续嵌入到离散标记的映射。
这里是另一个例子。
在我们的情况下，我们采取了一些声谱图的补丁，基本上是沿着时间的非常小的补丁，然后在整个频率轴上共享。
你拿这些补丁，学习一个嵌入表示。
在我们的情况下，它就像是一个三层自动编码器，完全连接的编码器，有三层解码器，并且在它们之间有一个瓶颈层。
所以那个瓶颈层基本上是在64维空间或120维空间中类似于这种图表。
你拿这些瓶颈代码，然后对它们进行k均值聚类。
突然间，你可以在某种程度上找到连续嵌入空间或连续片段的离散代码。
由于我们知道transformer（transformers）喜欢操作离散空间，你现在可以应用语言建模，然后你可以看看你能做什么。
所以在我们的情况下，我们只有很简单的三层全连接自动编码器，小的补丁。
代码的数量很重要，因为如果你有太多的代码，那么你就是在投入各种嘈杂的东西。
我将通过一些例子来说明代码数量为什么重要。
你有两个小代码。
你所做的事情实际上是在删除所有相关的信息，然后你只是把它们全部平均掉。
我将以 Jukebox 提出的这个想法为例，该想法是为音乐而提出的。
所以你做的事情和我稍微以不同方式讲述的一样，也就是说，你不能学到更长序列的代码。
以某种方式，学习那些移动缓慢并且只查看一定量音频的序列。
因此，在这些离散级别中对其进行编码，这基本上就是所有这些都是代码。
所以在每一点上，我定义好，比如说，这段音频可能有代码编号55，而在下一个级别，它可能有代码编号2，而在最顶层，它可能有代码编号2000。
所以我在某种程度上是在离散化整个代码。
现在我要做的是采用我喜欢的transformer模型，也许是类似因果自回归的模型。
我说过，好的，鉴于这些代码，尝试预测接下来会出现什么代码。
而且，transformer确实可以做到这一点。
所以我会在未来生成课程。
一旦我在未来生成了课程，我可以说，好的，现在这个问题有点像是一个文本到语音的问题，对吧？
因为我有这些离散的词，文本到语音在某种程度上是从离散的字母到连续的音频的转换。
所以我会加入最高级的技术，比如WaveNet，然后我会得到代码并获得生成的音频。
所以这在某种程度上就是我所描述的，它们接收连续的音频。
它们有这些经过压缩的代码，它们使用CNN进行编码。
方法并不重要。
你可以投入最高级的嵌入或潜在表示到这些连续的代码中。
你生成模式，就像是未来会发生什么。
然后你使用高级的WaveNet或最先进的模型进行解码。
所以这就是他们为音乐合成所做的事情。
我们所说的是，你知道，是的，这很好。
这可以生成相当数量的音乐，但是这些模型能用来生成当前音频的良好表示吗？
目标是，语言模型能否学习表示，可以封装我们作为输入信号的内容。
所以在这种情况下，我们尝试的是类似的想法，但是不是在VQV上，而是端到端学习编码，我们只是应用了香草K均值聚类，类似于我之前描述的。
我们对频谱图块进行操作。
所以你拿起这些音频的频谱图，然后将它们分成非常小的块，为每个块学习自动编码器编码，运行K均值聚类。
在这种情况下，比如说我正在学习16个代码，用16个代码表示连续的音频，有一个可以预测下一个代码的transformer。
如果我不断地更好地预测接下来会发生什么，那么在这个线性层中，我应该封装了过去发生的重要内容或良好总结。
这就是我们尝试这个的直觉。
正如我解释的，代码的数量起着非常重要的作用。
你可以在这里看到，这只是两个钢琴音符一个接一个地切换。
如果我只有，比如说，16个编码，碰巧只有一行编码，一个编码分配给所有这些。
而如果我分配更多的编码，那么它就变成了一种精细的预测，我实际上能够得到各个音符是什么。
最近，Facebook 也说，你知道，像，好吧，他们只是给整件事起了个不同的名字，我们也可以称之为无文本的 NLP，也就是说，你可以在没有文本的情况下进行 NLP。
但是这个想法是非常非常相似的。
你有一个编码器，这与 OpenAI 使用的相同。
你有一个 VQVAE，wave2vec 或者你想做的任何事情。
你可以对其应用 k-means 聚类。
你可以对其应用语言模型。
而解码器不是 wavenet，他们只是有一个解码器，这是一个不同版本的文本到语音，也就是在这种情况下的 Talkotron。
所以你可以看到，这些都是同一种酒，但是瓶子却是非常不同的，但核心思想几乎完全相同。
所以这就像，它创造了一个巨大的方法，改变了 NLP。
但这与过去人们所做的非常相似。
所以我已经解释过这是什么了。
所以在我们的情况下，我们只是试图预测接下来会发生什么，考虑到先前的上下文，并使用类似于每个单一的，像一次性学习或零次学习的方法的表示。
我也解释过为什么代码的数量很重要。
就像如果你的代码太少，那么你只是扔掉了很多信息。
如果你的代码太多，那么你就不会像以前那样抵抗噪音了。
所以这就是我们的设置。
在我开始之前，我应该补充一下其中一条推特，我从我们深度思维中最杰出的研究人员那里看到的，基本上就是很多时候很容易提高数字。
比如说，我可以在我的论文中没有提到这些细节，这实际上在提高性能方面帮助了很多，有时候不考虑实际模型正在包含的内容，或者模型正在贡献的内容，而只考虑实际的这些训练技巧正在包含的内容。
对于大多数这些方法，我们试图看到的是我们几乎保持完全相同的方法，没有数据增强，没有花哨的标签平滑或权重的移动平均或衰减或其他什么。
你只是有类似的基于食谱来看我们的表现如何。
对于这种情况，目标是看看我们的模型在纯监督方法方面表现如何，以及在类似无监督方法方面表现如何。
所以在第一种情况下，模型和所有的权重都能访问所有的标签，这只是显示为VGG监督，基本上你拿起一个音频理解数据集，看看你在准确度指标上表现如何。
那就是第一个。
在第二个中，我们应用了由Jeffington提出的Simclear，其中你可以采用同一输入的多个增强。
你可以移除像块一样的补丁。
你可以模糊信号。
你可以翻转信号。
你学习最后一层的嵌入，没有访问标签，然后只需有一个线性头来预测发生了什么。
通过使用这个，我们获得了55%的准确度。
你用transformers做完全相同的事情。
你不能访问标签。
你只是运行它们来预测下一行代码。
你拿线性层，应用相同的线性头，然后尝试预测内部发生了什么。
有了这个，我们得到了 60% 的准确率。
所以即使结果不好，但事实上神经网络确实非常非常擅长通过大量数据得到不断改进。
所以在纯监督和纯无监督之间仍然存在 10% 的差距，但是通过向这些模型提供大量数据来改进，因为它没有像样的标签。
这是丹·埃利斯和尼尔森·摩根在伯克利的一篇著名论文，他们实际上在 1999 年就展示了为什么对于深度神经网络来说大小和数据点的数量很重要。
因此，随着数据集和参数的增加，他们得到的词错误率越来越低。
这在任何数据集上都是真实的，这就是为什么整个兴奋点在于无监督学习。
因此，这在某种程度上是一种如何对连续信号进行语言建模和无监督学习的味道。
对于第三个子图，我将快速提到的想法与视觉转换中所见非常相似，但要注意我们如何使用某种信号处理来进一步提高性能。
所以基本方法仍然与视觉转换中所见的完全相同。
你有一个你想要分类的感兴趣的信号。
这里它们是原始波形而不是图像。
目标是预测其中的内容。
而且我们没有卷积，也没有之前使用的其他技巧。
我们需要做的就是它们可以自行转换，解决这个特定的问题。
所以对于数据集，整个设置仍然相同，没有数据增强和其他形式的技巧。
你得到了大约 40,000 个片段用于训练和 10,000 个用于验证。
我们的工作是尽可能地预测音频中的内容。
这个问题与你听到的声音和你看到的视频非常相似，即给定一个补丁，你必须预测，给定一个频谱图补丁，你必须预测其中的内容。
我们在某种程度上比简单的Transformer模型更进一步。
意思是，我们试图看看是否在转换域上的某种层次结构会在任何方面帮助我们。
所以为此，我们在中间Transformer emirates上使用了小波分解。
那么什么是小波分解呢？
用非常简单的术语来说，它可以是一种将中间嵌入分解为另一种中间嵌入的方法，从某种意义上说，我们正在将这些嵌入的高速公路放在一起，一些嵌入的移动非常缓慢，而另一些嵌入的移动非常快。
而且一些嵌入的保留恰好是原始信号的速率。
为什么这很重要呢？
因为你可以想象在每个中间状态中，你在某种程度上都在学习模型中的某种层次结构。
所以，如果我看看我们在之前和之后对小波分解所做的事情，比方说，你在这个方向上有时间，在这个方向上有嵌入大小。
而整个补丁就是你说的Transformer的最后清除的输出。
我现在要说的是，好的，我会将这个映射到我的兴趣映射中，使用小波分解，其中有一半的样本，我只创建了与transformer模型学到的确切嵌入相同的嵌入。
在接下来的一半中，我会开始一次组合两个。
所以在某种程度上，我正在学习transformer嵌入的单层内的树状结构。
就目前而言，我使用的小波或BCs函数只是简单的平均。
所以说，假设我从所有嵌入层中间得到，我只需要有一个嵌入根本不动，它只代表了整个潜在空间中存在的任何内容。
然后在下一层，我会一次使用两个，然后一次使用四个，直到达到我所拥有的确切分辨率。
进行这个操作不会增加任何参数。
你只是在定义你的BCs函数或者你的小波函数会是什么样子。
在这种情况下，它是一个硬小波，我开始将它们组合起来，我在每个transformer的单层中学到了一个层次结构。
这种方法明显提高了我们的性能，与不使用它们相比，额外的参数也没有增加。
稍后我会谈到结果。
这就是整个方法的样子。
你有一个前端。
前端基本上是一个由2000个神经元组成的单层，后面是一个由64个神经元组成的密集层，这只是为了确保它符合中间的Transformer嵌入。
假设对于Transformer，我定义它们的绑定大小为64，那么这就是我将它们映射到的维度。
所以我采用一个宽波形，我将它分成非常小的补丁，类似于你在视觉Transformer中所做的。
我只会有一个由2000个神经元组成的单层，后面跟着一个由64个神经元组成的密集层，希望第一层能够学习一个完整的年度BCs函数，这应该是根据我所学到的内容而适应的。
之后，我一遍又一遍地这样做。
就像我没有分类头或其他任何东西一样。
之后，我继续添加多个Transformer堆栈。
然后我有两种适应性的方法。
我可以对这些中间嵌入进行时间平均池化，因为这个想法与我们在经典视觉中所做的非常相似，即每个嵌入都在后续层中查看更广泛的输出。
或者我可以进行小波分解。
所以我所做的是，我拿到所有这些嵌入，并定义这些高速公路。
所以一些嵌入在快速移动，一些在移动得非常缓慢，而一些保持在与transformer所学习的完全相同的分辨率。
然后我一遍又一遍地重复这个过程。
我有一个密集层，我有我的softmax或sigmoid，无论哪个是我的分类头。
所以这就是这种方法的大致样子。
我们将其与所有传统的基于视觉的架构进行比较。
因此，基于视觉的模型一直表现非常出色，对理解音频的性能也很相似。
因此，我们根据平均平均位置比较了所有这些模型。
我们发现，即使是最小的transformer模型也超越了所有最先进的CNN模型，这是一个非常好的迹象。
然后我们开始提升，好吧，更大的模型应该继续提高性能。
通过多尺度模型以及池化层，它们进一步提高了性能，这对我们来说相当令人惊讶，因为参数的数量非常少。
这些是非常小的架构，然而它们却超越了像 DenseNet 这样的巨型模型，后者有着成千上万的参数。
因此，在那之后，我们说，我要快速总结一下，在那之后，我们说，好的，这看起来相当酷。
实际上，transformer或第一层在学习什么？
为了制作这个图，我们说，好的，如果你要进行经典的傅立叶变换，那么如果这个轴有点像频率，这个轴是滤波器的数量，这个轴是频率，那么从某种意义上说，它应该连接所有点形成一条直线。
这相当于 50 个点的数量。
所以我在这里定义了多少点？
如果我在这里定义了 2000 个点，那么我将有 2048 个正弦基函数，从最低频率到最高频率。
我们说，好的，我们会做完全相同的事情，但现在用滤波器。
所以我们在 Y 轴上有一个频率，在我的 X 轴上有点的数量。
如果这是经典的傅立叶变换，那么它会像一条直线一样连接。
但是我们所做的是，我们接管了前端，也就是由transformer知道的部分，取其傅立叶变换，根据其中心频率进行排序，以确定它激活最多的频率，然后不断堆叠它们。
当我们针对两个问题这样做时，我们发现我们似乎在学习一种不同的时频表示，这是特定于特定问题的。
所以，如果我试图了解音频内容，我学到的表示与傅立叶变换非常不同，后者本应是一条直线，而是一种曲线指数线，就像这样。
如果我做的是多音调音高估计，我会学到一个非常不同的前端，它适应于特定的问题。
所以这对我们来说非常令人兴奋，因为让计算机根据特定问题调整它们的听力方式是一个非常酷的想法。
第二件事是，我们实际上看到了每个滤波器在做什么。
这些基本上只是像这样的单个切片。
这就是我们所学到的前端神经元。
所以我们接管每一个神经元然后将它们绘制出来。
而要绘制这个，我们基本上通过你的转换将它们排序，根据中心频率的位置。
当我们刚刚看到前端的神经元在学习什么时，我们发现它正在学习的属性与传统的信号处理非常非常相似。
所以你会有类似于起始检测器学习标准的东西。
你正在学习窗函数。
所以在某种程度上，它正在学习拥有最适合时频表示的核，人们一直在信号处理中使用的东西，就像一个汉明或汉宁窗。
我们正在学习这些纯正弦波，它们负责激活特定频率。
所以你可以看到与在这里拥有一个固定的纯正弦波PCS函数相比，它的丰富性。
所以这就是我们所做的。
然后分享最终的想法，我将总结说，好的，transformer在各个领域的AI研究中都证明是一个重大的进步。
看起来它们现在似乎正在解决一切问题。
希望这不是结束，我们应该密切关注会改变并产生更大影响的事情。
谁知道接下来会发生什么呢？
是的，通过这样，我就结束讲话了，很高兴回答问题。
谢谢Prateek，你的演讲非常精彩。
你提供了一些关于transformer在音频案例中如何工作的非常好的见解。
谢谢你的演讲。
现在我要邀请同学们提问了。
我就停止录音。 
