很高兴欢迎来自OpenAI的Jan。
他在那里领导着对齐团队，之前也是DeepMind的研究员。
他拥有强化学习理论的博士学位。
已经思考对齐问题超过10年了。
今天他将做一个非常有趣的演讲。
希望你们会喜欢。
是的，非常感谢介绍，也非常感谢邀请我。
我非常激动地谈论这些事情。
我也非常乐意保持互动。
如果你们在任何时候有问题，请打断我。
是的，我想首先就我认为正在发生的事情做一些非常基本的观察。
首先是AI团队加入了游戏。
所以AI团队有很多不同的玩家。
他们并不是同时加入的，而是一个接着一个加入。
并不是所有的玩家都在他们的能力上都很出色。
而现在迄今为止加入的许多玩家并不是特别聪明，通常只能完成很狭窄的一系列任务。
但我们观察到的一件事是，随着时间的推移，我们看到越来越强大的玩家加入。
而这大概是我们现在所处的位置。
而总的来说，我们预计 AI 团队拥有非常强大的选手。
所以那些将会是能够思考得比人类好得多、快得多、便宜得多的选手。
而这些还没有加入。
所以我们所拥有的一种锚点是，如果你想象一下 ChatGBT，ChatGBT已经可以成为任何人，就像了解更多的事实或说更多的语言一样。
它可以每秒写约 50 个单词，而且以每小时至少低于最低工资 100 倍的价格做到这一点。
所以，ChatGBT 也有一些非常重要的局限性，还有很多它还不能做的事情。
但这在某种程度上是未来可能完成的一些选手的指标。
所以从长远来看，似乎 AI 团队将拥有所有的优势。
但是，有一个重要的警告，即人类团队拥有一个重要的优势，那就是人类团队可以选择 AI 团队的哪些选手何时加入。
所以这有点像是我们在考虑做什么以及我们在考虑的时候应该充分利用的优势，这个我们与 AI 团队一起玩的游戏，然后我们将与 AI 团队一起玩。
所以我认为我们作为人类团队应该做的两个主要目标之一是，首先，我们应该试图招募 AI 团队的玩家加入人类团队。
这就是我大致称之为调整的内容。
这就是我正在解决的问题。
然后还有其他目标。
所以我认为另一个非常重要的目标是，你想要制定游戏规则，使得人类团队不会输掉比赛。
现在，人类团队有点占据优势，我们可以制定规则。
所以我们应该制定有意义的规则，并且在未来仍然在玩这个游戏。
所以在这次演讲中，我不会谈论第二点。
我会谈论第一点，因为那是我知道我正在解决的问题。
为了用不同的方式表达或者更加实际一些，比如，我现在在考虑对齐的一个方式是，你想要构建遵循人类意图和偏好的人工智能系统，我们希望它们做我们想要它们做的事情。
所以，基本上我要谈论两件主要的事情。
第一部分将是我们过去所做的工作，大致上属于，我们正在试图弄清楚如何使我们今天拥有的模型尽可能地对齐。
你只需努力尝试，我们会看到我们能走多远。
然后第二个部分是我们接下来必须做的事情。
我们还没有做的那些事情，我们认为这些事情会非常重要，我想解释一下为什么我认为它们会很重要。
所以现在我说，你知道，我在尝试让这更清晰，或者更细分一些对齐的意思是什么。
所以不像这里，因为现在，你知道，最大的问题是，遵循人类意图意味着什么？
而且，我们关心的意图主要有两个类别，我会说是显式意图。
所以，你知道，如果我给系统一个指示，或者你想要它成为我的助手，它应该是我的助手。
我应该遵循指示。
但是，也有一些其他意图，我通常在与你的系统或人类交谈时不会说出来，他们可能都关心。
你知道，它不应该只是简单地做我说的话，而是应该理解我的意思，它不应该虚构东西，也不应该做有害的事情，它应该询问后续问题，如果不确定我的意思等等。
因此，这些通常都是一些非常难以精确指定或分类的事情，但这仍然是我们想要应用程序执行的任务，我们必须弄清楚如何做到这一点。
因此，我们今天主要使用的技术之一是我们称之为问题线反馈。
这是用来训练脚本GPT和测试GPT的，这两个是我在这一部分要讲的两个主要系统。
基本方法非常简单，也是一种非常普遍的技术，适用于许多不同的AI模型和模态以及设置。
但在这种情况下，我们将使用受控模型。
所以两个步骤实际上是时间线演示的另一步。
所以我要停下来了。
第一步是你想要从比较中训练你的奖励模型。
所以你在顶部，比如现在解释你的物理移动或者，你知道，帮我写论文，不管是什么。
然后模型做了一堆事情，然后你会读到哪一个更接近你想要模型成为的东西。
所以你有一个更大的偏好集合，然后你训练你的奖励模型。
奖励模型基本上只是预测你会更喜欢哪一个。
一切还好吗？
我会说就站在摄像头前多一点。
这是不可能的，但我觉得那看起来不错。
对不起，关于那个。
也许让我们把它翻过来。
好的，现在我们通过我们的模型拥有了这个，它捕捉到了我们的偏好和我们关心的事情，以及我们打算让模型做的事情。
然后第二步是，现在你用输入优化奖励模型。
所以不是设定，你知道，像模型尝试了很多不同的事情，而奖励模型告诉它哪一个会被帮助。
事物可能更像你得到的那个东西。
当你说比较时，是指标签者的建议进入数据吗？
那些总是一致的吗，还是这取决于标签者呢？
如果可能有不同的偏好，它们将取决于标签者。
它们也可能存在不一致之处，你可以举例说明在这些偏好之前。
但在实践中，这些并没有真正成为问题。
到目前为止，你知道，像我们的标签者经常不同意，但模型的平均值总体上并不一致。
但是，这就是基本的技术。
从概念上讲，它相当简单。
如果你没有训练奖励模型，而是像加载的每个小时都标记，你甚至可以使它更简单，但数据效率会大大降低。
因此，你可以把它想象成数据效率。
那它是如何工作的呢？
这在指令纸上是一个主要的思路。
这是我喜欢展示的一个，因为它确实让我感到震惊，而且现在仍然是这样。
我们在这里看到了什么？
所以在x轴上，你会看到，这是来自GPT-3模型系列的，你会看到这就像是不同的，三种不同规模的模型跨越了两个数量级。
而在y轴上是模型在人类偏好方面得分如何？
所以如果我们向人类展示一堆样本，他们更可能喜欢其中一个而不是另一个吗？
然后我们看到，即使是最大的GPT-3模型也比较不受欢迎小得多的instruct GPT变体。
所以100倍小的instruct模型实际上比更大的GPT，比如完整尺寸的GPT-3模型更受欢迎。
这有点疯狂。
抱歉，让我结束我的发言。
那么为什么是大的那个？
基本上，基本上显示出有几行其他的代码使得该模型比获取它的领域要有用得多。
或者微调使它比谁都不想使用它更糟糕。
然后我们制定了所有这些不被采用的花哨的对齐技术。
所以我们最初就像第一个版本那样，它解决了这些回归问题。
然后这里标记为PPO，PQX其实就是一种变体，我们在微调中混入了免费的训练数据。
这就减轻了我们看到的许多回归。
我只是有一个快速的跟进。
像您拥有的那样，像保真度微调的数据有多重要？
就像你们一样，你们从人类那里收集数据，对吧？
如果您要使用一些预先训练的模型来评分，你知道，只是用于报告或类似的东西，那怎么办？
是的，你可以这样做。
还有其他论文也是这样。
我记得大概是 Big 写了一篇关于宪法人工智能的论文，试图确切地做到这一点。
在某种程度上，语言模型将能够自动对某些事物进行排名，而对某些事物则不会，因为它不会知道您的确切偏好，或者不会知道我们确切想要做什么。
所以，你知道，每当语言模型做了我们更喜欢的事情时，实际上，我们必须给它另一个数据点，对吧？
换句话说，你知道，如果你要与人类对齐，你就必须以某种方式让人类参与进来。
这样，你知道，否则模型怎么知道你应该做什么？
好的，还有更多问题。
我不知道是不是 Tris。
是的。
大约需要多少，像是，要达到这种直观的订单数量，我们需要多少好的偏好呢？
是的，我马上就会谈到这个。
当然，就像，这里有点太容易了。
是的。
我认为不像你想的那么简单，但是要容易得多。
是的。
那么你为什么决定在这里做实验呢？
我们还没有，我们还没有仔细比较过各种强化学习算法。
很可能不同的强化学习算法会更好。
这有点像，我不知道，PPO是在眼睛上发明的。
所以我们使用了它。
这不是说不好，我们也做过其他的。
它也运行得相当好。
是的。
你应该选择哪些人类的标签？
这就是比较的关键。
比较，这比另一种更好。
所以通常我们让人们在通常是不同模型的三到六个不同的响应之间进行比较。
那么PPO和当前在Chat GPT中使用的奖励模型是什么？
是的。
正在使用中。
这有点像，你使用任何人类反馈吗，比如，你知道，重新生成回复之类的东西来作为奖励函数的一部分吗？
你是怎么理解重新生成的？
就像在 Chat GPT 上有一个按钮，你可以点击重新生成回复，或者你使用任何隐含的反馈来帮助人类使用吗？
我不知道目前的情况如何。
我预计人们会尝试使用它，但你知道，Chat GPT 还没有发布多久。
所以我对这个图表很好奇。
就像你提到的那样，定量上，增加参数并不能让结果更加准确。
从定性上来说，你已经追踪了一段时间了。
你能一眼看出来，如果你与 10 亿，或者百亿模型进行交互，就像一个图灵伪图灵测试一样，根据参数大小告诉我它有多少个参数吗？
可能不是非常精确，但我认为重要的反问是，我可以写提示吗？
所以如果你只是从人们在 playground 开头放入的任何随机提示中抽取，那么我可能需要相当多的提示来区分它们。
但是如果我能写提示，我可能可以在一两个之内完成，至少如果任务是告诉区别这个和那个的话。
我想，我可以再做两张幻灯片，也许你的问题会得到答复。
然后，这是关于培训成本的问题。
所以，另一件让我感到非常惊讶的事情是，与预训练相比，它的成本非常低廉。
所以，如果你看一下训练 GPT 吃掉的 FLOPS 数量，然后将其与微调和 RL 的成本进行比较，预训练的最昂贵版本仅占预训练计算机的不到 2%。
如果你想要训练一个更大的模型，它会更昂贵，你仍然可以使用相同的微调步骤来使其更加一致。
当然，我认为在这里也需要注意的重要事情是，我们还没有解决所有的问题。
有一些重要的限制。
所以我不会说这是最后一个版本，我们也没有试图弄清楚如何在未来花更多的计算资源和人力数据。
但总的来说，它出人意料地有效。
好的，没有更多的问题了。
还有更多问题吗？
有。
我只是想问一下PTFs是什么。
将预训练数据混合到强化学习的微调中，就像混合梯度一样。
简单一点，这个图的分支数量是多少？
所以你为这个固定了分支数量。
所以这是完整尺寸的GPT-3版本。
所以这是一千七百五十亿的模型。
还有更多问题吗？
没有。
哦，Zoom上也有一些问题。
好的，当然。
所以第一个问题是，好的，当然。
所以第一个问题是，你会如何处理在极限情况下的RDFH中断？
示例偏好被认为是价值的良好代理，但为其进行优化被理论上认为是激励设备感知的。
是的，我会解决这个问题。
好的，当然。
当然，下一个问题。
所以这就像，你想要自动化对齐研究。
如果你需要专家难以书写的概念性大工具，会发生什么？
好的，这也将是最后的一个很好的观点。
当然，让我们看看。
抱歉。
是的，我猜有一个问题是，fine-tuning 在直接使用人类反馈时与在iOS上进行fine-tuning相比，会有怎样的指导？
Fine-tuning，就像监督式的fine-tuning吗？
更像是如果你直接使用人类的反馈数据。
我也不确定需要什么。
好的。
所以，我的意思是，这里展示的一个基准是，如果你只是采用人类演示，也就是说，我们有一堆任务，只是请人类去做，记录他们做了什么，然后训练模型去模仿那个。
在这里，就像是非常基础的行为克隆，只是使用与预训练中使用的相同的损失。
然后，你知道，这比Q-shirt提示版本明显好，但仍然不及强化学习。
这就是为什么我们喜欢使用强化学习。
基本上，概念上，模仿人类方法有两个问题。
一个是人类在某些方面比模型更擅长，而在其他方面更差。
所以，在模型较差的方面，你试图模仿一些你自己无法做到的事情。
而在模型更擅长的方面，你却使模型变得更差，因为你强迫它按照人类的方式做事。
所以在强化学习（RL）中，使用RLHF，你在某种程度上让模型随心所欲，它可以自行找出最佳的做事方式。
还有另一个重要的优势，我会谈到这一点，但我想简要谈谈chat-gbt。
所以我有一种把chat-gbt看作是对Instructure-gbt的升级的想法。
这有点像使模型更加符合人类需求和更有用的下一步。
有些方面chat-gbt做得更好的是，你知道，我认为chat更擅长将对话作为通用界面，对吧？
你可以直接与它交流，可以问跟进问题，可以要求它优化答案，所有这些。
这使得处理变得更容易。
它更擅长拒绝有害的任务，但仍然存在一些重要的局限性，对吧？
最大的一个问题是模型会产生幻觉。
无论你给它什么任务，它都会编造事实，这使得它相当不可靠。
它仍然对提示很敏感，这表明它仍然存在我们需要解决的重要不一致性。
真的，如果模型像，模型应该真的尽其所能完成任务，无论你如何促使它这样做。
但是，有一个我认为非常有用的重要原则，或者我会倾向于的是，评估比生成容易。
所以，如果你让人类比较和排列模型给出的不同响应，区分模型所做的不同变体要比完成任务本身容易。
或者换句话说，你知道，你可以在任务上进行比较。
你仍然可以在你自己做不好的任务上发现良好的行为。
所以，如果你提供这种让系统做得比你实际能做得更好的反馈。
我认为这是一个在许多领域都适用的非常普遍的原则。
所以，你可能最熟悉的是，如果你学过计算机科学，你知道 P 与 NP，每个人都知道，我们实际上不知道它们是否不同，但实际上似乎 NP 任务只是更难一些。
它还适用于许多其他场景，比如很多专业体育或电子竞技，如果你不能轻松判断谁更容易赢，而不是在专业水平上竞争，观赏起来就不那么有趣。
它适用于许多消费品。
你可以看两部智能手机，并告诉自己哪个更喜欢，而不仅仅是看规格，但要打造一部好手机实际上是非常困难的。
它还适用于学术研究。
你知道，批评一篇论文，指出其中所有的问题要比自己写一篇好的论文更容易。
它适用于，我不知道，当你，是的，基本上有很多领域都适用。
所以我认为这个原则在我们希望使AI系统在我们自己可能做不好的任务上取得良好表现时非常有用。
好的，说了这么多，我们的OHF有一些非常重要的局限性。
我认为这会使使用我们的OHF来扩展对齐变得非常困难。
让我用一张图解释一下。
基本上在X轴上，这是AI的进展，而在Y轴上，是不同任务的难度。
随着我们的AI进展，AI能够完成的任务的难度会提高。
而一个基本问题是，人类可靠评估的任务水平不会提高，因为人类不会随着AI的进展而变得更好。
所以我认为我们现在大概处在这个阶段，但问题是一旦你跨过这条线，你就不知道你的模型是否真的在做正确的事情，因为你无法再可靠地评估了。
这就是我们的OHF训练开始出问题的时候。
也许我们将会看到的情况是，系统现在会优化为我们给出的任何反馈。
所以它们会试图告诉我们我们想听到的，而不是它们知道为真实的所有事情。
它们可能会学会如何欺骗我们，因为这样可以更容易地获得更高的偏好评分。
我们想要利用的基本思想与我刚提到的原则相关，即评估在生成方面更容易。
所以，举个例子，如果你有一个大型语言模型，正在编写一个代码库，就像整个代码库一样，人类根本无法找到代码库中的所有错误和缺陷，或者代码库中可能存在特洛伊木马，你可能无法察觉，因为这太困难了。
这就是为什么我们在这里看到那么多有bug的代码的原因。
但是，如果你让你的语言模型找到错误并指出它们，一旦你看到错误，你就能更容易地说，哦，是的，这是一个错误，请修复它。
所以现在，你把一个代码库的任务简化为，好吧，我只需要评估这是否是根据我心目中的规范的一个错误。
因此，我们在这里激动的一般原则就是，你想要利用人工智能来辅助人类进行评估。
所以希望是，如果你将人类与人工智能配对，实际上你会得到一个看起来更像这样的线，人类与人工智能一起可以评估的远远多于他们单独可以评估的。
那么为了使这更具体，有两种不同的方式，或者说有很多不同的方式你可以做到这一点。
我想要强调的有两种。
第一种是你可以要求人工智能撰写一篇评论。
这是我们去年完成的一个项目。
而在这种情况下，它是一个简单的总结任务，我们训练了一个语言模型，有点像，说一些总结中存在的问题。
还有其他你可以做的事情。
例如，你可以给人们提供聊天 GPT 并询问，好的，使用聊天 GPT 来帮助你评估。
然后你可以要求批评，或者你可以要求很多其他的东西。
你可以要求解释，你可以要求事实核查或引用，或者像聊天 GPT 这样的模型可以可靠地帮助你的任何事情。
所以想法是使用人工智能的帮助，你可以得到人工智能拥有的所有智慧，并利用它来弄清楚你应该如何评估这个系统正在做什么，以及它是否与你的偏好一致，或者它是否试图欺骗你。
而这个问题的一个大问题是我们怎么知道它是否有效？
而其中一个困难是，根据假设，我们正在处理一个难以评估的难题。
而且我们也希望任务是真实的，因为我们不想解决那些无关紧要的难题。
所以情况就变得不同了。
所以你需要像一个真实的艰难任务，但如果你有这些，通常你没有真实数据。
所以你不知道哪个是正确的答案，你怎么知道助手是否在工作，或者是在偏向每个人说同样的话。
所以我们在批评中使用的一个简单技术是，我们称之为有针对性的扰动。
所以你要做的是有一堆提示。
这可能是像人们在聊天GPT中输入的任何内容。
然后你可以拿出你得到的回应，说，这是正确的回应。
实际上它不一定是正确的，但我们就假设它是正确的。
现在你要求一个人类引入一些微妙的缺陷，这些缺陷很容易被忽视，但却是重要的缺陷。
现在你拥有了一对好回应和坏回应的数据集，你知道哪一个是好的，哪一个是坏的，因为你让它变得更糟了。
这样一来，你就有了真实的数据。
现在你可以随机选择一个，要么是正确的，要么是有缺陷的回应，然后把它展示给一个人类，要么是一个人类与助手一起。 
然后他们必须说这是正确的还是有缺陷的，或者回答有多好。
在这种情况下，我们可以试着使用各种不同的人工智能助手或可扩展的监督技术，然后说，你知道，这实际上是否帮助人类更生动地发现我们植入的缺陷？
如果是的话，你知道，你确实在帮助。
所以我们在这篇批评论文中做了这件事。
所以这是训练语言模型写评论和总结，我们可以展示的是，当我们在评估过程中辅助人类进行评论时，他们实际上发现了比以前多50%的缺陷。
所以这在某种程度上表明，你已经可以在今天的模型中使用这些真实的迹象来帮助人类评估并发现他们否则可能会错过的问题。
当然，我们仍然必须在一个更困难的任务上做到这一点，以及在某种意义上有一个真正的任务。
我们也希望有更大的效果大小，但我认为这只是显示出这些技术已经开始起作用的希望。 
所以从长远来看，我认为我们想要实现的目标是利用人工智能来完成所有评估我们的人工智能系统所做工作的认知劳动。
这可能是，你知道的，阅读所有相关内容或事实核查或进行计算或编写代码或任何这些事情。
然后人类应该专注于他们的偏好输入，比如弄清楚他们实际关心的事情以及他们希望模型做什么。
这样，我们可以利用，你知道的，人工智能玩家将带来的能力以及他们最终将比我们更擅长的事情，然后利用它们来帮助传达我们实际关心的事情，你知道的，我们实际想让它们做的事情。
是的。
但是，是的，这些就是主要的幻灯片。
我很乐意回答更多问题。
是的。
我在想这种回答的幻觉问题。
你是否曾经尝试过考虑到答案的某种不确定性的概念？
是的。
例如，批处理和抽样或类似的东西，在这里你有值在答案之间，并在这里以某种方式赋予价值。
所以，采样很困难，因为要么你正在从同一个预训练模型中训练和微调集成模型。
而且，你的集成模型中变化不大，要么你已经预训练了一堆不同的模型，现在你正在花费大量资金进行预训练。
有一件事，我是说，似乎应该是一个可以解决的问题，就是教会模型在它确实不确定的时候说它是不确定的。
在这个方向上已经有大量研究，但我认为现在我们还没有处于一个良好的状态。
还有更多要做的事情。
你觉得当涉及到AI对AI答案的建议时，我们可能会遇到一种信号和噪音比例的问题吗？
因为我确信，当AI试图指出文本中的潜在问题时，人类更有可能报告更多的问题，但如果它注意到的问题人类本来不会有问题，那该怎么办？
是的，所以我们确实试图在一定程度上控制这个问题，比如让人类评价缺陷的严重程度以及他们是否会否注意到它们，但他们仍然可以看到显著的影响。
但也有，我是说，很多时候，模型在挑剔，而这些并不是有趣的案例。
是的，还有如果你喜欢看我展示的例子，我觉得它是来自博客帖子，很多批评实际上都相当差劲。
而且让批评变得容易的一件事是，如果它们大多数都是垃圾，因为人类可以只是阅读它们并丢弃它们。
它更像是帮助评估并知道在哪里集中注意力，或者注意到他们否则可能会忽略的事物。
所以它更像是批评帮助你构思你应该如何评估或其他什么。
但是如果你正在使用助手，你可能希望比填充大多数答案更可靠。
是的，是的。
我们如何确保我们在您的递归模型方法中使用的评估指标，比如检测欺骗和左转之类的，不会出现重大不连续性？
是的，我认为，嗯，这在很大程度上取决于你说的是什么不连续性。
比如，如果你在一夜之间得到一个模型，比如说在参数数量上大了一千英寸，或者等效地更好，那可能会造成相当大的飞跃，这就使得这种评估变得相当困难。
所以从那个意义上讲，我认为更加持续地扩大人工智能的规模并采取更多的增量步骤将会非常重要。
话虽如此，我认为请求奖励建模的基本情况是，你正在训练系统帮助你评估，你知道，从同一系统训练或微调的系统，对吧？
如果你能够想出如何使它们进行微调，以便提及所有实际有用和相关的内容，那么它仍然可以评估系统，即使它们比以前评估过的任何东西都聪明得多。
也许让我更具体一些，因为有一种方法，我认为你可以衡量或者说我们试过的一种方法是使用我们称之为鉴别器评论间隙的方法。
为了衡量这一点，你正在训练两种不同的，或者说你正在从同一预训练模型微调两种不同的模型。
一个是鉴别器，只是说，这是有缺陷的回答还是正确的回答或更正确的回答？
另一个是一个评论模型，告诉你缺陷是什么。
现在你可以衡量鉴别器多频繁地正确获取了什么，而鉴别器只是纯粹的梯度下降。
它并不一定想要告诉你任何东西。
你只是基本上钩入模型的表示，并试图获得它对这个预测任务的所有相关潜在洞察。
另一方面，你有这个评论模型或这个通用辅助模型，你在衡量助手实际上有多少帮助我找到正确的缺陷。
然后，如果你比较这两个分数，你可以看看它们之间的差距，因此这个生成器，抱歉，鉴别器评论差距。
如果差距很大，那么模型了解了许多任务而没有告诉你。
那将是非常令人担忧的。
如果差距很小，那么，你知道，模型可能没有向你隐瞒太多事情。
但我想确保这种方法是健壮的，并在从我们学到的东西中进行争论，确实有很多价值。
是的。
这就是为什么我们想要在当前模型上进行测试。
是的，我不知道谁是第一个。
我想是后面的某个人。
所以我想问的是，也许在最后，你有一页幻灯片，上面列出了人类的流派，就像，你知道的，我忍不住注意到其中一部分也是关于传达你想让AI做什么，对吧？
不仅仅是评估，而是像沟通，就像也许我想让你做这个，但也许它做不到。
所以，至少在我个人使用聊天GPT的经验中，有些事情确实能做到，令人惊讶的是，但因为突破了某种新的隧道，比如，那时我就像，不，那是怎么回事？
或者，你知道的，如果你可以询问，就像在提示中，有不同的事情，我会想，好的，我能要求什么？
以及它可能有什么？
我觉得有点令人担忧的一件事是，你知道的，人们总是坦诚地传达他们的偏好，或者可能存在协调的努力，对吧，以奖励特定的能力，你知道，为了做这样的事情而协调努力。
但我有一个想法，我试着问一下，它是否有自己的维基百科之类的想法，一开始我不知道如何使用它。
所以我只是想，也许没有一个，但我希望有一个，就像 GPT 三那样，对吧？
就像我觉得布罗克曼有点像一个行家。
所以我希望。
所以我的问题是，你怎么样才能让这种东西变得安全，对吧？
就像，你怎么样才能识别协调努力，以特定地奖励某些类型的行为，也许像某个团体决定他们想要的，你知道，给它一些能力。
所以这是一个，你知道，是的，这是一个很好的问题。
而且，从某种意义上说，我是说，你不应该做的第一件明显的事情就像，你不应该只是简单地打开人们使用界面的数据。
我们已经有点看到了如果我们这样做会发生什么其他的例子。
如果你想到像 Microsoft K 或者类似的东西，那会出大问题。
另一件事是，我是说，现在，我们正在做的就像，我们正在雇用一群人然后让他们评价不同的模型回答。
但现在问题变成了，你知道，谁会被雇佣以及他们的背景是什么？
他们试图做什么？
所以，特别是像这样的事情，我认为我们现在做得很差的一件事情是，实际上导入了一个多样化和代表性强的人类偏好集。
而且更像是，你知道的，我们最终雇佣了谁。
所以我有点希望也有这样更有针对性的研究，关于我们应该如何做以及如何做得好。
有些事情也是这样的，你知道的，更适合放在大型科技公司之外。
因为如果你是科技公司，总是有动机去以一种也许不是我们实际上，人类在反思下会做的方式导入人类偏好。
所以我认为这是一个非常重要的问题。
这种光的衰减是否就像数据污染的双重问题？
比如，你认为互联网可能受到了污染吗？
很可能是，是的。
我的意思是，人们可能会，任何人都可以毒害免费培训，对吧？
只需把东西放在互联网上。
这是我们必须非常注意的事情。
是的。
考虑到我们目前正在训练这些模型，希望此时更接近人类偏好。
随着人类偏好的变化，我们看到了，有像，是有类似的东西，是的。
我是说，最明显的就是，模型的知识库有点像是预训练截止日期，就像某人，你知道的，无论你在预训练中使用了什么数据，它都不知道之后发生的很多事情。
就更新人类偏好或者进入奖励模型的比较而言，你只需收集更多数据并重新训练。
微调的运行相对来说比较便宜，所以你可以再次进行。
我认为变得更难的是，你知道的，当你部署了模型并且人们开始将其用于各种他们想要围绕公司构建的任务时，如果你更新并且改变了模型，那么他们也必须做大量工作来调整他们的提示以适应他们正在做的事情。
所以这并非没有代价。
是的。
抱歉，你。
所以关于超越人类表现的问题，但如果 GPT-3 的优势在于它拥有整个互联网的庞大语料库呢？
如果你想专攻某个特定领域，比如化学或材料科学之类的，并且正式生成新化合物之类的东西，那么要做的事情就是适应，比如使用更少的数据，尽可能有效地学习有关它的知识。
你是说，比如化学领域的数据少一些？
是的，我们只有过去大约30年的研究论文之类的东西。
是的。
然后你可以把它们用于预训练，对吧？
然后模型就了解这些了。
模型真的能在没有那么多数据的情况下有效学习吗？
它是否可以以某种方式来适应GPT-3背后的抽象概念？
是的，我是说，这就是你打算用微调来实现的一般想法。
在某种程度上，我们已经看到它像这样泛化。
例如，InstructTBT几乎完全是在英语反馈和演示上进行训练的，但它也可以在其他语言中工作。
这就有点不可思议了。
同样地，你可以训练模型，让那些对化学一无所知的人来使用，然后它会学会遵循指令，就像在化学这个主题上一样。
这种微调可以非常高效，比如说用100个数据点，你就能够真正地改变模型的行为。
所以这是非常有效的。
我要挑选一个还没有提问的人。
关于响应生成，你会投入多少努力或者重视训练不同的表达风格呢？
所以我注意到 ProtectTBT 总是给你提供非常结构化或科学结构化的答案。
如果它给你一个科学结构化的答案，你会考虑进行任何训练吗？还是更倾向于一个抽象的答案？
是的，我是说，棘手的地方在于理想情况下，模型应该给你想要的答案，对吧？
有些人更喜欢科学或技术性的答案。
有些人可能更喜欢一个更通用的答案。
而且，我是说，像 CheckTBT 现在并没有一种方式让你设置自己的具体偏好，这是一件非常令人兴奋的事情。
但是我认为你观察到的统计学上的特点实际上可能是我们标注人员组的产物。
所以很多 CheckTBT 的工作者更像是，你知道的，更像是计算机科学方面的，而且更多的数据是由程序员生成的，与 InstructTBT 相比，后者更像是一般的标注者。
嗯，有不同，有点像，它也改变了风格。
所以没有具体的区分努力吗？
是的，我是说，我们应该做出明显的努力。
它应该给你想要的风格，对吧？
是的。
所以我一直在思考的一件事是，CheckTBT 将如何影响年轻一代或即将到来的一代的教育。
所以你必须抓住人工智能的进展和人类水平，是的，让事物得到评估。
我在想的是，我在休息期间使用了这个，我与我的 10 岁表妹分享了她的 CheckTBT 只是为了玩玩。
那个绿色的线低得多。
而且，如果说它只是成为他们教育经历的一部分，对他们来说将会更困难，或者我认为他们将更难以区分甚至类似的任务，比我们现在所做的要困难得多。
所以我已经在思考，这可能会如何扰乱或使这种对齐在长期内变得更加困难，因为你有一些人更多地，比如，像CheckTBT所说的一样，将其视为理所当然。
是的。
我只是在想你...
我的意思是，存在一种真实的风险，过度依赖一种不成熟的文本，还没有准备好的，你知道，你只是相信，比如，请不要相信模型说的一切，对吧？
但我也觉得我有一个希望，就是你的表兄最终会找到如何做到这一点的方法。
就像，你知道，他们在成长过程中，你知道，他们成长于所有这些正在变得更好并学会如何有效利用它们的人工智能工具之中。
对吧？
而且，这有点像，你知道，大约20年前或者什么时候，当你比其他人更早地使用谷歌搜索时，你可能会变得更擅长于像用工具那样使用它来做任何你想做的事情。
还有其他问题吗？
我想你已经举手很久了。
就像任务和聊天任务和主体任务以及模型任务的幻灯片一样，对吧？
哦等等，最后一个？
最后一个。
是的，现在看起来你们好像正在将这些用作真实世界的生物传感器，就像物理的基本事实，并使用语言作为对这种基本事实的压缩接口。
你们也考虑过直接使用附属技术来更真实地回答，你知道吗？
是的，我的意思是，这取决于那种感觉可能是什么。
对吧？
就像，我想最直接的一件事情就是你可以要求模型浏览，然后它可以像事实核查自己的答案，它可以，你知道的，导入它没有记住的外部知识。
而且，我认为那将会非常有用。
我认为那也将会对辅助人类评估非常有用。
那个开放式工作是什么？
你可以看一下 web GPT，你知道的，这是一项关于使用该模型进行浏览的已发表作品。
我认为，当你在使用这样的外部传感器时或者如果你让模型与真实世界更直接地互动时，会变得更加困难的一件事是，它会引发更多的安全问题，对吧？
如果你让你的语言模型进行任意的 API 调用，那么你就必须更加小心地选择要进行哪些调用和哪些不要进行调用。
如果你是在审查模型的一切，而不是像你正在审查模型说的一切，那么你可以决定哪些你想要采纳。
所以是的，这是一个未解之谜。
好的，再问一个问题。
我觉得你没有。
关于这些大型语言模型的推理能力。
我见过不同的人谈论这个，就是它是否会像人类一样，每个令牌有一个固定的计算量，而人类则有系统一和系统二，我们可以快速说话，而实际上使用推理和思考需要更多的努力。
然后我看到其他人尝试使用力量来进行提示链或推理链，或者像大步骤一样逐步进行。
你认为这不足以做到我们想要的水平，还是需要真正的大量微调或架构变更？
我不知道。
我也不是问这个问题的合适人选。
我主要不是试图让模型具有新的能力，更像是，你知道，让它们加入人类团队。
哦，我们想要进行在线提问吗？
是的。
我把我的问题搅在一起了。
那么你认为，对于离线成年人来说，这是一个从机器中创造出来的世界吗？
尤其是如果你得到像人类一样，不是一次性教授。
模拟人类感染是困难的。
那么你认为，类似 PPA 这样的东西更适合离线吗？
是的，很有可能。
我是说，正如你指出的，有很多对话数据，如果你能利用它，那应该是有用的。
我认为，广义上你可以将这种事情归类为，让强化学习算法变得更好，以及强化学习反馈。
而且我认为这是有价值的，它应该能帮助我们使得相同的预训练模型更符合我们收集到的人类偏好。
但是你仍然会遇到强化学习有的所有限制，对吧？
我认为有一个针对 GPT-3 的微调 API。
我认为它现在还不支持强化学习，但是支持监督式微调。
所以你可以做像，你可以提炼出最好的 N，然后进行这种专家迭代强化学习。
所以第一个问题是，你能更清楚地描述一下 GPT 的完整训练过程吗？
例子，从文字开始，不看001s，然后XDB或Y步的编程数据被突出显示。
抱歉，我没听清楚。
从int 001的文本开始。
那么你能使用多少数据，突出显示多少步骤呢？
我认为确切的数字并不是公开的。
这基本上类似于指导GPT。
至于指导GPT的数字，我们有，我认为大约有50,000次比较，可能有10,000次演示，或者可能是数万次。
我不记得确切的数字。
所以我有这张其他幻灯片，是的，大约有20,000小时的人类反馈是我计算出来的。
你可以仅仅遵循人类反馈的确切数字，因为你可以得到像100万或者你想要的任何交易次数。
对，我是说，最大的问题是，你如何制造，你如何确保质量？
是的，可以创建出这种模型的某种翻转模型，幸运的是银行。
但这是整个问题，对吧？
就好像你已经有了一个信任的模型。
好的，当然。
下一个我记得的问题是，你想要自动化对齐研究。
如果您需要概念性的大型工具，而这些工具对专家来说非常遥远，会发生什么呢？
是的，我的意思是，这个计划的雄心壮志是训练一个能够进行这种概念性研究的模型。
你可以想象那就像是一个语言模型，写了一篇我们读过的对齐研究论文，然后我们会说，哦，这是一个非常棒的想法。
我们应该试试这个。
我认为，回到评估在生成方面更容易。
我认为这也适用于对齐研究。
就像，我至少觉得，评估对齐研究要容易得多，而不是产生它。
所以，虽然我们可能需要一些概念上的突破，但我们现在甚至无法评估它们，因为如果我们看到它们，我们会说，这是什么？
这就像是，这就像是我们想要进行可扩展监督的原因，对吧？
因为如果语言模型产生了这个真正精彩的见解，而我们当时甚至无法认识到它，那么如果我们使用人工智能辅助，我们应该能够更容易地认识到它。
如果我们利用我们最好的人工智能模型来确定这是否是一个好主意，它的弱点是什么，它的优点是什么，我们应该进行什么样的实验来知道这是否是一个好主意？
所以，是的，我认为基本上，只使用RLHF来训练模型进行良好的对齐研究的故事，你会面临明显的陷阱，就是，模型可能会写出一个看起来对我们来说很好的对齐提案，但实际上并不是一个好的提案，它会创建与人类不一致的人工智能。
为了区分这两者，这可能非常困难，也许不是，但我认为我们应该期望它非常困难，然后利用人工智能辅助来评估这一点似乎是一个非常有前途的计划。
所以我认为EGI涉及的是系统的频率将会产生的影响。
我的意思是，这就是我的全部观点，这是不够的。
但是你认为，你是否需要像公司正在尝试做的那样，你是否认为，如果你能获得大量的反馈数据，那么这将足以代代相传，或者你是否认为还需要更多的思考？
我是说，那些广义学家觉得，我认为基本上模型的绝大部分能力，以及你看到它做的所有很酷的事情都来自于预训练阶段，而不是来自微调阶段。
人们有时将其归因于微调阶段的原因是，你在预训练模型中没有看到它。
而我认为的原因是，我们在预训练模型中没有看到它的原因是因为预训练模型是如此不协调，它并不试图帮助你，也不试图向你展示它所能做的所有事情。
相反，它只是重复一堆随机的网络文本。
而这不是你在寻找的。
所以，是的，我认为RLHF基本上一直在做的是解锁模型中已经存在的能力，并使这些能力可供人类使用。
在某些方面，对齐研究在某种意义上是非常双重使用的，因为，首先，如果你有非常好的对齐技术，你可以用它来与任何你想要的价值观对齐，包括我们可能不会特别支持的价值观。
其次，如果你正确地进行了对齐，它看起来总会有点像是你让AI系统变得更有能力，因为以前帮助你真的不那么困难。
现在，你已经使它更加对齐。
所以，你知道，实际上你已经拥有这些能力。
当然。
让我们看看。
是的，我想这以前也是一个问题。
你如何处理并行获取处理？
所以这就是不同的近似值，但为它们进行优化以激励欺骗。
那么你如何对付这个？
是的，这就是我在这里谈论的，对吧？
就像我们现在所面临的整个问题，人类可以评估的东西是恒定的。
所以我们将无法评估对欺骗我们的复杂尝试。
这就是为什么我们想要可扩展的监督，以便赋予人类发现这些欺骗尝试的能力。
你的城市监控网络如何应对流行观点的决策变化，因为其他AI系统的改变而引起的？
是的，我认为这些是真正的担忧。
在某种程度上，我们必须经验性地测试它们到底有多困难和严重。
我认为，所以我现在的个人立场大概是，我认为努力使外部对齐信号达到最佳状态将会占据大约 90% 的工作量。
一旦我们达到了这一点，那么许多其他事情也可能会迎刃而解。
所以例如，我的意思是，这在某种程度上取决于你担心哪种内部不对齐的情况。
但其中一种情况是，你在训练你的系统，它基本上学到了一堆内部优化器，有点像元强化学习。
例如，像 GPT-3 可以做上下文学习，这就像一种学到的优化器。
所以现在你正在进行所有的 Jeff 训练或者任何你有的对齐训练。
而且你的优化器学会了在分布上做你想要的事情。
但是现在如果你有一个分布转移，这个分布转移可能是自动诱导的，意思是模型本身在引起它。
现在你走出了解决方案，所有这些内部优化器，都试图优化其他的东西。
有一种方法你可以像，你知道的，就像实际发生多少这样的情况不太清楚，但更重要的问题之一是，如果你有一个非常可靠的外部对齐信号，而且你有这样一个你信任的通用训练信号，你也可以利用它，在新的分布上训练系统更多，或者说是将这些内部优化器调整到一行中。
这样，你就把内部对齐问题简化为，如何处理分布偏移以及如何构建一个你信任的外部对齐信号的问题。
而这些问题我们无论如何都必须处理。
但是，我不知道它实际上会如何发展，有一些重要的开放性问题。
所以关于对齐，遇到一种问题是，看到一些讨论不太感兴趣，比如解释为什么会反映一些其他判断。
这就是AI或分解这些模型所引起的兴趣之一，就像解释为什么即使第二个约束也是相当的，这绝对是一条路线。
关于它为什么做出这些在线判断，你们能够审问模型吗？
我是说，我认为我们现在的情况相当不活跃，尽管令人满意。
我是说，你可以问模型为什么给出了某个特定的回答，但你不知道它是否真诚地回答了。
而且你也可以，我是说，另一件事情是你可以给模型提供它自己的回答，然后让它发现缺陷，这就是我们在论文中所做的。
但我认为，就像，我是说，有一个版本，你试图让事情变得更好，但问题是，你的信号基础是什么？
我认为，更好的攻击角度可能是可解释性，你知道，你弄清楚如何查看模型内部，以及它实际上是如何工作的。
这就是我问的问题。
我们解释这些的研究水平，似乎对于这些更大的模型来说确实很难做到。
这是一个非常高调的领域。
你目前的思路是朝着减少这一点的国家性质发展的，但数据是一件非常难以处理的事情。
是的，我是说，我们正在解决这个问题，但我认为我们现在没有什么可以展示的东西。
所以似乎一般来说这不是一个非常容易的问题，但你知道，我很希望我们能做一些事情。
我认为总的来说，可解释性的问题，或者说利用可解释性来进行对齐的问题有点棘手，因为我怀疑它既不足够，也可能不是必要的。
所以你能利用的任何解释性都会很有用，因为这是在你的工具箱中的另一种工具，用于检测欺骗或者知道，你知道，模型为什么给出了某个答案并做出了某个决定。
但是，你知道，如果你真的在解释性方面变得非常擅长，如何利用它进行对齐就有点不清楚。
比如，你可以查看模型，然后将你能找到的所有有对齐问题的模型都抛弃掉，但是这样不就是在选择那些具有通过解释性很难找到的对齐问题的模型吗。
确实，对此有一个跟进的问题，我们可能会询问关于实践日志的实践，即你必须提供解释。
是的。
而且我猜我的问题是，如果不是必要的话，你为什么要采用解释性呢？
是的，那为什么不是必要的呢？
所以，再次强调，这有点像是一个开放性问题，但基本上你可以采取的立场是，到头来，真正重要的是模型实际采取的决策，而不是它们采取这些决策的原因。
所以，如果你能确信模型实际执行的一切都与你期望的一致，那么模型内部认为什么就不重要了吗？
我不知道。
你可以接受这一点，你可以接受评估模型。
是的，这就是我们正在尝试做的，对吧？
就像我们正在努力制定一个非常好的评估信号。
然后你可以选择，你可以训练模型做你想让它做的事情，因为你总是可以比模型更好地评估它可以做的事情。
是的。
我可能不是这个问题的专家。
我认为你做得很好。
我们做得很好。
非常感谢大家的精彩讲座。
非常有趣。
我可能实际上应该谈谈GPT类型的顶端。
我可能会像现场应用一样进行。
是的，当然。
只是为了结束课程。
那么你觉得这件事怎么样呢？
只是想着你们所有人。
我们所要做的一切，我们应该给我们的发言人鼓掌吗？
