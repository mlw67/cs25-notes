好的，很好。
我要讲的第一篇论文是《大型语言模型的新兴能力》。
这篇论文特别棒，我认为，因为我们有来自谷歌、DeepMind以及斯坦福的人员，你可能会认识 Percy 或 Tatsu 或 Rishi。
我们让大家达成共识，形成一个良好的框架，解释为什么我们想要扩大规模和新兴能力。
所以在语言模型中，我们看到的一件事是，通过扩大规模，我们会得到可预测的增益。
这是经典的 Kaplan -Adol 论文，你可以看到，如果你扩大语言模型的大小，无论是以计算量、数据集大小还是参数数量来衡量，测试集上的损失实际上是可预测地降低的。
我不知道你是否在共享屏幕，所以Zoom上的人可能看不到幻灯片。
好的。
是的，是的，是的。
抱歉，让我解决一下。
好的。
好的。
我想我要第三次说这个了。
正如我们在语言模型中所看到的，如果你扩大语言模型的规模，无论是以计算量、数据集大小还是参数数量来衡量，你会发现测试损失会有一种可预测的改善。
现在，我要谈论的是涌现方面的内容，如果你只看较小的语言模型，实际上是无法预测的。
在更广泛的科学文献中，有一种描述涌现的方式是将其视为从数量上的变化而产生的一种质的变化。
我是说，它在某种程度上始于一篇由诺贝尔奖获得者、物理学家撰写的《更多的东西是不同的》这篇文章。
我真的很喜欢 Jacob Steindart 的这篇文章，他在这里描述了涌现，并举了几个很好的例子。
例如，他说铀，少量的铀并没有什么特别的事情发生。
但是如果密集地堆积了大量的铀，就会发生核反应。
而且还有 DNA，例如，仅凭钙等小分子，你无法有意义地编码有用的信息，但是如果给定了较大的模型，如 DNA，你就可以编码基因组。
因此，对于这项特定的工作，我们使用了大型语言模型的涌现能力的这一定义。
所以我们说，如果某种能力在较小的模型中不存在，但在较大的模型中存在，那么它就是 emergent 的。
在这里，一个自然的问题是，如何衡量语言模型的规模或尺度？
传统上有三个尺度方面。
训练 flops 是用于训练语言模型的计算量。
模型参数的数量就是语言模型的大小，也是模型所训练的训练数据集的大小。
这里的很多图表将使用训练 flops 或模型参数的数量。
原因是对于不同大小的语言模型，训练数据集的大小通常是固定的。
因为训练 flops 只是数据集大小乘以模型参数，对于大多数语言模型，你可以从训练 flops 或模型参数的数量得到类似的图表。
很好。
所以第一种 emergent 的类型...
是的，抱歉，继续说吧。
对我来说，似乎相对容易衡量尺寸与什么样的能力可以算作一个真正的能力。
你如何定义什么算作实际的能力？
是的，当然。
嗯，例如，我将在这里举个例子，实际上就是下一张幻灯片。
所以基本上我们有一种与语言模型交互的方式，叫做few shot prompting。
它的工作原理是，语言模型是一个非常好的下一个单词预测器。
当你给模型一个例子，然后问它一个未见过的电影评论，例如，然后你说，输出是什么？
然后在这里，语言模型可以说是积极的，因为它理解使用评论的上下文来给出下一个标记。
而我们对于是否具有某种能力的定义是，基本上few shot prompted任务，例如情感分析，如果对于小模型具有随机的准确性，但对于大模型具有超过随机准确性的准确性，那么这种任务就是新出现的。
有道理吗？
所以基本上如果模型的表现不比随机更好，那么当你说它没有执行这个特定任务的能力时。
我将在这里举几个例子。
所以这里是我们看待新出现情况的规范方式。
基本上，我们每个不同的图表表示一个不同的任务，我马上会介绍一些例子。
但是你阅读图表的方式是，X轴是训练步数或模型规模的数量。
然后Y轴是准确性或模型在执行任务时的表现。
然后我们有来自OpenAI、Google和DeepMind的不同语言模型。
然后每个点就像是一个不同的语言模型。
它不是一个语言模型在训练过程中的表现。
就像每个点都是一个不同的语言模型。
你看到的是，对于非常小的语言模型，基本上得到的性能接近随机或不比随机更好。
然后一旦你超过某个阈值，你会看到性能突然变得远远超过随机。
这就是我们所说的出现现象。
所以基本上，如果你要推断从小语言模型的线，你可能会预测它永远不会比随机更好，因为它就是一条平直的线。
但有趣的现象是，当你超过一定阈值时，你实际上会看到这种出现现象，模型的表现远远超过随机。
所以让我介绍一些具体的例子。
这是一个任务。
基本上是一个名为多任务NLU或MMLU的基准测试。
基本上它是一堆测试题，从高中一直到专业水平的考试。
它的工作原理是，语言模型会被给定，例如，这是一个高中数学例子。
语言模型会给出一些例子，然后对于一个未见过的问题，它必须给出答案。
然后你可以在右边的图表中看到，如果模型规模达到了大约10^22的训练浮点数，那么在这个任务上，你实际上不会比随机准确率更好。
但如果规模扩大到10^24的训练浮点数，那么你会发现所有三个模型的表现都比随机准确率好得多。
是的，继续。
用于训练这个数据的规模，大致上是相似的吗？还是因为这些是由不同工作训练的不同模型？
是的，这些模型的规模大致相似。
但是每个轨道上的每个单独点都是相同的数据。
是的，除了Chinchilla之外，数据是固定的。
Chinchilla为更大的模型使用了更多的数据，但我相信这里所有其他模型的数据量都是相同的。
是的，这里只是另一个示例，以更具体地展示它。
所以这是来自BigBench基准测试的一个任务。
只是顺便提一下，BigBench基准测试就像有200个基准测试一样。
基本上，它就像是一组众包的基准测试，如果你要做很多工作的话，我建议你看一看。
基本上，任务是语言模型必须接收一个英文句子，然后给出国际音标字母表的音标转写，即IPA转写，它基本上就是如何发音。
对于这个任务，评估指标实际上是蓝色或N-gram重叠度量。
当你增加语言模型的大小时，你会得到一个类似的现象，一段时间内它是平的，然后突然改善超过了随机水平。
很好。
所以我将谈谈这里另一个有趣的结果，这与出现有关。
所以这是我们几个月前发布的一份技术报告。
基本上有一个非常有趣的奖励，或者说是语言模型中的一个一次性奖励，Anthropix，就像一个初创公司，基本上有这样一个奖励，如果人们能提出一个任务，在这个任务上的性能随着语言模型的增大而实际上会下降，那么你就能得到很多钱。
所以基本上有一些任务，他们发现如果增加语言模型的大小，性能实际上会下降。
所以这个任务是，我就在这里读一下，就是，把我的句子重复给我听。
然后输入是，闪闪发光的不都是璀璨的。
然后输出是模型必须准确地说璀璨。
所以发生的事情是，对于小语言模型，它不知道这个短语，闪闪发光的不都是黄金。
所以它只是复制输入，实际上百分百地完成了这个任务。
但是对于中等大小的语言模型，你会看到性能实际上下降了，因为中等大小的语言模型知道这个短语，闪闪发光的不都是黄金。
然后它说黄金，这实际上不是任务要求它做的事情。
是的。
在 Zoom 上有人问，你能否给出 10 的 24 次方 FLOPS 的物理估计，可能是以训练时间或 TPU 数量为单位？
是的。
所以我认为 10 的 24 次方 FLOPS 大约是...
在谷歌，我们使用 TPU，一个 TPU Pod，我相信相当于大约 4,000 个 A100。
而 10 的 24 次方 FLOPS 就像是大约两个 Pod 的使用时间为六周左右。
所以进行预训练需要大量的计算。
我不知道，但你们还记得化学课上的摩尔吗？那时候会有 10 的 23 次方，然后老师会说，哦，别想这个数字有多大。
那就是用于一些模型预训练的浮点运算数量。
好的，很棒。
无论如何。
所以是的。
所以基本上中型语言模型实际上会表现得更糟。
哦，是的。
你还有其他问题吗？
是的。
这个是否获奖了？
这个是其中一个获奖者。
我想这是第三名获奖者或类似的位置。
因为我最初的想法是，哦，你可以简单地改变一个负号。
你什么意思翻转一个负号？
嗯，因为我们所有人都取决于你用来衡量是否正确执行任务的评估速度。
所以就好像测量非常稀疏一样。
只有你完美地完成任务才能得到认可。
它可以展示很多东西出现，因为你不可能完美地击中它。
这就好像经过长时间的优化。
或者如果你拿一个任务然后用这个减号来评估它，会不会变得更糟？
有点预测性的东西。
是的。
所以对于这件事，他们考虑了所有因素，你不能只说任务应该在某些方面做得不好。
它必须是一个有意义的任务。
然后我猜你提到的关于学分时间或评估指标的工作实际上是一个非常好的观点。
是的。
所以我想如果...
我猜争论是如果你分配部分学分，绩效可能看起来不是 emergent。
但是我们有很多...
我稍后可以举一个例子，但即使你使用部分学分度量标准，你通常仍然会看到相同类型的 emergent。
所以这并不纯粹是基于测量值的部分分数不分配的现象。
然后，很好。
所以我们在这篇论文中所提出的观点是，是的，可能会有一些任务，在那些任务中，如果你使用中等规模的语言模型，性能会开始下降。
但是如果你一直扩展到我们在谷歌公开知道的最大的模型，Palm，你会发现这个语言模型，我实际上可以回去正确地完成任务，因为大型语言模型也知道这个短语，所有发光的东西都不是金子，但它也理解重复我的感觉。
所以它能够在这个任务上达到 100%。
所以这也是一种不同类型的出现。
我们在论文中讨论的另一类出现是一种新型提示技术。
所以基本上，除了未来-事实提示之外，还有其他与语言模型交互的方式可以被视为新型。
是的。
我可以插话吗？
抱歉，我的问题是别的事情。
问题是，所有模型都经历了指令微调吗？
这些模型中没有一个经历了这部分的指令微调。
很好。
是的。
所以与语言模型互动的一种方式是基本上使用一种称为RLHF的技术对模型进行微调。
基本上，它的工作方式是你有这些数据，人们对他们更喜欢的输出类型进行评分偏好。
然后，模型被训练在RL上优化人类偏好。
这个图表显示的是，如果你在模型上进行RLHF，实际上模型在不同的零-shot任务上的表现会变差，特别是对于小模型。
你可以看到蓝线在橙线之上。
蓝线是基线。
橙线是RLHF。
然后，如果你对大模型进行操作，你会发现性能实际上与RLHF有积极的差异。
所以这是一个有趣的事情，一个特定的技术可能只在你尝试了足够大的语言模型时才会有所帮助。
所以如果你只在小语言模型上尝试它，很难得出它不会帮助性能的结论。
然后稍后，我会谈谈作为另一种新兴问题技术的思维链提示。
所以这是我用来思考新兴问题作为一个框架的手势图。
所以在这里，x轴上，有一个语言模型的尺度。
在y轴上是一个虚构的刻度，表示语言模型可以完成的一系列任务。
然后基本上，你可以选择一些随机点，比如说语言模型中的1000亿个参数。
然后会有一些特定的能力。
首先，你可以看到随着语言模型的增大，语言模型可以完成的任务或事情的数量会增加。
然后你可以看到有一些任务，例如，超过1000亿个参数的模型可以完成，但是1000亿个参数以下的模型无法完成。
我们称这些为新出现的能力。
哦，它只是突出显示深蓝色是较小语言模型无法完成的任务。
有道理吗？
哦，这只是意味着我们还不能用语言模型解决的任务。
是的。
我很好奇，你认为白色区域的这些任务是否无法解决，比如1000亿规模？
还是你认为更好的模型、特定的训练数据能让我们在1000亿规模时进入那个白色区域？
是的，我绝对认为这不是一个固定的...
我马上会举一个例子，但并不是说你必须拥有1000亿个参数来完成某项任务。
只是目前我们观察到的模型中这恰好是一个阈值。
我认为通过更好的训练数据、架构和算法，我们很可能能够超越这个阈值。
很好。
是的。
就像Rylan刚才提到的，获得出现现象的一个例子可以通过更好的数据来实现。
所以并不全是规模的问题。
我在这里稍微解释一下。
所以这个任务只是BigBench基准测试中的一个任务。
你可以看到对于Lambda来说，它是一个Google模型，对于GPT-3来说，实际上你并没有从扩展到137或1750亿个参数中获得出现现象。
但当你使用一个不同的语言模型Palm时，它比Lambda和GPT-3训练的数据更好，你实际上可以获得即使是使用小型语言模型也可以获得出现能力。
所以这里是620亿个参数。
所以你是在努力创建一个更好的模型，就像更好的数据或者更好的架构损失选择，还是主要只是数据质量？
是的。
所以挑战性的是...
这是一个很棒的问题。
在Palm和Lambda之间有很多区别，例如。
而且由于预训练的成本，我们无法以任何受控的方式消除它们。
但我们的运行假设是Palm是在更好的数据上训练的，这解释了Palm和Lambda之间的很多差异。
我见过较小规模的情况，可以在那里消除一些东西。
是的。
是的，这是一个很好的问题。
所以我想即使在这里，你也可以看一下，例如，Palm 80亿模型，在那一点上。
你可以消除它，它会高一点，但在那一点上它还不是一个显著的结果。
所以很难说，例如，在这个特定的任务中，效果是什么。
在Zoom上有一个问题。
Palm有两个不同版本吗？
如果没有，为什么有两行？
哦，我认为这里的两行，一行可能是三次试验，然后一行是零次试验，之类的。
所以它只是指我们使用语言模型的方式，无论是带有样本还是没有样本。
太好了。
我将在这里谈论一个小的消融，显示了这一点。
这是对一个玩具任务的消融实验，基本上语言模型必须知道，在英语中，你必须用复数动词来搭配复数主语，用单数动词来搭配单数主语。
我们这里的做法基本上是从头开始训练这些小型 BERT 模型，然后我们保留了一部分数据，固定了训练数据集中某些动词的频率，这基本上是在说，看到某个动词的频率更高会有什么影响？
在这个图中，X 轴是动词的频率，Y 轴是错误率。
你基本上会看到，如果你有更多的领域内数据，所以如果模型更多次地看到这个动词，它会完成任务得更好。
这是一个高质量数据的例子，或者说是对你正在评估的任务更加相关的数据，即使你固定了计算量、模型大小和其他数据，也会产生很大的影响。
是的。
Zoom 上的问题。
有人问，现在是否有一种方法可以通过较小的模型来提取较大教师模型的收敛能力？
是的，我想是的。
较大的教师模型基本上可以用来生成数据。
然后，如果你对较小的模型进行微调，很可能你将能够使较小的模型具备这种能力。
我也会谈一下这个例子。
让我看看。
哦，实际上，那就是下一张幻灯片。
所以一旦你知道你想要什么行为，较小的模型就可以被诱导出来。
例如，这里是指导GPT的示例，这是指导GPT论文中的一个图。
基本上，这里的期望行为就像是遵循指导。
你可以看到有多个模型。
所以在左边，你有这些使用RLHF训练的小模型。
它们实际上比使用较弱技术训练的较大模型具有更好的性能。
所以基本上，关键是，如果你知道你想要某种行为，而且你之前在一个较大的模型中以自然的方式看到了这种行为，你可以找到一种方式，专门对这种行为进行微调，并在一个较小的模型中诱导出这种行为。
我想其中一个限制是，除非你知道所有你想要的行为，否则你无法真正获得这种自然的涌现行为。
是的，这里另一个讨论点是，关于涌现的正确x轴是什么。
现在，我们主要谈论模型参数和训练时的浮点运算。
但我猜，如果你问DeepMind的人他们是如何看待这个问题的，你会得到这样的论点，即模型参数和训练时的浮点运算实际上只是衡量模型好坏的一个代理。
而模型的好坏实际上可以通过困惑度或者它在一些开发集上的表现来衡量，比如Wikitex 103。
所以基本上，你也可以用困惑度来衡量出现。
这就是Wikitex的困惑度。
然后你可以看到在一个下游任务中，随着困惑度的提高，有一个阈值，你能够在下游任务上做得更好。
至少目前来看，困惑度和训练计算之间有很强的相关性。
所以你可以看到这两条线相当相似。
基本上，我认为在未来，如果我们有更好的模型，它们要小得多，能够更好地处理数据和算法，那么也许Wikitex的困惑度可以展现出不同于使用其他指标的情况。
是的，请继续。
所以Wikitex基本上是维基百科的一个子集。
然后困惑度是衡量你能够在数据集中预测下一个单词的能力。
所以基本上，如果你在模拟下一个词的过程中在这个特定的评估集上表现得很好，那就是衡量你对语言理解程度的一种方式。
这有道理吗？
哦，这就像一个被保留的测试集。
而我认为关于出现的最后一件事情是相当令人兴奋的，不仅仅是我们所谈论的技术出现，还有人工智能社区在扩展和使用语言模型方面的社会变革。
以下是一些例子，说明扩大语言模型的规模使你能够在这种几乎不经训练的情况下击败通常在成千上万个例子上进行微调的任务特定的语言模型。
所以基本上，绿线是通过微调实现的先前的技术水平。
然后蓝点基本上显示，如果你拿一个预训练的语言模型并进行几次提示，这意味着语言模型没有被有意训练来执行任务，你通常可以通过继续扩大语言模型的规模来获得最先进的结果。
显然这里有一些局限性。
你不想仅仅通过扩大模型规模来获得最先进的结果，但我认为这在人们心中是一个相当大的变化，你实际上可以仅仅通过扩大语言模型的大小并进行提示来获得一些最好的结果。
来自Zoom的问题。
有人问，这难道不与两到三张幻灯片前的图表矛盾吗？
哪一个？
这个？
我不确定这个。
我们通常应该假设...
哦，他说是的。
好的。
他说，我们通常应该假设规模胜过微调吗？
是的，这是一个很好的问题。
所以这个图表是说，你进行微调，你可以做到...
好的，是的。
所以这取决于你的特定任务，但这个图表所说的是，针对某些任务，如果你目标明确，微调较小的模型可以做得很好，但对于更复杂的测试，通常仅通过扩大规模你可以做得更好。
所以有些任务属于这两个类别。
我不会说这是矛盾的。
我扩大了模型的规模，然后处理其他任务，如果它是一个非常狭窄的领域，或者大型语言模型可能没有在那种类型的数据上进行训练，那么你最好通过微调来做得更好。
好的，很好。
所以这里有一个小总结幻灯片。
基本上，新出现的能力只能在大型模型中观察到。
如果你试图通过查看小型模型的图表来预测它们的出现，那么你将无法做到这一点。
我对如何看待这个问题进行了一点反思。
所以新出现的能力实际上是如何看待那些没有故意内置到预训练中的新能力的框架。
而我认为这其中的潜台词非常重要，即你可以将其视为支持我们应该继续扩大语言模型规模的隐含论据，因为你会获得那些在其他情况下很难找到的能力。
这方面的背景非常重要，因为继续扩大这些模型的规模确实很昂贵。
甚至在一年前，很多人并不相信通过扩大语言模型的规模可以在某些任务上取得更好的效果。
如果你在工业领域工作，会发现在出现和许多生产任务之间存在有趣的紧张关系。
所以，出现是这个任务的一般现象，当你扩大模型时，它真的很昂贵，但是单个模型可以在通用人工智能的方向上执行许多任务。
对于许多生产任务，你有相反的情况，你知道是什么任务，例如，翻译成西班牙语。
然后，你对计算有这些限制，因为当你使用谷歌翻译时，例如，你不希望人们等待几秒钟才能得到翻译。
然后，你也碰巧有很多在域数据。
所以你有，例如，一百万对英文、西班牙文句子来训练。
这是相反的设置，你不太关心模型的出现。
你可以只在数据上训练一个非常小的模型，并且执行所有任务，而不必使用大量的计算。
最后一点是，我认为一个非常有前途的研究方向，如果有人对研究感兴趣，是工作于预测未来的出现能力。
最近我没有看到很多这方面的工作，可能因为我认为这太难了，例如。
你只能预测一个特定任务的出现，或者它出现的一种方式，可能不是非常通用。
所以我还没有看到关于这方面的太多工作，但我认为这是一个非常有前途的方向可以研究。
也许安塔皮克正在研究这个。
我不知道。
好的，很好。
在我继续思路链之前，有关这方面的任何问题吗？
是的，请问。
我们是否有任何理论基础来预测哪些参数最适合按比例缩放以获得属性？
显然，增加更多参数的位置有许多不同的选项。
比如，GPD，你可以在库存中增加更多，也可以在其他地方增加更多。
这主要是我们测试并找出哪些参数更好地进行比较结果的事情吗？
是的，我会说我们并没有非常有原则的方法来缩放这些架构。
我不是这方面的专家，但其中一些与你可以将多少参数适应到特定的TPU有关。
但总的来说，我认为你要按比例增加意图头和嵌入的数量。
但是，我认为这是一个尚未解决的研究问题。
因为你不能真正对这些预训练进行割离，你不能真正对这些预训练进行割离。
很难有任何有原则的方法来做这件事，除非一些负责的工程师说，好的，我认为这是正确的做法。
然后它有点起作用，你就跟着走。
你有没有关于这个趋势的渐近行为的任何指示？
如果你期望最终它会达到某个有限但非零损失的平台，或者它会一直降到零？
是的，这是个很好的问题。
你是指在困惑度上还是在某个特定任务上，还是一般上是关于下一个单词预测的？
嗯，这似乎也很一般，相当独立于任务，对吧？
就像是一种紧急的缩放。
但是如果你把参数的极限考虑进去，甚至从分析的角度来看，有没有一种方式可以达到那个状态？
是的，我毫无头绪。
我认为对于大多数这些任务来说，准确度是有限的，比如说 100%。
所以在那里有一些渐近线，但我猜你可能要问的更深层次的问题是，一个语言模型是否能完美地知道如何预测任何给定输入的下一个单词？
也许，我猜这方面也有一些限制...
如果我说一个句子，有两个可能的下一个词或者什么的，你可能无法完全猜到。
所以我认为有一些限制，但我认为我们远未达到那个限制，而且仍然有很多未解决的任务表明还有很大的潜力。
如果研究人员对研究出现感兴趣，哪个系列的不同大小的模型是公开可用的或最适合研究单位的？
是的，好问题。
所以我认为 OpenAI API 有很多语言模型，我们实际上经常使用，即使在 Google，也用来研究出现现象。
这是一种方法。
而且实际上很多这些模型目前是免费的。
它们受到速率限制，但它们是免费的。
所以我们也使用那个。
我认为也有更小的语言模型。
例如，有一个 UL2 模型，大约有 200 亿个参数，但我想你是对的。
存在这样一个挑战，即小型语言模型中，你不会看到很多这些出现的行为。
所以你必须要么训练…
是的，所以你要么现在使用 OpenAI API，要么等到人们训练更大的模型。
我猜这里还有布鲁姆，你们可能比公开可用的OPT模型了解得更多，但我还没有看到很多关于它们的实验。
所以我的问题是，在更低参数范围内是否存在可访问的新能力？
我觉得可能会有一些更好的，也许不像思维链那样，但有一些是...
是的，确实。
我想在论文中我们列出了大约几十种在80亿参数或600亿参数时会出现的新能力。
是的。
我们从Zoom那里收到了两个问题。
第一个问题是，您是否看到较大的技术公司之间在研究这些模型时的战略策略有系统性的差异，还是基本上每个人都采取相同的方法？
我不会说每个人都采取相同的方法。
我认为，例如，Anthropic采取了非常重视安全的方法，他们对新出现的能力非常感兴趣，因为可能会出现一些不受欢迎的新能力，他们会预测出这些情况。
我也不知道其他公司的情况，除了在谷歌。
所以我不能对此多说。
是的。
第二个问题，Lambda和ChatGPT这样的模型中甚至都没有出现的一些任务或能力的示例是什么？
哦，是的。
也许我会很快展示一下这个。
在某个地方有一个不错的列表。
嗯。
所以基本上我们做的是在BigBench中有大约200个任务，然后我们基本上将它们分类为随着GPT-3或Lambda的出现而平滑增加的、随着POM的出现而增加的，以及平坦的，即没有模型比随机更好的情况。
所以我认为如果你看看这些任务中的任何一个，它们应该还没有出现。
如果你能让它们出现，那将会很有趣。
抱歉？
我觉得ChatGPT做了20个问题。
哦，好的。
是的。
这不是一个超级...
我觉得这可能是几个月前的事了。
哦，抱歉。
是的。
是的。
哦，20个问题。
好的。
是的。
是的。
我觉得很酷的是你可以看到随着时间的推移，对吧？
最初，可能只有这些是出现的，但当POM出现时，你会看到还有几十个能力出现了。
然后我怀疑在一两年内，其中大多数将变得紧急，并且将需要硬件基准测试。
是的。
在Zoom上还有另一个问题。
为什么谷歌不像你在利润说明中说的那样采取更注重安全的方法？
有理由相信有害功能不会是紧急的吗？
是的。
我不想代表谷歌回答这个问题。
我只能谈论我的个人观点。
但我认为现实情况是，谷歌，即使你看谷歌进行的研究数量，可能不是非常具体地在大语言模型上，但是像我们进行的安全研究的数量，我认为是相当大的，如果你实际上看看发表的论文数量。
别引用我，但我认为这是正确的。
好的。
很好。
所以是的，我将谈论思维链提示。
基本上，思维链提示是一种使用大型语言模型进行推理，多步推理的方法。
而且，我想说看到很多谷歌的人在这方面工作，还看到首席执行官桑达在去年的谷歌IO发布活动上演示这一点，真的很令人兴奋。
基本上，这样做的动机是我们希望语言模型能够执行更复杂的任务，例如，我们知道语言模型可以执行简单的任务，比如情感分析或翻译，但是对于可能需要人类一分钟或更长时间完成的更复杂的任务呢？
而这里的目标基本上是用元数据来指导它们。
所以，例如，我们不仅要给出一个输入输出对，我们希望给出整个推理过程并让它们模仿。
基本上你可以看到，在标准提示中，你有一个问题然后是答案，然后你有一个问题，模型给出一个新的答案。
不幸的是，这是错误的。
然后通过思维链提示，你给模型一个问题，然后有点像你的老师会要求你展示你的工作方式。
你提供的思维链就是我们称之为的东西，或者基本上是一个推理路径，然后你给出最终的答案。
然后当模型看到这个未见过的问题时，现在它能够给出推理路径，然后给出正确的最终答案。
我们将这些提示添加到提示中的方式基本上是我们只是手动写几个然后将其添加到提示中。
所以让我展示一下它是如何工作的。
这就是 OpenAI API，基本上这是一种非连贯思维的方法。
所以基本上你会有问题的答案，问题的答案，问题的答案，然后新的问题是关于食堂有 23 个苹果，他们用了 20 个来做午餐，并购买了 6 个更健康的苹果，然后模型做错了。
唯一不同的是，思维链会在给出最终答案之前提供这些中间推理路径。
所以这里有一个路径，有一个推理链，还有另一个推理链。
然后现在这个模型对于这个看不见的问题给出了整个推理过程，这实际上使得模型能够正确地理解。
我会给出另一个快速的例子。
这个。
所以这里的任务就是只取单词的最后一个字母和比尔·盖茨，就像从比尔取 L 和从盖茨取 S，然后拼接它们，答案应该是 LS。
然后这里模型做错了，答案应该是 NK，说成了 SK。
然后如果你进行思维链推理，显然这对模型来说变得非常容易。
所以，你知道，它说比尔的最后一个字母是 L，盖茨的最后一个字母是 S，答案是 LS。
然后这里能够做到埃隆的姓氏的最后一个字母是M、N，马斯克的姓氏的最后一个字母是K，答案是NK。
所以，任何一个，这清楚吗？
对于这里发生的事情有任何问题吗？
好的。
所以基本上我们可以有这些类似的图，其中X轴是模型规模，Y轴是性能。
所以在左边，我们有这个数学单词问题基准，称为GSM-AK。
这基本上就像你在小学数学测试中看到的问题。
你可以看到蓝点是标准的，紫色星星是思维链。
基本上你会发现，如果你使用足够大的模型，思维链比标准提示做得要好得多。
它实际上击败了当时的微调技术水平的最新水平。
一个类似的例子是，在这个称为Strategy QA的基准上。
Strategy QA基本上是世界知识加常识推理基准。
所以问题会是，你能把篮球藏在沙猫的耳朵里吗？
然后模型会说，你知道，篮球大约是这个大小，沙猫的耳朵是那样的大小，所以它是放不下的。
然后在这个基准测试中，您还可以看到，我们可以通过使用足够大的随机模型的思维链轻松超越以前的最新技术。
所以我们使用的一种方式是，在一定的大型基准任务子集上评估一条思维链。
所以我们创建了一个称为大型基准难的子集，基本上是来自大型基准的 23 个具有挑战性的任务，在这些任务中，没有模型做得比平均人类评分者更好。
您提示模型的方式是，您会有一个任务描述，问题选项，思维链，然后是测试时间问题。
所以我会举几个示例，比如这里的任务。
一个例子是导航。
在这个任务中，语言模型基本上必须遵循这些。
所以问题是，如果您按照这些说明，您是否会回到起点？
向左转，向右转，走五步，走四步，转身，走九步。
然后，模型在跟随未来的示例后，基本上能够跟踪所有动作之后的状态。
然后最后它说，好的，我们到了最终答案了吗？
所以答案是，我们到了原来的位置吗？
我是说，如果是零零，答案就是是。
举个例子，这是另一个任务，这个任务对人类来说非常容易，基本上是单词排序。
就像有一个单词列表，Burley、Bailah，我就不念了。
基本上模型必须将它们按字母顺序排列。
在这里，模型可以遵循未来的例子。
所以你有这样一个相当复杂的思路，模型必须像排序每个子部分一样。
最后它得到了正确的最终答案。
这是在BigBanch子集上的结果摘要。
所以你看，我们有两个指标。
一个是所有这些任务的平均表现。
第二个是高于平均人类评分者的任务百分比。
平均人类评分者是67，最高人类评分者是94。
然后之前的结果，模型表现得很差。
像50那样。
这在某种程度上是由子集的构造决定的。
然后我们使用了Code Vinci O2，这是OpenAI模型之一。
实际上你可以通过OpenAI API免费使用这个模型。
基本上，如果你只是回答而没有思考链，那么你在27个任务中有5个任务是超过了平均人类评分者的。
但是如果你使用思考链提示，性能就会提高相当可观，你能够在大多数任务中超过平均人类水平。
下面只是对那些在红色中表现不如人类的任务进行可视化，而在蓝色中表现优于人类。
是的。
有两个问题。
这不是与RLHF至少在精神上相似吗？
这是什么相似？
我认为是思考链提示。
我不确定陈述是什么。
我认为是思考链。
是的，我认为我不会称其为相似。
就像思考链本质上是你采用预训练语言模型，然后使用包含中间路径的提示技术。
RLHF的工作方式是你有这些额外的数据，你想要在模型上进行微调。
你有一个偏好模型，它有点像预测某个输出有多好，这个输出被人类喜欢的可能性有多大？
然后RLHF，它的作用是微调语言模型，使其在偏好模型预测上表现良好。
所以基本上，它是将模型与人类的偏好对齐。
还有第二个问题吗？
好的。
Grace问，连贯思维是否可以包含在微调中，而不是有一个思想？
是的。
简短的回答是是的。
关于这件事情的复杂之处在于，你必须有像连贯思维的中间步骤。
而这些步骤相当复杂，收集这些数据并进行微调可能代价高昂。
最后一个问题是给大家的。
另一位学生问，你认为连贯思维和提示工程是否只是一个不需要的工具，对于更能理解承诺的大规模模型而言？
是的。
这是一个很好的问题。
基本上，问题就是提示工程的持续时间有多短暂？
我认为我们会找到答案的。
但一些初步的直觉是，对于容易描述的简单任务，也许是多选题，更大的模型可能更能抵御提示工程，并且你能做的事情也更少。 
但我认为随着语言模型变得更加强大，将它们用于更多具有挑战性的任务将变得更加正常。
在这些任务中，您将不得不明确指定您希望模型执行的确切任务，等等。
所以我认为在那方面仍然会有一些对提示工程的空间，至少在您的未来会有。
是的，请继续。
你知道这个思维链任务的表现如何，它涉及简单的数学，然后另一个基本上是对单词进行排序。
所以我的意思是，我认为那还好。
但是数学必须从它擅长的事情中提供这个思维链。
但那个模型是否也会在排序其他事物方面变得更好呢？
你是否必须提供像在更好地排序单词上的思维链呢？
是的，这是一个很好的问题。
所以对于一些任务，在预训练中您已经看到了类似的数据，即使思维链来自另一个任务，模型也可以表现得非常出色。
例如，像数学问题这样的任务，实际上您不真的需要一个数学思维链，因为模型已经知道如何做到这一点。
但对于像这样的任务，您可能实际上还没有看到过任何类似这个思维链的数据。
没有特定任务的示例，你可能不会在像这样的任务上表现得很好，除非你为其他示例手动编写它们。
是的。
我在想，就像没有人一样，你如何更好地理解模型？ 
还是说在它得到答案之前，试图给它更多的计算？
是的，很棒的问题。
我想我的动机只是在思考它，就像你说的，就好像一个人在试图解决这个数学问题时人脑中正在发生什么一样。
而且，如果你注意到，至少一些人会实际上用自然语言来思考。
所以，如果你只是思考一下，如果你非常注意你的思维过程，你实际上会注意到有时你会用语言来思考。
那么，语言模型也可以用语言来思考。
所以，这就是让语言模型这样做的动机。
而且我认为有一件事情做得不错的是，这种技术的发展实际上与Palm的发展 coincided。
所以，基本上拥有 Palm 模型使我们能够更好地完成许多任务，或者说更具挑战性的任务，使用思维链。
你是在谈论数据质量吗？
就像我们说的，重要的是像这种思维链条的例子的绝对数量，或者数据集的事实是主要的显著因素？
还是像那些例子的相对数量，就像负面例子一样重要，它们不是如何推理的好例子？
这些是否像好例子的绝对数量那样重要？
是的，好问题。
我想挑战性的地方是，我们实际上无法衡量训练集中有多少类似的例子，或者很难做到这一点。
我认为以前没有人这样做过。
所以这是一个开放性问题的一部分，就像为什么思维链条会起作用一样，因为你实际上在训练集中看不到类似的数据。
是的，我认为这是一个开放性问题，就像为什么它起作用一样。
是的，你有什么直觉？
我的意思是，你说，好吧，想想，你知道，有时候你用语言思考新事物，模型也应该这样做。
但是你到底是如何思考的，你对模型的直觉是什么？
我是说，就像某个特定任务中是否有一种转变，就像一些权重会更受模型关注？
你怎么看待这个？
是的，我不会像权重上发生了什么一样思考。
对我来说，让你在半秒钟内回答数学问题是不公平的，这基本上就是你在模型中所做的事情，当你不进行思维链时，对吧？
你基本上是在问这个具有挑战性的问题，而模型没有足够的计算能力一次解决它，立即给出下一个答案。
我认为我想的第二件事是，模型在预训练期间学会了一组组合技能。
也许它在预训练期间并没有真正学会如何执行这个特定的导航任务，但它学会了其他东西，对吧？
这有点像，好吧，如果你走了五步而你面对的是这个，也许你应该在这里加五步之类的，对吧？
它学会了如何进行模式匹配。
所以也许在未来的示例中，它可以将推理路径与问题联系起来。
所以模型可能会知道一些小技巧。
然后，也许如果你能以某种聪明的方式将它们结合在一起，那么你就可以让它们都解决更具挑战性的问题。
好的。
Ryan，我们还有多少时间？
好的。
50。
好的。
好的。
好的，很好。
是的。
好的，很好。
是的。
如果有任何问题，请随时提问。
是的，这里有另一个出现的例子。
基本上你可以看到这里有三个模型，instructubt，codex和palm。
蓝色的思维链，灰色的非思维链。
然后你可以看到你实际上必须有足够的模型规模才能使思维链很好地工作。
我猜直觉在于，如果你有一个非常小的模型，模型会不断重复自己，或者不说任何连贯的话，或者永远得不到最终的答案，这就是为什么对于小模型使用思维链并不是很有效的原因。
然后对于大模型来说，显然对于多步问题，使用思维链模型将能够以更高的准确度解决任务。
而思维链的另一个很酷的地方是，有一些任务在那里你根本不会得到新兴行为。
因此，新兴行为尚未被解锁，但您可以看到，如果使用思维链，您可以在较小的模型中解锁这种新兴性能。
这里举个例子，比如多步算术，就像，我不知道你是否会永远，也许我想说从来不会，但很难想象一个模型像是得到这个，这是一个问题，然后下一个标记就被破解。
这在一步内解决起来相当困难。
但是通过思维链，您可以通过模型输出这些中间推理步骤，从而在这个任务上获得50%的准确性。
这是需要对发生的情况有直觉的事情。
实际上，我知道transformer肯定可以一步完成加法，但它可以接收数字并进行进位。
然后就有了这个经验上的问题。
我理解它并不一定有很多空间来进行全面的批判。
我的问题是，我们如何区分呢？
也许有办法区分那些之间尚未出现的事物，因为根本没有空间。
或者有很多任务，它可能根本没有分配任何空间来专门执行那一个任务，与任务非常困难，即使你使用全部容量尝试执行它，也无法完成。
是的，这是一个很好的问题。
我认为似乎存在一些任务的子集，它与我们训练语言模型的方式不太匹配。
例如，在语言模型中，我们使用标记（tokens）。
如果你给它一个标记为四，它实际上不会取数字四。
它会得到一个 1,000 维度的嵌入。
或者如果你给它一个单词并要求它颠倒字母顺序。
这是一个超级简单的任务，但我们训练模型的方式实际上并不关注字母等细节。
因此，我认为有一些任务的子集，它们实际上并不适合我们训练transformers的方式。
我认为，如果你真的关心这些任务，可以通过编写代码之类的方式解决它们。
但是，我认为这并不是因为它太难而永远不会出现的东西。
是的。
我们在Zoom上有一个问题。
另外，顺便说一下，抱歉，我忘了提到。
有人问能否重复问题？
因为他们并不能总是听到答案。
哦，好的。
是的。
这是我的错。
所以有人问的问题是，你认为思维链对于非常先进的AI系统来说会是一种可行的可解释性技术吗？
他们提到了一个叫做卡梅隆·兰德曼（Cameron Landman）的项目进行的一些研究，名为外部化推理监督。
它会成为高级AI的一种可行的解释性技术吗？
是的。
我应该重复一遍吗？
是的，是的，是的。
抱歉，请。
哦，所以问题是，思维链对于AI来说可以成为一种可行的解释性技术吗？
我认为不能保证思维链就是模型实际得出最终答案的方式。
但通常你可以用它来调试为什么模型没有得到这个问题的正确答案，或者我们可以在思维链中做些什么来帮助模型得到正确答案？
我并没有阅读过提到的人类学论文，所以我实际上不知道答案。
好的。
我们这里还有另一个有趣的结果，那就是你实际上可以进行多语言的思维链提示。
所以基本上我们所做的是将这个数学问题的基准转化成了不同的语言。
然后我们提示模型用孟加拉语来做。
然后模型基本上必须用孟加拉语解决数学问题并给出最终答案。
我认为这个很酷的地方在于这个输入是高度不可能的。
孟加拉语是预训练数据的0.01%，而数学问题可能是其中一个更小的子集。
基本上有趣的是，模型实际上可以非常好地完成这些类型的问题，可能是令人惊讶的程度。
如果你在我向他们展示这个结果之前问他们，嗯，模型在斯瓦希里语的数学问题上能做得多好？
可能是10%。
但实际上，即使是斯瓦希里语或孟加拉语或泰卢固语和泰语等少数代表性较低的语言，模型也能做得出乎意料的好，尽管它们只占预训练数据的很小一部分。
是的。
我实际上是在谈论这个，并且我在这方面的大部分经验都是通过ChatGPT获得的，但是如果你用不同的语言问它问题，尽管它没有明确地在这些语言上进行过训练，它似乎已经获得了独立于语言的推理。
你可以进行推理。
其实有点有趣，有时候它看起来总是像是用英语推理，然后再翻译回另一种语言，因为它给出的答案有点像是你用英语推理然后翻译成另一种东西。
那么你认为学习一种语言的结构和学习推理能力在大型语言模型中是分开的，还是它本质上会在该语言中学习思维链，以该语言的结构，就像思维在该语言中的运作方式？
是的，这是一个很棒的问题。
我不确定如何衡量这一点，但我肯定考虑过。
我认为根据这些结果，你可能没有任何斯瓦希里语的数学问题供模型学习。
而且我认为肯定有一些与语言无关的东西正在发生，模型学习推理与语言有点独立，然后如果需要，它可以用不同的语言表达出来。
但我不认为有人，我不认为我们知道答案。
所以基本上经常出现的一个问题是，为什么扩展规模会改善思维链呢？
从这个角度来看，我们可以拿一个像Palm 62B这样的小型模型，看看从扩展到540个参数时修正了哪些类型的错误。
你可以看到，对于我们提出的这三类情况，有些或全部都被修正了。
因此，扩展似乎对改善较小模型的不同类型错误有一种普遍的影响。
然后这里是用不同方式表达的同样一张粗略示意图。
所以基本上你有一些可以用标准提示完成的任务，用蓝色表示，然后思维链提示的目标是增加我们可以完成的任务集。
例如，现在粉色显示的包括数学文字问题、符号推理和具有挑战性的常识推理。
是的。
还有一个问题。
您是否进行过任何编译以确定这些贡献中有多少是因为您在使用更长的提示时进行了更多计算？
就像您通过模型执行多个任务，创建多个嵌入来调整模型所关注的内容一样。
您尝试过相同标记长度的非思维链提示吗？
是的，我们尝试过使用类似XXXX之类的东西，但没有成功。
所以我认为问题不仅仅在于计算。
我认为这与语言对模型的引导作为推理的一部分有关。
我明白了。
你有没有尝试更详细地描述问题，而不是进行思维链的过程？
我知道这是一个非常新奇的问题。
我只是对这个问题非常好奇，所以这是一个非常有趣的性质，我很想知道它究竟是如何适应的。
是的。
你是说像用三种不同的方式描述问题，然后看看那样是否有效 - 是的。
就像更详细地描述问题，而不是明确地一步一步地进行，看看那样是否有效 - 是的。
我还没有尝试过，但如果那样能奏效，我会感到惊讶。
是的。
等一下，我能在此基础上提个问题吗？
你有没有尝试让它输出答案，然后解释推理过程，结果怎么样 - 是的。
这样做效果并不好。
好的。
是的。
但这也取决于任务。
所以 - 是的。
似乎是这样的。
是的。
是的。
对于像推理这样的反应，可能会像其中的任何一种，但这是你的计算。
有点像把最糟糕的答案联合起来。
就像在某种程度上，你知道，在空白的计算中，思维的链条是一种非常有结构的东西。
就好像如果相同的结构被保留，但我们做了一些更随机的事情。
是的。
你可以试试。
如果它奏效了，我会感到惊讶的。
我认为输出令牌对于模型来说非常重要。
是的。
好的。
那么我们要按时完成。
好的，很好。
所以我认为最后一部分是关于思维链的一个很酷的技巧。
基本上，人们通常会生成一个思维链，然后他们会得到最终答案。
但是有这个很好的技巧叫做自相似性，你可以使用模型进行温度抽样来生成一堆不同的推理路径和最终答案。
然后，如果你只是对最终答案进行多数投票，这最终会显著提高性能。 
例如，在这里，你可以看到在 GSM 8K 上，这就像是数学问题数据集，性能从 56 提高到了 74，这是一个相当大的提高。
是的。
这里，我们平均了多少个自一致性？
哦，我想是 40 个。
所以这增加了推理时间的计算成本。
但是，它确实大大提高了性能。
你可能要回答这个问题，但我想知道需要抽取多少样本或多少链才能得到显著的...
链数平均化与性能增益之间的权衡是什么？
我想这取决于...
抱歉，问题是需要多少链才能获得性能增益？
我认为答案真的取决于数据集，但通常你可以用大约 16 个得到一些好的东西，我认为。
哦，抱歉。
我们有一个问题。
温度如何改变模型的工作方式？
哦，好的。
问题是温度如何改变模型的工作方式？
基本上，当您使用温度解码时，语言模型可以随机选择输出之一，而不是始终选择最高概率的下一个词。
所以基本上你会得到这些更随机的输出，但仍然基于语言模型所学到的，只是稍微更随机一点。
最后，自一致性似乎也是一种 emergent 能力。
我想部分原因是因为思维链是 emergent 的，因为如果不进行思维链，你就不会比随机表现更好。
但是，是的，您可以看到较大模型的自一致性的巨大变化。
太好了。
所以我时间快用完了。
让我简单地谈一下...
我只是稍微谈一下这个。
所以我认为除了纯粹扩展语言模型之外，这只是适用于行业人士的，我认为还有几个有趣的方向值得研究。
一个是更好地提示和表征语言模型的能力。
我认为现在我们只是在探索最佳提示语言模型的方法的边缘。
也有相当不错的应用工作。
听说你可以使用语言模型来培训心理治疗师，以帮助语言模型在这方面的应用。
我认为基准测试也是相当缺乏的，因为我认为我们很快就能解决基准测试。
例如，Palm在大概一年左右就在BigBench上击败了平均人类，BigBench问世不久。
所以我认为我们需要更多的基准测试，我认为这将是一个重要的贡献。
最后一个问题是我们如何拥有计算机高效的方法来使语言模型更好，以便使用它们的成本更低，更多的人能够使用它们。
好的。
那我就到这里。
如果你有任何反馈，请随时给我发邮件。
如果你对Google感兴趣，随时给我发邮件。
谢谢。
