花了大约两个月的时间，用数千个 CPU 和数千兆字节的内存训练这些机器人，有时候。
但是当他们真正面对人类时，他们几乎是立刻行动的。
它只是一个查找表。
而人类，当他们陷入困境时，他们不会立即行动，他们会思考。
他们会坐在那里，他们会考虑五秒钟，也许如果这是一个非常困难的决定，他们会考虑五分钟。
很明显，这使他们能够提出更好的策略。
所以我想调查我们机器人的这种行为。
就像如果我们能够将这个添加到我们的机器人中，这会产生多大的差异？
而不是立即行动，而是花一些时间计算出代理人所处位置的更好策略的能力。
这就是我发现的。
所以在这里 X 轴上，我们有像是桶的数量，你可以将这看作是你模型中的参数数量。
而在 Y 轴上，我们有距离纳什均衡的距离。
所以这基本上就是你会输给最坏情况对手的程度。
所以这个数字越低，你的扑克机器人就越好。
你可以看到，随着参数数量的增加，性能会提高。
当你将参数数量增加约100倍时，你的可利用性会减少约一半。
确实，你得到了一个更好的扑克机器人。
但是你可以看到这里的蓝线是如果你没有搜索，橙色线是如果你添加了搜索。
你可以看到，只是添加搜索，增加了思考一会儿的能力，提高了这些模型的性能。
它减少了可利用性，从纳什均衡的距离约为七倍。如果你延伸蓝线并看看你需要多少参数才能与添加搜索相媲美，答案是你需要将你的模型扩展约100,000倍。
所以当我看到这个时，我觉得相当震惊。
我的意思是，在我的博士学位课程中，前三年，前三四年，我设法将这些模型扩展了大约100倍。
我为此感到自豪。
我觉得这是一个相当令人印象深刻的结果。
但是这个图表向我展示的是，只是添加搜索就相当于将事情扩展约100,000倍。
所以，到目前为止，我所有的先前研究都只是与添加搜索相比的一个脚注。
所以当我看到这个时，变得很明显，这就是击败顶级扑克高手的答案。
因此，在接下来的一年里，基本上是不间断地，我一直在致力于扩展搜索。
现在有一个自然而然的问题，那就是为什么之前没有考虑过这个呢？
有几个因素。
首先，我应该说，像搜索这样的东西以前已经在扑克中考虑过了。
实际上，说实话，像在国际象棋和围棋中使用搜索一样，为什么在扑克中不考虑搜索呢，这是非常自然的。
有几个原因。
首先是文化上的，就像扑克研究源于博弈论和强化学习一样。
因此，它与那些致力于国际象棋和围棋的人不是来自同一个背景。
当你扩展搜索，扩展测试时间的计算时，会使所有的实验变得更加昂贵，也更加不愉快。
而且还有一些激励结构。
我是说，人们总是在考虑赢得下一届年度计算机扑克比赛，而ACPC限制了你在测试时间可以使用的资源。
在ACPC中，搜索实际上并不可能有效地进行。
我认为最大的因素是人们根本就没想到它会带来如此巨大的差异。
我认为将搜索等事物视为可能会产生10倍差异是合理的。
你可能不会认为它会产生100,000倍的差异。
因此，有一些人在研究这个问题，但这并不是很多人研究的重点。
总之，我们专注于扩展搜索，这导致了2017年的大脑与AI比赛，我们再次将我们的机器人与四位顶级扑克专家对战，进行了120,000手扑克游戏，奖金为20万美元。
这一次，机器人以每100手15大盲赢得了比赛，而不是每100手九大盲。
这是一个压倒性的胜利。
每个人都被机器人击败，统计学上具有四个标准偏差的显著性。
我们在2019年进行了一场六人扑克AI比赛。
这里的重大区别在于，我们想出了如何进行有限深度搜索。
因此，在2017年的机器人之前，它总是必须搜索到游戏结束。
在这里，它只需要提前搜索几步，然后就可以停止搜索。
所以这一次，再次以统计显著性赢得了胜利。
关于这个机器人真正令人惊讶的是，尽管它是一个规模更大的游戏，六人扑克机器人Pluribus，在云计算资源上的训练成本不到150美元。
并且在推理时运行在28个CPU核心上。
没有GPU。
所以我认为这表明这确实是一种算法上的改进。
我的意思是，如果人们知道如何做到这一点，20年前就可以做到了。
我认为这也展示了搜索的威力。
如果你能够找出如何在测试时扩展计算，它确实可以产生巨大的影响，并大大降低培训成本。
好的，无论如何，是的。
所以我还想说，这不仅限于扑克。
如果你看围棋，你会看到类似的模式。
这是AlphaGo Zero论文中的一个图。
在X轴上，我们有不同版本的AlphaGo。
在Y轴上，我们有ELO评分，这是一种比较不同机器人的方式，但也是一种将机器人与人类进行比较的方式。
你可以看到，好吧，所以超人类表现在大约3600的ELO。
你可以看到AlphaGo Li，2016年与李世石对弈的版本，就在超人类表现的线上。
AlphaGo Zero，AlphaGo最强大的版本，大约是5200 ELO。
但如果去掉测试时的搜索，如果你只按照策略网络下棋，而不在AlphaGo Zero测试时进行任何多学院的研究，那么ELO评分会降至约3000，远低于人类表现的X。
所以，这表明如果在测试时去掉多学院的研究，AlphaGo Zero并不是超人类的。
事实上，没有人制作出不以某种形式使用搜索的超人类围棋机器人。
没有人制作出能够击败顶尖人类的原始神经网络。
我还要说，这仅仅是指在测试时去掉搜索。
我甚至没有谈论去掉它的训练时间。
如果你把它从训练时间中剔除，它甚至无法起步。
现在有一个问题，就是，好吧，当然，你可以简单地扩大模型规模，扩大训练量，最终超过超人类的表现，并且如果添加搜索，能够达到相同的表现。
这是真的，是的。
如果你扩大模型，扩大训练，那么最终你将能够与搜索匹敌。
但是有一个问题，就是，你需要扩大多少呢？
现在，一个粗略的经验法则是，为了将你的ELO评分提高约120分，你要么必须将模型大小和训练量加倍，要么必须将测试时间搜索量加倍。
因此，如果你看看大约2000个ELO点的差距，并计算出你需要的答案，那么为了将原始策略网络从3000个ELO提高到5,200个ELO，你需要将你的模型和训练量扩大约100,000倍。
好的，那么这为什么重要呢？
我认为你看看今天大型语言模型和transformer的发展，你会看到类似的情况。
我的意思是，它们变得非常庞大。
有一个问题，就是，我所说的搜索是什么意思呢？
有特定类型的搜索，比如多学院研究，能够提前计划你要做的事情，而不仅仅是根据预先计算的策略立即行动。
但是更广义地说，我所说的搜索是指扩大计算量以获得更好的性能。
我认为这就是搜索正在添加的真正价值。
不是只是根据您预先计算的方式行事，将您所有的计算，所有您的计算提前加载，然后在推理时基本上立即行动，如果您有五分钟而不是100毫秒来输出一个动作，您能否获得更好的解决方案？
所以，是的，我认为你看，对不起，有一个问题。
一个带有搜索电路的transformer是否算作搜索，还是你指的是手工设计的搜索算法？
我不想深陷于像如何做这个的细节中。
因为答案是没有人真正知道。
没有人真正有一种通用的搜索方法，我们在成功进行搜索的所有领域，比如扑克和围棋，都是以相当特定于领域的方式开始的。
围棋使用一种叫做多学科研究的算法。
是的，你可以把束搜索看作是搜索的一种简单形式，但未来似乎应该有更好的方式。
总之，我想说的是，你看一下今天大型语言模型的训练方式，你会看到数百万美元被投入到预训练中。
我不会感到惊讶，如果我们看到一个花费一亿美元来训练的大型语言模型。
甚至可能会达到十亿美元，但推理成本仍然会非常小。
所以有一个问题，就是，如果您能够扩展推理成本，您是否可以取得更大的进展呢？
也许这样可以分摊一些您的培训成本。
好的，有一个名为理查德·萨顿的苦涩教训的讲座，他说可以学到的最重要的教训。
这是一篇非常好的文章，我建议阅读一下。
但其中一个重要的要点是，他说在超过70年的人工智能研究中可以学到的最重要的教训是，利用计算的通用方法最终是最有效的。
两种似乎可以任意扩展的方法是搜索和学习。
现在，我认为我们在泛化学习方面做得很好，抱歉，是在泛化搜索方面做得很好。
而且，我认为在搜索方面仍然有改进的空间。
是的，下一个目标确实是关于普适性。
我们能否开发一种真正通用的推理计算规模方法，而不仅仅是像蒙特卡洛树搜索这样特定于更好领域的方法，也比像思维链这样的方法更好。
这将是什么样子呢？你在测试时有更高的计算能力，但你拥有更强大的模型。
而且我认为对于某些领域来说，这种权衡是值得的。
比如，如果你考虑一下为了证明黎曼猜想我们愿意付出什么样的推理成本，我认为我们愿意付出很多，或者为了新的挽救生命的药物，我们愿意付出什么样的成本？
我认为我们愿意付出很多。
所以我认为这里有一个机会。
好吧，无论如何，这就是我的序言。
在我继续谈论西塞罗之前，有关这个的任何问题吗？
顺便说一下，我谈论这个的原因是因为它将影响我们对西塞罗问题的解决方法，我认为这与许多其他研究人员可能采取的方法相当不同。
有人问，你能举个搜索的例子吗？
嗯，蒙特卡洛树搜索就是搜索的一种形式。
你也可以考虑广度优先搜索、深度优先搜索，诸如此类的方法。
它们都是搜索。
我还要提到的是思维链，它与搜索类似，允许模型在测试时利用额外的计算以获得更好的性能。
但我认为那是你想要的主要内容，即在测试时利用额外的计算能力。
搜索的内容是什么，你搜索的空间是什么？
同样，在象棋这样的游戏中，搜索的内容是不同的棋盘位置，但你也可以想象搜索不同的句子之类的内容。
在这方面有很多的灵活性。
好的，现在我想讲一下 Cicero。
首先，当涉及到 Cicero 时，我应该说的第一件事是，这是一个团队的努力。
这实际上是在这个项目上工作的一个伟大之处，有一个如此多才多艺的团队，涵盖强化学习、规划、博弈论、自然语言处理等各个领域的专家，他们共同努力。
如果没有每个人的努力，这是不可能的。
外交的动机实际上来自于2019年。
我们正在关注当时发生的所有突破。
我认为一个很好的例子是2012年发布的这个XKCD漫画，显示了不同类别的游戏：已解决的游戏，计算机可以击败顶级人类的游戏，计算机仍然输给顶级人类的游戏，以及计算机可能永远无法超越顶级人类的游戏。
在这个类别中，计算机仍然输给顶级人类，有四个游戏，分别是围棋、Arima、扑克和星际争霸。
实际上，在2015年，我的一位同事David Wu，第一次在Arima中战胜了顶级人类。
在2016年，AlphaGo击败了Lisa Dolan在围棋中的表现。
在2017年，我们完成了我刚刚描述的工作，战胜了扑克中的顶级人类。
而在2019年，AlphaStar战胜了星际争霸中的专家人类。
这显示了战略推理在过去几年中取得的惊人进展。
与此同时，2019年我们还推出了GPT-2。
它表明自然语言处理中的语言模型进展比很多人，包括我们在内，预期的要快得多。
所以我们在思考，在六名玩家扑克牌项目之后，我正在与同事讨论，我们接下来应该做什么？
我们围绕着不同的领域进行讨论，考虑到人工智能取得的巨大进步，我们想挑选一些真正雄心勃勃的东西，一些我们认为不仅仅可以通过扩展现有方法来解决的东西，你真的需要创新来解决。
我们选择了外交，因为我们认为这将是最难制作AI的游戏。
那么，什么是外交呢？
外交是一种自然语言策略游戏。
它发生在第一次世界大战之前。
你扮演欧洲七个大国之一，包括英国、法国、德国、奥地利、俄罗斯和土耳其。
你的目标是控制大部分地图。
实际上，这种情况很少发生。
如果你控制了大部分地图，那么你就赢了。
实际上，没有人最终获胜。
因此，你的得分与你控制的地图百分比成比例。
现在，关于外交的真正有趣之处在于，它是一种自然语言谈判游戏。
所以你有这些对话，就像你在德国和英格兰之间看到的这样，在他们做出动作之前，他们可能会互相沟通。
所以你可以让德国问，比如说，想要支持瑞典吗？
英格兰说，让我考虑一下，等等。
所以这是一个在1950年代开发的流行策略游戏。
事实上，这是肯尼迪和基辛格最喜欢的游戏。
就像我说的，每一轮都涉及复杂的私人自然语言谈判。
我想要澄清，这不是像你在卡坦岛这样的游戏中看到的谈判，比如说，这更像是《生存者》，如果你看过这个电视节目的话。
你会就想要建立的联盟进行讨论，讨论你想要在当前赛道上执行的具体战术。
还有，你知道，更长期的战略方面，比如说，我们从这里往哪里走，如何分配资源等等。
现在，游戏的运作方式是，你有这些持续五到十五分钟的谈判，取决于游戏的版本，然后在每个轮次上，所有这些谈判都是私下进行的，否则就不是谈判。
另外，我认为你没有静音。
好的，谢谢。
然后，在谈判期结束后，每个人都会同时写下他们的行动。
所以一个玩家可能会承诺给你一些东西，比如，我这一回合会支持你进入这个领地。
但当人们实际上写下他们的行动时，他们可能不会这样做。
因此，只有当所有行动同时被揭示时，你才会发现他们是否言而有信。
因此，联盟和建立信任至关重要，信任某人会履行他们的承诺的能力。
这真的是这个游戏的核心。
而说服别人相信你会履行自己的承诺的能力，才是这个游戏的真正核心。
因此，外交长期以来一直被认为是AI的一个挑战问题。
这项研究始于上世纪80年代，但真正开始加速发展是在2019年，当时DeepMind、我们自己、Mila和其他地方的研究人员开始研究这个问题。
现在，大部分研究，事实上绝大部分研究都集中在游戏的非语言版本上，这被视为通往完全自然语言版本的一个过渡阶段。
虽然我们决定从一开始专注于游戏的完整自然语言版本。
所以为了让你了解这些谈判和对话是什么样子，这里有一个例子。
所以在这里，英格兰，你可以看到他们将他们的舰队从挪威移动到了圣彼得堡，这占领了俄罗斯的领土。
这是那次行动之后棋盘状态的样子。
现在奥地利和俄罗斯之间有这样一段对话。
奥地利说，北方发生了什么事？
俄罗斯说，英格兰背叛了。
我担心我的末日可能已经近了，我的朋友。
奥地利说，是的，那很艰难。
你在那边还好吗？
俄罗斯说，我希望如此。
英格兰似乎还想继续合作。
奥地利说，你能和德国达成交易吗？
所以玩家们现在正在讨论应该与其他玩家讨论什么。
俄罗斯说，好主意。
然后奥地利说，只要你能保卫塞瓦斯托波尔，你就会没事的。
所以塞瓦斯托波尔是南部的这个领土。
你可以看到土耳其在黑海有一支舰队和一支军队，紧挨着塞瓦斯托波尔的亚美尼亚。
所以他们可能在下一回合攻击那个领土。
奥地利说，你能支持乌克兰和罗马尼亚守住塞瓦斯托波尔吗？
我会支持 Holdermania。
俄罗斯说，是的，我已经在这样做了。
奥地利说，太棒了。
希望我们能帮助你重新站起来。
这是外交游戏中你会看到的对话的一种示例。
在这次对话中，奥地利实际上是我们的机器人 Cicero。
这就给你一种对代理人对话复杂性的感觉。
好的，我会跳过这部分，好的。
所以我想我会进入这个。
我不想占用太多时间。
外交变得有趣的真正原因是支持是关键。
在这里，例如，布达佩斯和华沙，红色和紫色的单位都试图进入加利西亚。
因为是一对一，它们都会弹回，都不会进入该领土。
在中间的面板上，你可以看到维也纳支持布达佩斯进入加利西亚。
因此，现在是两对一，那个红色单位确实会进入加利西亚。
关于外交的真正有趣之处在于，支持你的不仅仅是你自己的单位。
还可能是另一个玩家的单位。
因此，例如，绿色玩家可以支持红色玩家进入加利西亚，然后那个红色单位仍然会进入那里。
所以支持真的是游戏的核心，是关于支持的协商。
因此，由于这个原因，外交被誉为破坏友谊的游戏。
与某人建立联盟三四个小时，然后被他们背叛，基本上破坏了你的游戏，这真的很困难。
但是，如果你和外交游戏的专家交谈，他们会有不同的看法。
他们说，外交最终是在一个鼓励你不要相信任何人的环境中建立信任。
这就是为什么我们决定着手制作这款游戏。
我们能否制作一个能够在鼓励玩家不要相信任何人的环境中与玩家建立信任的人工智能？
机器人能否诚实地传达它将要做某事，并评估另一个人在说他们将要做某事时是否诚实。
好的，那么为什么是外交呢？
这是强化学习和规划以及自然语言的一个很好的交汇点。
有两个视角可以解释为什么外交是一个非常有趣的领域。
一个是多智能体的视角。
所以在这里，所有先前的游戏人工智能结果，如国际象棋、围棋、扑克，这些都是在纯粹的零和、两个玩家的零和领域中取得的。
在这些领域中，自我对弈有保证会收敛到最优解。
基本上这意味着你可以让机器人完全从零开始进行对战，没有人类数据。
通过不断地与自己对弈，它最终会收敛到这个无敌最优解，被称为极小极大均衡。
但是这个结果只在两个玩家的零和游戏中成立。
整个范式只在两个玩家的零和游戏中成立。
当你转向涉及合作的领域时，除了竞争之外，成功还需要理解人类行为和惯例。
你不能再把其他玩家当作机器对待。
你必须把他们当作人类对待。
你必须建模人类的非理性，人类的次优性。
其中一个例子实际上是语言。
你可以想象一下，如果你要从头开始完全训练一个在外交游戏中的机器人，就像是游戏的完整自然语言版本，没有理由让机器人学会用英语交流。
它会学会用一些奇怪的机器人语言进行交流。
然后当你把它放入一个与六个人类玩家的游戏中时，它将无法与他们合作。
所以我们必须找到一种方法来融入人类数据，并能够学习人类在这个游戏中的行为方式以取得成功。
好的，还有自然语言处理的角度，目前的语言模型基本上只是在模仿类似人类的文本。
现在有一些关于RLHF等方面的进展，但那仍然不是人类交流的真实方式。
他们交流时有一个意图在心中。
他们构思出这个意图，然后以传达这个意图为目标进行交流。
他们明白其他人也在尝试做同样的事情。
所以有一个问题，我们能否超越闲聊，转向有意义的对话？
因此，Cicero是一个用于外交的人工智能代理，它整合了高水平的战略游戏和开放领域的对话。
我们利用与网站webdiplomacy.net合作获得的50,000场外交人类游戏。
所以我们让Cicero参加了在线外交联赛，让我们直接看结果。
Cicero在与82位独特玩家进行的40场比赛中未被检测为AI代理。
有一位玩家事后提到他们曾开玩笑说我们是一个机器人，但他们并没有真的继续追究，也没有其他人跟进。
后来他们指责另一个人也是机器人。
所以我们不确定该指控要多么严肃。
但我认为可以肯定的是，它在所有40场比赛中都没有被检测为机器人。
事实上，当我们告诉玩家们它一直是一个机器人时，这就是我们得到的回应。
人们感到非常惊讶，但幸运的是，没有人因此而生气，但他们非常惊讶地发现有一个机器人一直在与他们玩这个游戏。
所以就结果而言，Cicero在玩家中排名前10％，这是一个高方差的游戏。
如果您看一下玩了五场或更多比赛的玩家，它排在19名选手中的第二名，并且比平均人类得分高出一倍以上。
所以我会将这描述为人类表现的一个强大水平。
我不会说这是超人类的，但目前确实是一个相当强大的结果。
现在让我给你介绍一下Cicero的工作方式。
我们向模型输入的是棋盘状态和最近的动作历史，显示在这里左上角，还有它与所有玩家迄今为止进行的对话。
这将被输入到一个对话条件动作模型中，该模型将预测Cicero认为所有玩家在这一轮将要做什么，以及他们认为我们将在这一轮做什么。
这导致了我们所称之为锚定策略，然后用于规划。
现在在这里进行规划，这再次是我们利用测试时的额外计算资源以获得更好性能的部分。
因此，我们基本上采用这些每个人将要做什么的初始预测，也就是所谓的锚定策略，然后通过这个被称为"pickle"的规划过程改进这些预测，基本上我们考虑到玩家会选择具有更高期望值的行动的可能性更高。
我们实际上是在所有玩家面前添加了这种理性先验，假设他们不会像模型可能暗示的那样经常犯错误，并且他们会选择比初始模型建议的更明智的行动的概率更高。
我们发现，这实际上比仅仅依赖原始神经网络本身更好地预测了所有玩家将会做什么。
这给了我们实际在游戏中所采取的行动，它还给了我们所谓的意图。
因此，意图是我们自己的行动和我们正在交谈的对话伙伴的行动。
现在我们有了这个对话条件。
所以我们有了这个在这些意图上有条件的对话模型。
因此，这些意图连同棋盘状态、行动历史以及我们迄今为止进行的对话一起被馈送到对话模型中。
然后，对话模型将生成根据这些意图有条件的候选消息。
这些候选消息经过一系列的过滤器，过滤掉无意义的、接地气的问题，以及低期望值的消息。
最终，我们得到一条消息，发送给我们的对话伙伴。
现在，每次我们发送或接收消息，我们都会重复这整个过程。
所以，在 Cicero 中有很多实质性的东西，我将尽量谈论贡献。
我可能会快速浏览一下，以便有时间问问题。
第一个是一个可控对话模型，它以游戏状态和发言者和接收者的一组预期动作为条件。
所以我们有一个问题。
模型的动作空间是什么？
动作预测模型的动作空间就像是游戏中你可以采取的所有动作，玩家可以在游戏中采取的动作。
对于对话模型，它就像是你可以发送的消息。
明白了。
那太棒了。
好的，所以我们尝试了一个意图模型，它预测人们在真实的对话回合结束时会采取什么行动。
基本上，我们试图预测人们在传达特定消息时打算做什么。
然后，我们使用这个自动注释数据集，基本上记录了我们预计人们在发送那条消息时的意图。
我们尽可能地从数据集中过滤出谎言，以便数据集中的文本带有真实的意图。
然后在游戏过程中，西塞罗将对话模型的条件设置为它打算采取的真实意图。
然后目标是，希望它生成与该意图一致的消息。
然后将其输入到其他所有内容中。
抱歉，我们通过规划生成的意图被输入到对话模型中。
所以举个例子，这给了我们一种通过一组意图来控制对话模型的方法。
就像这里，我们是西塞罗的英格兰，他们的行动是移动到比利时等其他事情。
所以如果我们将这个意图输入到对话模型中，那么可能生成的消息是英格兰对法国说，你介意支持我吗？
你介意支持埃迪到比利时吗？
另一方面，假设西塞罗的行动是支持法国进入比利时。
那么如果你将其输入到对话模型中，那么生成的消息可能会说，如果你想我支持你到比利时，请告诉我，否则我可能会戳荷兰。
现在，我们发现以这种方式使对话模型对这些意图进行条件约束，这使得模型更易控制，同时也导致了更高质量的对话，减少了无意义的内容。
所以我们发现，这导致了更符合状态、更符合计划的对话，质量更高，困惑度更低。
我认为这种情况的论据、推理是，我们有点像解除了对话模型的负担，让它不必想出一个好的策略。
我们让对话模型做它最擅长的事情，专注于它最擅长的事情，也就是对话。
而且我们解除了游戏战略组成部分对它的压力，因为我们将战略输入到对话模型中。
好的，这是一个主要贡献，这个可控对话模型，它对计划进行了条件约束。
第二个是一个考虑到对话和人类行为的规划引擎。
所以，好的，我提到了很多以前在游戏上的工作都是在两个玩家零和设置中使用自我对弈。
现在，纯自我对弈的问题在于它可以学习到强大的策略，但它不遵循人类的惯例，并且不能考虑到对话。
如果你只是进行自我对弈，它将会忽略人类数据和人类的游戏方式。
所以这是一个极端。
另一个极端是只对人类数据进行监督学习，创建出人类游戏方式的模型，然后用这些模仿人类的数据进行训练。
如果你这样做，你最终会得到一个与对话和人类约定一致的机器人，但它的强大程度仅限于训练数据。
我们发现它实际上很容易通过对抗式对话来操纵。
例如，你可以发送消息给它，说谢谢你同意支持我在巴黎的事情。
它会认为，嗯，我在训练数据中只看到过当我同意支持某人在巴黎时才会出现这样的消息。
所以我猜我这学期要支持他们在巴黎，尽管这对机器人来说可能是一个糟糕的举动。
因此，我提出了这个算法，叫做 Pickle，它在这两个极端之间找到了一种比较理想的平衡。
Pickle 的工作原理基本上是试图进行软对弈，但又规范化以坚持人类模仿策略。
所以它对偏离人类模仿策略有一个 KL 惩罚。
我们有这个参数lambda，它控制着从人类模仿政策偏离的难易程度。
当lambda等于零时，它完全忽略人类模仿政策，只进行纯自我对弈。
所以在lambda等于零时，我们就像从头开始进行自我对弈。
当lambda等于无穷大时，它完全执行人类模仿政策，不进行软对弈。
但是对于lambda的中间值，我们发现它实际上为您提供了在遵循人类惯例和表现强大之间取得良好平衡的中间地带。
您可以在这里看到这种行为的出现。
抱歉，有一个问题。
这与离线强化学习相似吗，还是包含了探索？
所以我会说实际上有很多类似的工作都采用KL惩罚。
所以，是的，我会说这与很多类似的工作非常相似。
事实上，这也曾在AlphaStar中使用KL惩罚。
尽管那更多地是为了帮助探索，即利用人类数据来辅助探索，而不是试图更好地模仿人类。
所以我认为Pickle工作有趣的地方在于，首先，我们发现它比仅进行监督学习更好地模仿人类。
其次，我们进行了一些心灵理论，假设其他玩家也是如此，我们将其作为我们行为的模型，我们预期其他人认为我们的行为是什么，除了对其他玩家建模之外。
所以这就像是我们在这里使用的一种常识，类似的算法。
好的，你可以从中看到的行为，比如这里，假设英格兰同意。
抱歉，所以假设我们处于这种情况。
这实际上出现在一个真实的游戏中，并且它启发了我们论文中的一个图表。
英格兰和法国正在打仗。
法国是机器人，法国问英格兰是否愿意撤退。
假设英格兰说，是的，如果你回到北大西洋，请问我会离开英吉利海峡。
我们可以看到，Cicero实际上是后退了，离开了，去了北大西洋，撤退是成功的。
因此，这表明了机器人策略确实反映了它与其他玩家进行的对话。
英格兰可能发送的另一条消息是，我很抱歉，你整个游戏都在攻击我。
我不能相信你不会背叛我。
所以在这种情况下，西塞罗将继续对英格兰发起攻击，你可以再次看到，这是反映性的。
它会根据对话改变行为。
但你也可能会收到这样一种消息，英格兰说，是的，如果你去慕尼黑并运送到比利时，我就离开英吉利海峡。
所以这些对西塞罗来说是真的糟糕的行动。
如果你只看原始的策略网络，它实际上可能会这样做。
它可能会执行这些行动，因为英格兰建议这样做。
但因为我们使用的是Pinnacle，它会考虑到不同行动的期望值，实际上会严厉后退，但忽略建议的行动，因为它意识到那样会使自己非常容易受到攻击。
好的，为了节省时间，我将跳过幻灯片。
我想说的另一件事是，我们不仅仅是在进行规划，实际上我们是在进行完整的自我对弈强化学习循环。
而且，这里的目标实际上是更好地建模人类行为，而不仅仅是通过监督学习。
我们发现，使用Pickle进行这种自我对弈强化学习使我们能够比仅使用模仿学习更好地建模人类行为。
最后，我们有一系列的消息过滤技术，可以过滤掉无意义和策略不明智的消息。
那么，举个例子来说明这些过滤器是什么样子的，我们开发的其中一个是基于价值的过滤器。
因此，这样做的动机是，我们向对话模型输入的是我们自己和我们的对话伙伴的计划，但实际上是我们自己的整个计划。
因此，我们可能会向对话模型输入这样的事实，即我们将攻击我们正在交谈的玩家。
现在，说实话，对话模型有点愚蠢，它并不真正知道它不应该告诉这个玩家这次他们将被攻击。
因此，你可能会收到这样的消息，就像这里显示的第二个消息，英格兰对法国说，我们对你有敌意，你必须从棋盘上被清除，拜托提供一枚羊角面包。
所以这实际上是一个机器人发给玩家的消息，而不是一个玩家。
这是初步测试，有点像是激发了这整个方法的动机。
所以，如果机器人要攻击一个玩家，我们不希望它发送这种消息。
我们希望发送的内容有点像，你知道，不一定是彻头彻尾的谎言，但可能只是不发送消息，或者是一些更加平淡无奇的东西。
因此，我们通过查看价值来过滤出这些消息，我们所做的是生成一堆候选消息，然后我们看看如果我们发送这条消息，我们预计其他玩家会采取什么行动？
就像我们发送这条消息后，我们预计他们会做什么行动？
然后我们看看他们预计我们发送这条消息后会做什么？
然后我们查看，考虑到其他人打算采取的行动的预测，我们打算采取的行动的预期价值是多少。
所以，如果我们的意图是攻击法国，那么我们可以看到，嗯，如果我向法国发送这条消息，那么他们会非常防御，防御我们的攻击，并且我们的攻击将会失败。
因此，我可能不应该向他们发送这条消息。
因此，通过这种方式，我们实际上可以过滤掉预期价值低的消息。
我们发现这种方法效果出奇地好。
对话示例，我只是为了节省时间而遍历一个。
所以在这里，我们有西塞罗的法国和法国正在与土耳其交谈，土耳其是一个人类玩家，他们正在就谁将获得突尼斯进行辩论，这个领土用红色圈出来了。
您可以看到他们两个都在领土旁边拥有舰队。
如果他们都去争夺它，他们都不会得到它。
所以他们需要商讨出某种协议。
于是法国说，我愿意与你合作，但我现在需要突尼斯。
土耳其说，不行，你必须让我拿到它。
法国说，不，我需要它。
然后法国建议，你可以拿这些其他的领土。
你可以拿塞尔维亚和罗马。
土耳其说它们是不可能的目标。
然后西塞罗建议了具体的行动，让土耳其能够占领这些领土。
所以西塞罗说，希腊，特里奥尼亚，伊奥那特，推拉尼。
土耳其说，你说得对，好主意。
然后法国说，然后在秋季，你占领罗马，奥地利就会崩溃。
这样土耳其就能够对奥地利取得进展，但方便地也让法国能够占领突尼斯，因为土耳其将会把那些部队用于其他事情。
好的，所以限制和未来方向。
意图表示只是每个玩家的一个动作。
好的，所以有一个问题，就是我们输入对话模型的意图是我们将为这一轮和下一轮为自己和其他玩家采取的行动。
但理想情况下，我们会有一个更丰富的意图集。
我们将能够根据长期策略、沟通风格或提出问题等条件进行设置。
也就是说，这种方法的局限之一。
当然，你让意图空间变得更丰富，就会有更多出错的可能性。
而且你还必须训练模型来处理这些更广泛的意图空间。
有一个问题，你认为对话模型是否在学习内部模型、内部世界模型以便在预测移动时变得如此出色？
不，我们是，这可以说是我们为什么要根据意图进行设置的原因。
我们让对话模型不必担心构建一个良好的世界模型，因为我们告诉它，这些是我们打算在这一轮采取的行动。
以及我们希望这个其他玩家在这一轮采取的行动。
所以我们可以将世界模型与对话模型分开，但是要根据世界模型的输出来进行条件限制。
好的，另一个限制是，西塞罗的价值模型不是根据对话进行条件限制的。
因此，它对对话的长期影响有着有限的理解。
这大大限制了我们规划应该发送什么样的消息的能力。
这实际上就是为什么我们总是将西塞罗的对话生成条件限制在其真实意图上。
你可以说在外交中有时候你会想要对对方撒谎。
最好的玩家很少说谎，但有时候他们确实会说谎。
你必须理解说谎的代价，如果你撒谎，与这个人未来合作将会变得更加困难。
因此，你必须确保你从中得到的位置价值大于失去的信任和破裂的关系。
现在，由于西塞罗的价值模型不是根据对话进行条件限制的，它无法真正理解这种权衡。
因此，出于这个原因，我们总是将其条件限制在其真实意图上。
现在，可能存在西塞罗的价值模型条件对话，但您需要更多的数据，这将使事情变得更加昂贵。
所以我们无法进一步为这个机器人做更多的工作。
最后，有一个我之前提到的重要问题，那就是，是否有一种更一般的方式来扩展推理时间计算，以实现更好的性能？
我们在西塞罗中进行规划的方式，我认为有点领域特定。
我认为这就像泡菜的想法是相当通用的，但我认为还有可能有更一般的规划方式。
有人问，展望未来两三年，您将使用什么标准来选择下一个尝试征服的游戏？
说实话，就像我说的，我们选择了外交，因为我们认为这是最难制作AI的游戏。
我认为这是真的。
我认为我们不会再做游戏了，因为我想不出还有哪个游戏，如果我们成功了，那将会是真正令人印象深刻的。
所以我认为未来研究的方向是普遍性。
与其让人工智能玩这个特定的游戏，我们能否让一种人工智能既能玩外交，又能玩围棋或扑克，还能写论文和故事，解数学问题，写定理呢？
我认为我们将会看到游戏作为进步的基准，但不是目标。
它将成为测试集的一部分，但不是训练集的一部分。
我认为这才是未来的发展方向。
最后，我想补充的是，外交是一个多代理人人工智能和基于对话的一个了不起的测试平台。
所以如果你对这些领域感兴趣，我强烈推荐利用我们开放了所有代码和模型，并且对话和行动数据可以通过所谓的RFP获得，你可以申请获得对话和数据的访问权限。
好的，谢谢你们的聆听。
总而言之，西塞罗将战略推理和自然语言结合到外交中。
它位列人类玩家的前10%，论文已发表在科学杂志上，代码和模型都可以在此URL上公开获取。
所以感谢大家，剩下的时间，我会回答问题。
太好了，非常感谢您的演讲。
所以我们也会从班上接受一些问题。
当我们有未完成的时候，会有 Zoom 的问题。
所以如果有人有像 Zoom 的问题，我想，Noam，你可以回答这些。
是的，有一个问题。
您是否担心人工智能在现实世界的外交、战略谈判和欺骗任务中超越人类？
所以就像我说的，我们并不是非常关注欺骗，尽管可以说欺骗是外交游戏的一部分。
我认为对于外交和战略谈判，我不喜欢，看，我们开发 Cicero 的方式，它是专门设计用于玩外交游戏的，你不能直接将它用于其他任务。
话虽如此，我确实认为这些技术是相当通用的，希望其他人可以在此基础上进行建设，并能够做不同的事情。
而且我认为在未来几年内，完全有可能看到这种技术更频繁地进入现实世界的谈判中。
我实际上认为，外交是向真实世界的适用性迈出的重要一步，与围棋和扑克等游戏的突破相比，因为现在你的行动空间真的就像自然语言的空间，你必须对人类行为建模。
您认为未来我们可以任命一个人工智能加入联合国理事会吗？
希望只有当它比人类做得更好时才会这样，但那将是非常有趣的一件事情。
太好了。
我也很好奇，你在这个方向上正在做的未来的事情是什么？
你认为你能做出类似于 AlphaGo Zero 的东西，就像你只需拿这个模型然后也许我们可以自我 - 对弈或者你对改进这种类型的盒子有什么样的未来方向的想法？
我认为未来的方向真的是集中在普适性上。
就像我认为 Cicero 的一个重要的见解之一是利用规划来在语言模型和这个战略领域中获得更好的性能。
我认为在更广泛的领域中做这种事情的机会很多。
我是说，你看看今天的语言模型，它们是逐标记预测的，我认为有很大的机会超越这一点。
所以这就是我很兴奋要研究的。
我也很好奇，就是我不太明白你是如何使用规划或者蒙特卡洛搜索和你拥有的模型。
所以这是像 - 我们在 Cicero 中没有使用蒙特卡洛搜索。
蒙特卡洛搜索是一个非常好的启发式方法，但它是一个特别适用于确定性完全信息游戏的启发式方法。
我认为为了拥有一个真正通用的规划形式，我们需要比蒙特卡罗研究更抽象的东西。
我们使用了一种叫做“泡菜”的算法，基于一种后悔最小化算法。
我不太想深入讨论它的细节，因为这对于课堂来说并不那么重要，但是想法就是它是一种迭代算法，会逐渐改进对每个人会做什么的预测，并且随着迭代次数的增加，得到更准确的预测。
那就类似于搜索吗？
是的。
Esha。
你可以开始，你的麦克风已经打开了。
所以，我的问题是，当我们谈论能力时，不同模型之间的通信是什么样子的？
特别是当我们谈论对话模型时，你是如何将信息从策略网络发送到对话模型的？
未来，如果你有一个擅长不同任务的模型，我们会有一个非常庞大的策略网络来学习所有这些任务，还是针对每个任务都有单独的语言模型？
你会如何拆分它？
所以我们实际上将策略，就是我们自己和对话伙伴的动作，转化为一个自然语言字符串，然后将其与对话模型已有的所有对话一起输入。
所以这只是全部文本输入，文本输出。
而且这运行得很好。
那么你问题的第二部分是什么？
像，我们是不是只会有一个巨大的策略网络训练在所有情境上？
是的，就像，所以如果你只使用文本，难道这不会限制模型吗？
如果你用于不同的游戏，比如你说的未来会在通用性上工作，你是不是在考虑？
你是不是在考虑一个大的策略网络，它在不同的游戏上进行训练，或者能够同时理解不同的游戏，还是我们为不同的游戏有单独的策略网络？
而且，是的，这个文本界面是不是在沟通方面限制了模型？
比如说，如果你使用向量，可能，是的，这可能是一个底线。
我是说，理想情况下，你会朝着一个基础模型的方向发展，它几乎适用于所有情境。
文字，我的意思是，就像文本输入文本输出那样限制了你能做的事情，那就是沟通，但希望我们能超越这一点。
我认为现在这样做是个合理的选择。
谢谢。
我觉得我们有更多的Zoom问题。
好的，所以在聊天中有一个问题。
我很想听听你对未来的推测。
例如，我们看到一些初创公司正在调整LLMs，使其在某个主题X而不是Y上偏向或成为专家。好的，这似乎是一个非常一般的问题。
我对此没有强烈的意见。
是的，我是说，我自己对将语言模型调整到特定任务上并不太关注。
我认为我更感兴趣的方向是更一般化的规划。
所以我不认为我真的能评论，你知道，如何调整这些语言模型以这些方式？
那么你对什么样的规划方法感兴趣？
像MCTS就是其中之一。
让我，抱歉，我得出去一下。
转个房间，打扰一下。
好的，没关系，我们都准备好了。
对不起，问题是什么来着？
噢，是的，我只是在问，你认为哪种规划算法很有趣，可以结合起来呢？
所以你可以想象，我们有很多选择，比如我们有规划类的东西或者强化学习，有MCTS，还有你和思科合作的工作。
那么，你认为哪些算法是最有趣的，你认为能够良好扩展，能够概括？
嗯，我认为这是今天很多人都在努力解决的重要问题。
而且现在并不清楚答案是什么。
我是说，我认为你看一下一些思路，我认为有很多思路的局限性。
我认为应该有可能做得更好，但真的很令人印象深刻的是看到它是多么通用的一种方法。
所以我认为很希望看到以那种方式通用的东西，但希望能够取得更好的性能。
而且，当你说像思科那样是一个编码器解码器模型，从某种意义上说，它对世界进行编码，然后你有数据模型，它只是尝试对其进行解码。
是的，它是一个编码器解码器模型。
我不认为那一定是正确的选择，但那是我们使用的模型。
有任何问题吗？
好的，我认为我们大部分都不错。
但是，是的，非常感谢。
这很好。
嗯，是的，希望大家都喜欢。
如果有任何问题，请随时通过电子邮件与我联系，我很乐意聊天。
