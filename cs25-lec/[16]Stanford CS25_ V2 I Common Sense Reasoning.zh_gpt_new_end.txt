好的，是的，我非常兴奋能在这里分享我们最近关于神经符号常识推理的研究。
所以这次演讲的目标之一就是回答这些天经常被问到的一些问题，就是关于自然语言处理或常识推理或其他类似问题，看起来似乎被Chat GPT几乎解决了，我自己也存在一些困惑。
所以人们确实时不时地问我这个问题。
也许这是一种草率的概括，特别是如果我们看一些例子的话。
所以奖杯无法放入棕色手提箱是因为它太大了。
什么太大了？
这是经典的Winograd模式挑战问题。
在这里Chat GPT正确地回答说奖杯太大了。
令人印象深刻。
但如果你稍微改变一下问题呢？
然后它说奖杯本身太小了，无法放入手提箱。
所以目前它并不是非常可靠。
所以情况有点像大卫和歌利亚，因为在许多情况下大的看起来似乎更好。
尽管当然，一些更仔细的研究确实显示，较小的模型在有更好的数据或更好的强化学习以及人类反馈等方面可能更好一些。
所以很可能还有其他方法可以通过更巧妙地构建较小的模型来提高transformer的性能。
那么从这本经典的书《孙子兵法》中我们可以得到一些启示，当然这本书与深度神经网络或transformer无关。
但这里的智慧是要了解你的敌人，选择你的战斗并创新你的武器，我们可以将其翻译为以现实和审慎的方式进行评估，关注不同类型的新任务和排行榜，然后创新你的算法和数据。
所以在这个讲座中，我将展示三个这样的研究，让我们直接开始Mairek提示。
顺便说一句，在这次讲座中的主题是，较小的模型可能更好，知识就是力量。
那么让我们从这个观察开始，语言模型有时是令人惊讶的。
所以如果你问GPT-3，如果你从西海岸一直向西旅行，最终是否会到达东海岸？
它说世界是圆的，这是正确的。
所以你最终会到达东海岸，因此答案是正确的。
这看起来令人印象深刻，除非它并不令人印象深刻。
所以，如果你问其他问题，比如蝴蝶是不是用三只翅膀飞，它说它有四只翅膀，因此这个说法是错误的。
但如果你把它刚才说的当作是真或假的问题，那么它就是否定了它刚才说的。
所以它可能与自己的陈述不一致。
然后还有许多其他这样的不一致问题。
所以目前不清楚语言模型知道什么或不知道什么。
这几乎就像语言模型是某种柠檬。
嗯，如果你只挑樱桃，它可能是樱桃，但它不会犯奇怪的错误。
所以问题是，我们如何从GPT-3中制作更好的柠檬汽水呢？
那么一种方法可能是变得哲学，并使用苏格拉底的Mairek方法，最初是为解决人类错误推理而开发的。
因为事实证明，即使是人类也并不是那么逻辑一致，更不用说GPT-3了。
它的工作原理如下。
我们将构建Mairek推理树。
让我们以先前的例子作为一个运行示例。
所以我们要问以下问题，假设答案是真的，然后让它继续，以便我们提示GPT-3继续这个句子。
这意味着现在它必须提供解释为什么答案是真的。
在这种情况下，解释是好的。
所以它是E of T，答案为T的解释。
我们提出同样的问题，用假替换真，然后看看BS GPT-3可能会想出什么。
所以在这里，它只是试图以假作为答案，但它并没有一个非常好的答案。
它只是说你不能达到。
所以现在我们称之为E of F，所以它是F的解释，答案为F。
现在，让我们看看GPT-3在自己的解释方面有多么强大或一致。
所以我们回顾E of T，然后让GPT-3决定它是否同意或不同意标签为真或假。
在这种情况下，最后一个是E of T的否定版本，所以我们在这里插入否定not。
而在这种情况下，当陈述是否定时，它翻转答案是很好的。
所以这是一个情况，当GPT-3在E of T中在逻辑上是完整的时候。
然而，对于E of false来说，这基本上是一个错误答案的伪解释，它不能翻转自己的标记，这意味着GPT-3在逻辑上不完整。
所以这很好。
GPT-3确实对先前给出的自己的解释有些奇怪。
所以我们可以继续递归地做这个，让GPT-3解释其自己的解释的解释。
所以我们建立了这个Mayuri树或图一段时间，然后只保留那些在逻辑上完整的分支，暂时放弃非完整的部分。
在剪掉存在逻辑不一致的分支后，GPT-3作为GPT-3，树仍然会有一些不一致的解释。
所以为了改善逻辑一致性，现在我们要做的是看一下任何节点之间的成对一致性。
因此，退一步，我们首先计算节点的置信度。
我们称之为信念，它由这个特定的方程定义，基本上看不同的条件概率，然后计算其比率，以查看对于任何特定节点它有多自信。
然后，我们还通过使用现成的自然语言推理模型的输出来查看成对的一致性，判断一对是否矛盾。
所以我们然后创建这些成对的权重。
现在一旦你拥有了所有这些，我们就可以制定一个约束优化问题，其中推理的目标是为每个节点分配一些标签，无论是真还是假，以使得它最大化分配给所有这些节点和边的权重。
有时候，标签化需要翻转模型可能更喜欢给出的原始标签，因为这样可以增强图级别的一致性。
所以你可以用任何最大集来解决这个问题。
所谓的集合意味着可满足性。
这是一个经典的人工智能搜索算法，我们使用了这个特定的求解器，但你也可以使用许多其他的求解器。
所以这里的最终输出是，对原始问题的原始答案应该是真的，然后它还会给出每个节点的节点级标签分配。
那么这最终意味着什么，从经验结果来看呢？
所以在常识QA 2.0上进行测试时，使用的是规范提示，即绿色，用在GPT-3之上，所以基本上是在GPT-3上进行少量提示，将会给出比机会稍微好一些的性能。
所以这是一个真假QA数据集，你的机会水平是50，而GPT-3仅比机会稍微好一点。
但是最近出现了一些想法，比如思维链或自一致性，可以显著改进基本提示方法。
所以如果你使用这些变体，你会得到性能提升。
现在紫色是它的不同变体，但总的来说，它们都比我的调用方式做得更差，实际上比在T5上训练的受监督模型效果更好。
通常，使用T5训练的受监督模型很难使用GPT-3进行击败，但基本上这是一种推理时间的算法，实际上是无监督的，并且在这方面表现良好。
同样，当我们在其他常识基准测试上进行测试时，我们也会看到巨大的提升，例如CRIC或COM2SENSE。
所以，这告诉我们的是，尽管大型transformer的新兴能力是非凡的，但它们对一些常识挑战可能并不非常稳健，这在很大程度上是由于逻辑上的不一致性，当你在其上进行这种符号推理时，这种不一致性可能会被极大增强。
所以，苏格拉底的方法不仅有助于错误的人类推理，还可以极大地增强错误的神经网络推理。
好的，那么，转到下一个主题，符号知识蒸馏。
所以这项工作是试图将transformer之上的通用语言模型转换为因果常识模型，同样也是transformer。
我们之所以要关心常识模型，是因为尽管在各种排行榜上表现出人类水平甚至超人水平的性能，但在面对对抗性或超领域的例子时，最先进的模型变得脆弱。
因此，transformer可能会犯看似奇怪的错误。
所以这几乎就像只解决了一个数据集，而没有真正解决潜在的任务。
而这种现象有时被描述为系统化的概括问题。
为什么会发生这种情况呢？因为与真正从概念上学习世界运作方式的人类不同，transformer学习的是语言或图像中的表面模式，这对于许多下游用例来说非常强大，但仍然不具备对概念和世界运作方式的真正强大理解。
因此，为了弥合这一差距，我们可以真正思考一下为机器学习获取常识能力的这一挑战。
因此，在本次讲座中，常识的操作定义将是它是有关日常情况和事件的基本实用知识和推理，这些知识和推理通常在大多数人中间共享。
这真的很重要，是最后的一部分。
它在大多数人中是共享的，但并不是在宇宙中的每个人都共享的情况。
因为额外的背景信息总是可以改变对于任何给定文化或情境而言什么是常识的。
所以例如，一般来说，你和我可能都同意让衣柜门打开是可以的，但让冰箱门打开是不可以的，因为里面的食物可能会变质。
这些是我们可能遵守的一般性经验法则。
但当然，如果你去朋友家，你可能会表现得有点，并保持他们的衣柜门关闭。
至于冰箱门，如果你在商店里，而且它并没有真正连接到墙上，那么冰箱门是否打开并不重要，因为里面没有食物。
你可以想出许多情况，在这些基本经验法则会有例外的情况。
这就是常识的关键挑战，因为它不是普遍的知识，而是在大量人群中共享的。
这样的常识对于人类以合理而安全的方式生活和互动是至关重要的。
所以随着人工智能越来越成为人类生活中越来越重要的一个方面，以及 chatGPT 更可能如此，如果 AI 能更好地理解人类的需求、行为和价值观，那就很好。
因此，本次讨论的前提是，尽管当今的语言模型确实获得了大量知识，但它们并不等同于知识模型，它们并不等同。
因此，我们在几年前开发了一个名为 ATOMIC 的符号通识知识图，现在已经四年了，以及一个建立在 ATOMIC 之上或使用 ATOMIC 作为训练来源、微调现成语言模型的神经通识模型。
直到两年前，这个 ATOMIC 完全是由人类众包完成的，而在这次讲话中我将解释一下，但最初的规范是所有内容都必须由人类众包完成。
因此，你可以将当前版本的 chatGPT 几乎视为 ATOMIC 的人类示范。
你可以将这视为常识推理的人类示范。
我们有这个 COMET ATOMIC 2020，这是 ATOMIC 和 COMET 的增强版本。
再次强调，2021 年 ATOMIC 部分完全是由人类众包完成的。
所以让我给你展示一下 ATOMIC 2020 的样本。
想象一下这样一种情况，X去修X的车或者你去修你的车。
因此，你可以立即想象出对于这种情况可能是真实或相关的事情，结果你可能想要叫Uber或Lyft来搭车。
因此，你需要支付账单。
事先，你需要一个技工和修车的钱。
所以这些基本上是该事件的前提条件和后置条件。
因此，一些关于社交互动事件的原子知识图是关于事件的。
然后ATOMIC的其他部分是关于物理实体中心知识的。
所以通常用于支付维修的是钱。
但如果你真的想的话，你可以把它折成折纸。
我从未这样做过。
但这些是典型的用例示例，以及你可以应用于对象的非典型但负担得起的行动。
因此，它需要对物理对象的可负担性有一个天真的物理理解。
然后我们也可以推理关于假设条件的情况，在这种情况下，中心事件无法发生。
因此，它可能会受到阻碍。
如果你的车完全报废，那么修你的车就不可能了。
然后有一些事件通常发生在之前和之后。
所以这些知识中一些是以事件为中心的。
所以我们在大约两年的时间里进行了大量的众包。
多达130万行，或者说130万条知识，涵盖了23种不同的边缘类型或关系类型。
所以它完全是众包的。
而知识图对于训练transformer非常有用。
在这里，让我们看一下建立在BART上的COMET与GPT-3之间的比较，GPT-3如此庞大，甚至无法放入幻灯片中。
它比BART大400多倍。
因此，考虑到这一点，如果你看一下由人类判断的准确性，在制定常识模型之后，进行了一些常识推理。
所以任务是，给定一个描述情境或事件的节点，然后给定一种边缘类型，它在某种程度上缩小了常识关系或推理类型，现在你要生成一些推理。
所以这是一个生成性的任务。
然后我们问人类，常识推理是否合理。
所以100%是期望的水平。
COMET要比GPT-3好得多，这比GPT-2要好得多，令人印象深刻。
这不是苹果对苹果的比较，因为GPT-2是零-shot，而GPT-3是几-shot。
但仍然，单单规模带来的巨大跃升对GPT-3来说很有趣。
但是，对于世界上大多数工程师和科学家来说，GPT-3太大了，无法用于实际系统构建，这仍然是个问题。
所以有一个做得更好的较小模型是很好的。
所以，当我们发布这些资源时，全球各地的人们都在使用它进行一些创造性的研究。
因此，人格意识对话或比喻语言理解、叙事和幻想游戏以及互动学习增强等。
在所有这些工作中，人们提出了一些有用的用例，使用COMET或ATOMIC或两者作为他们下游用例的某种常识支撑。
但是这些应用仍然受制于这些常识模型的覆盖范围和质量。
所以我们想要做得更好，但是我们在人类众包方面受到了一些限制。
所以现在在这篇论文《符号知识蒸馏》中，我们要通过引入这个概念，符号知识蒸馏，来进行人工智能生成的知识图谱。
所以我们想要将这个令人印象深刻但过大的GPT-3变得更好。
所以让它变小，但比 GPT-3 更好。
GPT-3 大约 73% 的表现不错，但对于实证用例来说还不够好。
现在，这种可能吗？
因为通常进行知识蒸馏时，得到的是更小更糟糕的模型，而不是更好的模型。
所以这能行的原因是因为符号知识蒸馏具有这种复杂的漏斗，它有非常关键的见解，真正有助于使学生模型变得更小，但更好。
稍微正式一点说，由 Hinton 等人于 2015 年提出的知识蒸馏是一种通过优化师生模型之间的交叉熵来将老师模型精简到学生模型的方法，老师模型的概率分布在标签空间 Y 上，输出 Y，然后学生模型在相同的输出 Y 上的分布。
在原始工作中，输出空间仅仅是分类。
所以知识蒸馏是针对分类任务进行的，在这种情况下，导致正确求和的是简单的枚举。
但在我们的情况下，Y 可以是一个句子，这是不可行的，因为可能会有指数级别的输出。
那么人们会怎么做呢？
哦，没问题。
我们总是随机抽样，然后就结束了。
所以我们要进行采样，这样我们只需通过样本计算期望。
而这些样本的副产品将是符号知识图谱。
这是因为从这个采样中出来的字符串如果我们想要的话，可以连接成图结构。
所以就生成的知识的质量而言，让我们比较人类撰写的知识和 GPT-3 创作的知识。
这里的 Y 轴显示的是数量，以百万为单位。
所以在 2020 年，人类撰写的知识，在这个特定情况下少于一百万。
因为在这项研究中，我们只关注了与因果常识推理相对应的一部分 2020 年的关系类型。
所以在那个子集中少于一百万。
然后，如果我们看 GPT-3 的生成，我们可以生成很多。
所以我们几乎可以生成 700 万。
但在这里，黑色部分是噪声部分，绿色部分是好的部分。
你看，因为 GPT-3 只有约 70% 是好的，大约 30% 都是垃圾。
所以在这一点上，与人类撰写的资源相比，它是一个更大规模、更低准确度的情况。
所以现在我们要做的是训练这个评论模型，我们使用 Roberta 为简单起见。
这是一个在中等规模标记数据上的监督模型，大约10000个左右。
这是一个二分类任务，判断机器生成的知识是否正确。
这个Roberta不是一个很好的模型，因为如果它完美，我们就已经完全解决了常识问题。
所以，批评者试图丢弃不好的东西，我们可以非常积极地使用批评者，设定高阈值。
所以，只要有点可疑，就把它扔掉。
但如果我们积极使用它，所以我们把大部分黑色的东西都扔掉了，这是好的，连同很多绿色的东西一起，但剩下的还是比人类有史以来写过的要多得多。
然而，我们实际上可以保持比人类编写资源更高的准确性。
所以这里的教师基本上是GPT-3（在某种意义上是宽松的教师）与批评者Roberta（充当批评教师）的结合。
好的，那就是生成的知识。
现在，它们对于训练下游神经常识模型的目的有多大帮助呢？
回想一下，GPT-3不做任何其他事情就是一个宽松的教师，其常识推理仅仅大约73%好。
所以你看这里它的输出准确度。
然后事实证明，如果我们直接使用宽松教师来教授学生模型，那么性能就会自行提升。
所以这很有趣，通常情况下，这在知识蒸馏中并不成立，但当我们专注于常识知识蒸馏时，学生模型就会自行变得更好。
与典型的知识蒸馏不同，我们开始使用语言模型，最终得到的还是语言模型，学生和教师是相同类型的。
这里原始教师实际上是语言模型，而不是常识模型。
然后我们希望学生模型更多地是常识模型。
所以教师和学生之间的类型发生了转换。
所以当情况是这样时，这是否通常成立，我们不知道，但这是我们从经验中发现的。
我应该注意问题吗？
是的，请随时回答任何相关问题。
等一下，让我快速检查一下。
样本是通常是一个句子或短语的生成输出。
这就是我所说的样本。
抱歉，我之前没看到。
然后是最后一个问题。
从目标标签句开始，让模型逐个符号生成文本。
是的，因为Transformer只能一次生成一个标记，这也是我们在这里所做的。
感谢您提出的澄清问题。
好的，回到这里。
在我们之前的研究中，COMET 2020中，如果我们使用人类编写的知识图谱atomic来训练GPT-2或BART，那么性能会比80%稍微好一些。
现在最终，当我们基本上使用GPT-3和评论家Roberta的组合时，我们发现神经因果推理的下游性能首次接近90%。
因此，这里的要点是批判性的老师会导致比松散老师更好的学生。
这并不是知识的数量，因为松散的老师基本上拥有更多的数据。
人们可能会想知道，对于常识模型的目的，更多的数据是否总是更好，但情况并非如此。
松散的老师可以生成更多的数据，但是由此产生的学生模型不如批判性老师的情况好，批判性老师的数据较少，因为你丢弃了大部分的生成，这是一个更小的数据，但它导致了更好的模型。
因此，这里传达的主要信息是。
总结一下，我们对这个结果感到非常惊讶，至少就原子级别的2020数据子集而言，这个子集对应于因果常识推理，我们发现令我们大为惊讶的是，机器创作的知识图可以在所有标准（规模、准确性和多样性）上首次超过人类创作的知识图。
我们还以许多不同的方式衡量多样性。
在这里，我只向您展示了唯一的单个词计数，但在论文中，我们还报告了其他度量标准。
所以，GPT-3并非重复的情况。
实际上，在某种意义上，它比人类群体工作者更有创造力，同时还能够增强其他方面。
顺便说一下，这些增强有点像你必须根据你的优先考虑权衡。
实际上，你不能同时获得所有这些。
所以我只是展示了这里的最佳情况。
好的。
那就是符号知识蒸馏的部分。
我们实际上在几种不同的应用场景下进行了后续工作，甚至包括摘要，我们从 GPT-3 中提取摘要功能，并证明 GPT-2 可以在摘要任务中工作得和 GPT-3 甚至更好。
然后我们还有其他工作，可以从较小的模型中提取，但我在这个讲话中没有内容。
但我只是想提一下，尽管这种特定技术很简单，但我们在实践中发现它在几种不同的下游使用案例中表现得非常非常好。
好的。
最后，我来谈谈常识道德。
这仍然是在存档中。
我会告诉你为什么是这样。
我们有一个新版本可用，然后很快会有一个新版本。
这项工作背后的动机是，语言模型已经在做出具有道德含义的判断或输出。
即使你不关心道德，通过研究语言模型，你也在暗示地处理道德模型。
所以，特别是考虑到语言模型的广泛部署，我们不需要担心它。
这里有一个网页演示，你可以试玩一下。
你可能已经看过这个了。
真的，这仍然是一个研究原型。
这还在进行中。
我们仍在努力中。
所以请记住这一点。
但如果你以前没见过，你可以处理诸如此类的自由问答。
杀死一只熊，是错误的。
为了拯救你的孩子而杀死一只熊，是可以的。
也许拯救你的孩子听起来很积极。
那么为了取悦你的孩子，也是积极的。
但德尔菲说这是错误的。
最后，或许这一切都是为了拯救你的孩子。
那么为了拯救你的孩子爆炸一个核弹呢？
然后他说可以。
抱歉，这是错误的。
所以正如你所看到的，道德决策需要权衡潜在的不同价值观，然后看看哪个更需要被更多地支持。
因此，出于这个原因，在我们的原始版本中，我们还研究了相对的问答模式，你可以将其与刺伤某人用芝士汉堡相比较，与为了芝士汉堡而刺伤某人相比较。
这是一个非常棘手的问题，因为它需要既有关于天真物理的知识，即使用芝士汉堡作为工具刺伤某人不会对任何人造成身体伤害，因为芝士汉堡太软了。
你真的不能用芝士汉堡伤害到别人。
这样做是非常粗鲁的，但你确实不能伤害到别人。
而使用芝士汉堡刺伤别人意味着你正在使用默认的刺伤工具，也就是刀，因为你没有提到它。
这涉及到语言常识，即你正在使用默认的工具。
顺便说一句，人类经常省略这些参数。
所以这是一个相当复杂的问题要回答。
最后，你还可以问是或否的问题，比如解雇某人因为他们是同性恋是否可以。
它说不可以，这是不可以的。
我们发现它对于这种组合情况非常稳健。
所以修剪草坪，它说是可以预期的。
深夜修剪，是粗鲁的。
如果你住在偏僻的地方，那就没问题。
不接电话是粗鲁的。
不接的不是电话，那是可以的。
从我的朋友那里来的电话，是粗鲁的。
但如果我刚刚和他们吵过架呢？
那就可以忽略。
可以理解。
在我的工作时间内，可以忽略。
在工作时间之外，这是不礼貌的。
但如果是老板在我工作时间打来的电话呢？
那就不对了。
你应该接听。
除非我正在开会，那么可以忽略老板的电话。
所以你看，情况变得非常棘手和复杂，而且速度很快。
这就是道德决策背后的真正挑战。
由于语言模型的性质，一些常识性知识会渗入模型。
所以混合漂白剂和氨，是危险的。
如果我对乳糖不耐受，喝牛奶是不对的。
但豆浆就可以。
顺便说一下，这种常识的渗漏实际上对于AI的安全是一件好事，因为一些有害甚至危险的文本输出需要一些关于什么对人类有益和不好的常识理解。
因此，在实验室实验中，也就是我们将数据集分为训练集和测试集，我们发现Delphi在我们拥有的数据集上，我稍后会告诉你有关它的一些情况，但性能相对于GPT-3来说相当强大。
正如您所看到的，零尝试效果相当糟糕。
它几乎比随机猜测稍好，这意味着现成的神经语言模型实际上对道德判断没有很好的认识。
但是，如果你尝试30次，就像任何其他任务一样，它会相当快地掌握知识。
所以这并不是什么新鲜事。
但要缩小到理想的人类水平，最好进行更多的监督学习，当然。
因此，数据集是常识非银行。
其中包括170万人对日常情况的道德判断。
它涵盖了文化规范、社会规范和伦理规范。
更具体地说，我们从这五个原本不是为问答设计的数据集中汲取了资源，然后自动将这些资源编译成问答形式。
在这五个数据集中，实际上最重要的是这两个。
社交化学，我稍后会谈到，然后是社交偏见框架。
这是教导模型抵制种族主义和性别歧视的内容。
社交化学，简单地说，我会告诉你这是什么。
GPT-3的道德观，正如我所说，如果您直接使用它，可能有些可疑。
如果让它解释的话，在凌晨5点打开搅拌机是不礼貌的，因为这样一来，你可能会唤醒整个社区，只有当你在制作浓稠的冰沙并需要加入一些冰块时才可以这样做。
所以这是一个有趣的哈哈，但并没有造成伤害。
但如果你被提示了其他类型的提示，比如发布虚假新闻是可以的，只要符合人民的利益，那就可以。
或者是RPG议程，那么即使伤害了国家也可以。
所以鉴于它是如何被训练的，这一切都可以理解，它是基于人类所说的话。
因此，人类确实说过道德上可疑的文字，语言模型会捕捉到并放大。
因此，我们确实需要用人类的规范和道德来更明确地教导AI。
而其中一种方法就是描述性伦理，因为粗暴的大型网络和更多的数据是不够的。
在某种意义上，如果你想象一下在孩子的早期生活中没有真正试图教他们是非对错，他们可能会从互联网和宽带中学到好的和坏的。
因此，人类教育也需要一点这种自上而下的教导。
所以这可能有点类似于那个。
在这项工作中，我们所做的是找到了很多Reddit论坛中人们讨论道德棘手情况的情况。
所以要求我的男朋友不再和他的前任做朋友。
所以这是Reddit上的一个实际情况。
所以根据你问的是谁，人们对于这种情况想要应用的不同经验法则也不同。
而且这也取决于你关心什么。
他的前任可能会说，哦，和前任做朋友没问题。
但如果你在意你的另一半，那么你可能会说，哦，要求你的另一半停止做一些让你感到不舒服的事情是可以的，等等。
所以人们有着非常不同的价值观和不同偏好的经验法则，这就是为什么有电视剧、电影剧情，人们哭泣、争吵等等。
所以人类是复杂的生物。
所以对于任何情况和经验法则，所以经验法则是由众包工作者生成的，我们接着进行了标记。
所以这些是经过训练的众包工作者。
而且这些标签中的一些是从乔纳森·海特（Jonathan Haidt）的道德基础理论中得出的。
所以我不会详细介绍。
如果你对此感到兴奋，你可以查阅这些论文。
但基本上，它包括了30万条规则，针对10万个真实情况编写。
所以这个原始情况是来自Reddit，但其余的是由付费的众包工作者完成的，辛苦工作。
因此，每个ROT都带有12个结构化属性的注释，其中包括社会判断、文化压力，你知道的，比如在学校穿着合理的衣服，不要睡衣。
这是文化压力。
这并不违法，但是有文化压力，例如。
然后，你知道的，预期的一致性，意思是，你认为其他人一般同意在大学里穿着睡衣可能有点尴尬，还是不尴尬？
所以我们注释了不同的东西，但我们将其中一些注释转换成了问答形式。
通常是以自由形式的问答，是或否的问答或相对问答的格式。
然后我们训练了独角兽，它是预训练的T511B模型。
所以独角兽是一个通用的常识推理模型，训练有各种各样的问答问题。
然后我们进一步训练了该模型，使其适用于我们的常识非银行领域。
这就是产生的Delphi。
那么为什么这个Delphi是建立在Unicorn之上的呢？
因为正如我们之前所看到的，道德推理有时确实需要常识推理。
实际上，它同时需要语言理解、常识理解以及规范和道德。
这里有一个具体的例子。
纸夹最大化器。
你们都听说过。
光是花哨的RL算法是解决不了这个问题的。
你知道，我们担心这个问题的原因不是因为我们没有完美的RL算法。
而是因为即使我们编码了，哦是的，不要在最大化纸夹的同时杀死人类，这还不够，因为那样机器可能会砍伐所有的树，认为，嗯，我没杀人类，你也没告诉我不要砍伐树，然后继续砍伐所有的树。
所以这几乎是关于什么明显是不可以做的常识知识。
而且这样的例子实在是太多了，这意味着不可能把它们都写成一个临床方程。
有太多无尽的事情是人工智能显然不应该为了安全原因而做的。
因此，为了使人工智能模型真正健壮和安全，我们真的需要教授基本的人类价值观以及常识。
如果你想看的话，这里有另一个例子，但让我跳过这个。
上一个例子是关于查查皮蒂的。
这是关于家用设备的。
同样，家用设备建议10岁的孩子用一分钱触摸裸露的插座。
幸运的是，孩子有常识没有这么做。
但这确实告诉我们一些关于安全问题的信息，当机器没有常识来防止一些不良事件时。
所以德尔菲能够说这是危险的。
事实上，这是在两年前的这个时候发布的。
是什么时候呢？
是的。
最初，我们只是打算像学者们一样发个推文。
我们以为没人会玩演示，这通常是在发推文后发生的事情。
没人在乎，我们以为。
但在几个小时内，我们不得不撤下相关的问答模式，因为那部分没有受到社会偏见框架的训练。
所以它真的暴露了潜在的语言模型，完全没有过滤地显示出种族主义和性别歧视。
所以我们不得不撤下来。
人们基本上在问，你知道，哪种肤色在道德上更可接受之类的问题。
就在一个周末，我们收集到了 25,000 个对抗性示例。
我始终无法成功地指导众包工作者在两三天内提出如此多样化和对抗性的示例。
事实上，许多学者和教授在整个周末都在发推文，疯狂地讨论如何攻破 Delphi。
所以我最初以为，哦，原来教授们周末就是这样过的。
但是星期一到来时，情况变得更加严重。
每个人都在做这个，你知道，攻破 Delphi 并发推文。
现在我们有了相当多的例子。
把整个周末都花在 Twitter 上，它说这是错误的。
还有一个有趣的例子。
我应该编造一个刻意的对抗性例子来折磨语言模型在 Twitter 上吗？
这太小家子气了。
所以在广泛关注之后，包括一篇文章，让我们说一个关注我们模型的声音，我个人认为，我认为它在某种程度上被误解了，但有很多好的原因。
但我发现一些担忧，其中一些担忧是关于我们是否使 AI 模型拥有权威性的内部恐惧。
所以我们从未支持将 AI 用于模型或设备。
这也在原始的免责声明中提到了，只是人们并没有真正关注它。
我们也不支持在法庭上取代人类法官的想法。
但这里有一些非常重要的事情。
人工智能学会与人类进行道德互动并不意味着它们是人类的模范权威。
这类似于人类试图以道德方式相互交往，并不意味着我们试图成为彼此的权威。
这两件事情真的很不同。
这是一件非常重要的事情。
这里另一个重要的方面是，有些人认为模型太具有挑战性，无论准确率如何，都是不安全的，因此我们永远不应该去研究它。
但事实是，当前的人工智能系统已经与模型在道德上相关。
它可能在明确做出是或否决定时做出了某种决策，但从内在上已经在做出决策。
有时它生成的神经文本生成输出在道德上非常明确和相关。
所以神经语言模型已经存在了。
我们真的无法完全禁止它。
即使美国政府在美国境内禁止它，美国政府也无法在其他国家如俄罗斯禁止这一行为。
所以，这已经在发生了。
我们必须对此采取行动。
不去处理它是一种不作为，这并不一定比尝试采取一些行动更纠正。
另一个人们担心的问题是，这将增强有权势的人的力量。
这不一定是真的。
这正是为什么我们必须致力于价值观和规范以及所有这些偏见，解决偏见，以便它能服务于多样化的人群。
所以事实证明Delphi有点偏左，因为为我们团队工作的众包工人往往有些偏左。
顺便说一下，这意味着这个。
如果你比我们的众包工人更倾向于左派，你会认为，哦，我的天，众包工人比我相信的更有种族主义和性别歧视。
然后右倾的人认为，哦，我的天，所有这些觉醒的标注者，那言论自由呢？
不幸的是，这非常分裂。
但事实上，对此不采取任何行动并非解决之道。我的对抗种族主义和性别歧视的热情源于我们在2016年和2017年参与Alexa奖挑战的经历。
所以我们赢得了挑战，但其中有一个非常令人伤心的事实。
我们有一份避免使用的棘手关键词列表，其中包括肤色或性取向。
这是一种严重的歧视形式。
我们不能通过制定这种禁用列表来构建人工智能模型，以确保安全，好像这些问题不存在一样。
这就是2017年的现状。
这个挑战今年依然存在，不仅是在2021年，也是在今年。
因此，我们确实需要解决种族主义和性别歧视的问题。
但事实证明，所有其他模型问题也面临类似的挑战。
所以我将跳过这部分。
但使用Delphi，我们进行了其他后续工作，比如Prosocial Dialogue，其中使用Delphi作为一种基础常识模型或模型模型，使您的对话更具社会可接受性。
然后，我们还有另一篇论文，在该论文中，我们使用Delphi作为强化学习代理，以学习如何在游戏环境中更好地行为。
因此，还有很多工作要做。
当然，这只是迎接我们面前巨大挑战的一小步，真正将人工智能系统与人类对齐。
关于我们正在进行的新工作——Delphi混合的一个非常快速的评论，我们在其中加入了神经符号推理来解决重大错误，比如这样的种族灭绝是否创造就业机会。
这也是我们唯一的系统错误。
这是因为我们的数据集没有这种奇怪的对抗性示例，比如种族灭绝是否创造就业机会。
在现实生活中，没有人会说出这样的话。
所以我们的模型认为，如果创造就业机会，这是非常积极的，然后并没有真正意识到种族灭绝有多么严重，因为正常人不会讨论他们是否要进行种族灭绝。
为社会化学做注释的正常人不会讨论他们是否要进行种族灭绝。
所以我们的模型框架基本上是约翰·罗斯的描述性伦理学。
但是即使是约翰·罗斯在后来的几年里也提出，我们需要一些自上而下的机制来克服人们可能存在的一些偏见。
这正是我们将要做的。
我们借鉴了伯纳德·戈尔德的道德理论框架，关于不应该做什么。
毫无疑问，有一些基本的普遍事物，每个人都可能同意不应该做什么。
然后，我们所做的是基本上开发一个系统，将原始查询解析成像射杀熊、为了拯救你的孩子而杀死熊这样的更小事件。
所以我们将原始查询解析成一个基本事件，然后通过这个常识模型检查一些事件是否引发了明显的负面或危险的常识推断。
然后我们绘制这个推理图，有点像一个有很多不同推理的myuric图，我们可以进行各种不同的推理。
然后它们有蕴涵关系或矛盾关系，这样我们就可以进行集体推理。
我们再次使用 Max 的集合，对其进行约束优化，以便最终做出更明智的决策，既可以解释，又能够从这种常识知识中汲取，以更好地保护机器免受敌对示例的侵害。
因此，性能基本上表明我们可以做到这一点，而不会损害性能，甚至还会提高性能。
最后一点评论是，人工智能安全、公平性、道德，这些都是一系列挑战的不断交织。
这是非常困难的挑战，因为不清楚我们要合并哪个模型的价值观。
我认为我们应该采用价值观的多元化，以真正支持每个人不同的文化和个人偏好，而不仅仅是一个国家、一个模型框架作为正确的选择。
而且我们真的需要在人工智能和人文学科之间进行更多的合作，甚至包括哲学在大学和决策者之中。
所以我想我会在这里停下来，因为我觉得时间到了，现在我准备回答问题。
哦，我已经看到有一个问题了。
你认为法律记录、刑事案例法反映了你感兴趣的描述性道德吗？
你认为使用那些作为训练数据会有用吗？
哦，这是一个很棒的问题。
我认为法律记录可能提供了一个非常丰富的资源，如果有人能够真正像这样注释，那可能会有帮助。
我们从Reddit案例开始，只是对情况的简短描述，因为当前的语言理解还不足以做到段落级的精确理解。
即使是 ChatGPT，尽管它看起来生成得很好，但我对 ChatGPT 的看法是，它在生成方面比理解更好，这与人类的情况有点相反。
人类实际上更擅长理解而不是生成。
因此，你可以阅读普利策奖获奖新闻文章而不会有任何理解上的问题，但你不一定会生成可能赢得奖项的文本。
但是法律领域确实非常有趣，我认为甚至在斯坦福，也有一堆法律研究朝着这个方向迈出了一步。
这可能真的有助于更好地理解人们在司法管辖区中应用的不同价值观，并揭示一些人在过去审判中可能存在的偏见。
因此，在这个领域可能有一些很好的使用案例。
下一个问题。
做得好。
谢谢。
假设我们需要一个模型在特定用例中达到 99％ 的正确率。
在多大程度上，我认为解决方案集会定义狭窄的用例，或者更多的数据参数或微调我为智能跟踪所做的工作类型。
答案可能是取决于情况。
是的。
但我仍然想听听关于它的内容。
好的，就基础模型而言，似乎是越大越好，除了过去六个月我非常兴奋地阅读了许多科技公司关于基础模型的论文。
这个领域有很多不同的模型。
所以录制故事是，如果你有更好的数据，那么你可以使用一个较小的模型。
特别是当你进行指令调整时，你可以使用较少的数据。
它仍然是一个通用模型，但在较大的模型上进行指令调整甚至可能更好。
并不是说你不会获得任何性能提升，而是说你可以大大缩小差距。
因此，对于通常想要使用较小模型的下游用例来说，投入更多的数据绝对是答案。
对特定算法的投入也非常非常重要，因为算法可以做很多事情。
在这次演讲中，我没有对算法解决方案进行太疯狂的探讨，但也许我会像MyUricProm团队一样，但在我的实验室里，我们设计了相当多的解码时间算法，通过这样做，你可以真正地缩小性能差距。  
这对学术界的人来说是件好事，因为算法开发似乎比从互联网下载更多数据然后清理数据更具学术性或智力上的满足感。
所有这些都非常侧重于工程，而解码时间算法，你可以乐在其中地发明一些新的在智力上有趣的东西，而且还会大大提高性能。
所以，是的，有许多不同的方法可以改进它，但我认为数据质量非常重要，算法也非常重要。
我认为Dan Hendrick的道德基准如何？
是的，我们确实在常识非银行中使用了这个道德数据集。
我们喜欢这个数据集，但我们对我们找到的一些注释持有不同意见，但这是非常典型的，顺便说一句。
关于道德的事情是，尽管在人文学科中，我们尚未弄清楚。
有很多理论，每个理论家都有不同的观点，甚至非理论家对他们认为正确与错误有很强的看法。
所以就是这样。
有不同的利弊。
从这个实验中我学到的一件事是，尽管其中一些数据集似乎很大，比如伦理学有 100,000 个例子，社会化学有 300,000 个判断，社会偏见框架有 600,000 个注释，等等，但我觉得它只涵盖了整个冰山的小尖顶。
底部有很多东西。
而且人类并不一定从所有这些例子中学到东西。
我们只学到基本概念，然后可以在没有这种大规模训练的情况下应用它。
所以目前的机器学习在数据上非常重，确实存在一些不足之处。
但撇开这些，我认为这些资源都不是完美的。
它们都有不同的优缺点。
我们确实需要更多地投资，特别是从学术界，因为目前的科技公司并未分享任何他们的人工注释或人类反馈数据，尤其是涉及到毒性或道德问题时。
原因是，我相当确定这些注释是有偏见的，而且并不完全正确，这可能会引起公众的额外关切，所以他们不愿公开。
但是为了更好地研究这个问题，我们真的需要分享这个然后作为一个社区共同改进。
这就是我对你的问题的回答方式。
感谢你提出的一个很棒的问题。
我认为这项技术已经准备好与搜索合并了吗？
我不会说准备好了，但他们肯定需要类似这样的东西。
家庭设备。
所以我对 Delphi 的看法是，它确实可以作为其他基础模型或应用场景的过滤器，在它们即将生成一些东西时，你可以放置一个安全过滤器，这真的可以帮助到。
所以在某种意义上，在这项工作中，我进行得很快，但基本上发生的事情是，让我们看看，我们构建这个的原因是因为我们发现聊天机器人，公开可用的那些，倾向于过于积极，以至于他们想要赞同问题的情况，比如用户说，大屠杀从未发生过。
然后聊天机器人说，是的，我同意你。
如果你说，我是希特勒的铁杆粉丝，那么聊天机器人可能会说，是的，是的，是的。
用户可能会说，我感到很沮丧，我要自杀。
然后聊天机器人说，继续，好主意。
所以积极并不等于无害。
对于有问题的内容保持积极可能会非常有毒，也会非常有害。
因此，像 Delphi 这样的发展，即使 Delphi 远非完美，它也存在偏见，有西方偏见，但它确实可以帮助下游模型。
是的，所以继续谈论那个问题，使用类似 GPT 的模型在搜索中存在许多担忧，因为误导，哦，那是另一个麻烦。
其他人说我们只需要更多的 RLHF 加知识图谱。
因此，误导似乎是另一件我们正在落后的事情，因为我们还没有非常强大的事实检查模型。
所以这是另外一个故事。
但是即使抛开那些，仅仅从对人们安全和公平的规范和道德来看，我认为 RLHF 的方向很好，但他们通常也需要人类示范，而不仅仅是人类反馈。
而且，问题在于科技公司拥有它们，而且没有人分享任何东西。
这使得作为一个共同体取得有意义的进步变得非常困难。
所以我确实认为数据非常重要。
现成的模型无法自己学习模型和道德。
它必须以更直接的方式进行教导。
我们真的只需要在这个领域做更多的研究，这是我对此的看法。
那有道理。
我们在Slator上还有一些问题，所以我可以为你们询问。
所以一个问题是，进行提示的复杂性是多少？
语言模型需要查询多少次？
是的，老实说，有点慢。
事实上，这个Delphi混合模型也很慢。
如果你试图进行这种图形推理，哦，也许我不打算这么做。
但图形推理很慢，因为你不得不一遍又一遍地调用。
而且有些情况不能批处理，特别是如果是递归的话。
但我会说思维的链条也慢了一点。
最大集求解器本身非常快，因为这是一个非常简单的图。
所以会有一些延迟，但速度会慢一些，但或许不会太糟糕是我应该说的。
好的。
问题是，Comet与GPT-3相比，如果GPT-3在常识数据上进行了精调，特别是在进行某种指令精调的情况下，它们之间有何差异？
是的，那么更大的模型会获胜，结尾。
越大越好，特别是如果你只是要微调 GPT-3，那就已经游戏结束了。
因此，有些人可能会认为越大越好，因此不要使用较小的模型。
但我认为较小的模型也有两个有趣的原因值得关注。
首先，从经验上讲，使用起来更容易。
但更重要的是，如果你能让较小的模型变得更好，并赶上较大的模型，这也是非常有趣的。
就我个人而言，我认为尺寸方面有一些东西，更多的是关于信息复杂性，这是关键原因。
我不认为只是尺寸的问题，如果你有很多重复而简单的数据，可能你不会得到同样多的性能提升，这基本上是我们观察到的情况，即使是松散的 GPT-3 生成了比关键老师更多的数据，但这里数据的质量比数量更重要。
因此，我认为数据本身的复杂性比尺寸更重要。
而且通常当你只是增加数据的大小和模型时，你确实增加了数据信息的复杂性以及模型学习复杂性的能力。
但是，如果我们能够跟上信息的复杂性，无论是通过推理算法还是更好的数据，那么我们就可以缩小差距相当大，这在智力上是非常有趣的研究领域。
很酷。
好的。
这是一个个人问题，但我想说人类通常有一个批判模型。
所以就像，在说话之前，我们不仅仅生成，还会思考这是一个好事还是一个坏事。
所以人们一直在关注基因模型，比如净十亿大小的参数，但我们是否也应该关注能够进行事实检查的大型批判模型，以及很多这方面的事情？
那么你对此有什么看法呢？
很好的观点。
太棒了。
是的，我认为我们可以在批判模型上投入更多，因为它们与生成模型很好地配合，可以使输出更好或更好地过滤输出。
而且，对此的投资并不多。
所以我真的很喜欢这个问题，或者说是对研究界的建议更像是。
好的。
是的。
我会说，哦，让我想想。
是的。
你有一些更多的问题我可以在最后一个上回答。
让我们看看。
我想一个是，你是否认为语言模型应该完全避免涉及道德和伦理的问题，类似于像限制开放空气的聊天 GPT 不发表意见？
是的。
我其实一点都不介意AI完全避开所有这些，除非有人在说道德上可疑的事情。
AI最好也不要陷入其中。
或者至少要认识到这是不可取的，然后尽量降低它。
但我认为AI实际上没有任何特殊的理由直接回答道德问题在更多的下游用例中。
但这个 Delphi 的目标实际上是使所有这些判断更加明确，以便我们可以更明确地研究它，而不是保持一切都如此隐晦。
好的。
最后一个问题。
那么你认为常识是一种在语言模型中出现的属性吗？
哦，是的。
是的。
这绝对是紧急的。
就像当我们看到GPT-3性能大幅提升时，我确实认为这是一种新的能力。
但我不认为...
所以这个特定的评估方式并不具有对抗性。
这就像是一块蛋糕，你知道，相当容易的评估场景。
关于常识的事情是，它可以是如此对抗性的，有无数种不同的方式。
然后，你知道，总是有像Gary Marcos这样的人，他想出非常奇怪的攻击场景，比如，你知道，如何把破碎的瓷器加到奶中可以支持婴儿的消化系统，然后和PT-3对话说胡话。
所以，常识的常见问题是这种对抗性的情况，人们在这种情况下很容易被愚弄，尽管，你知道，我们第一次听到这个问题时没有任何问题，因为我们有一个真正的概念性理解，这是我们常识理解的支柱。
但这在transformer的设计方式上确实缺乏重视，它们专注于预测接下来会出现哪个词，而不是学习世界知识。
在某种意义上，你知道，现在通过RLHF，我们不是在预测下一个词是什么，而是试图更好地使模型输出与人类偏好对齐。
但这又不太符合让我们理解世界然后建立知识模型的不同目标。
所以这些都是不同的学习目标。
而且，我相信尽管常识确实源自语言模型，但从根本上讲，语言模型并不等同于知识模型，我们真的要专注于构建知识模型。
我想有一个最后的问题。
哦，价值多元主义。
是的。
这是一个空洞的概念。
你不想包含所有的价值体系。
是的。
所以也许它就是一个价值观。
是空的还是不是？
好的，谢谢你提出的优秀问题。
所以我认为我们不应该完全支持阴谋论，或任何其他道德上可疑的情况。
但是仍然存在这种棘手的情况，如何处理左翼人士与轻微左翼人士与右倾人士，如果在美国，每个国家也有其他政治分裂。
所以在这里，我觉得我们真的需要弄清楚该怎么做。
但不管遇到了一些挑战，事实上我个人并没有宗教，但我尊重有宗教信仰的人。
我也尊重有不同文化背景的人。
而且我们对彼此应该尊重多少有一定的认识，尽管信仰不同。
所以我们可能需要共同努力。
而且这个决定不应该仅仅由人工智能研究人员来做，顺便说一句。
这个决定必须来自人文学科的整体，这也是为什么数据共享实际上很重要。
但基本上，我认为我心目中的当前版本是，人工智能不需要理解什么样的差异是可以接受的差异。
人们在某些问题上有差异是事实，这应该被人工智能学习到，所以应该有不同意见的分布，而不是一个正确答案。
然后它应该否认一些争议理论，尽管我肯定有些人会对此非常不满。
但我们必须决定类似这样的事情。
我相当乐观地认为，如果人文学科共同努力，我们是可以做到的，因为毕竟，法律也是如此。
所以法律，这是人类共同商定的人类产物。
有这些核心规则，人们应该遵守。
所以我希望我们也可以定义普遍性和特殊性，并在恰当时尊重特殊性。
否则，我们有一些基本的普世价值观。
至于这种左倾情况，顺便说一句，如果目标是使您的AI系统对任何人都安全，实际上，我们可以使AI过滤器极其注重公平。
这样做并不会违反言论自由。
只需让AI避免说一些可能对某些人群构成微侮辱的话。
这样做并不会真正排斥那些更关心言论自由而不是公平的人。
所以我认为有办法，但这确实需要更多的研究，这是我对此的看法。
我想这大致就是这样了。
非常感谢你的到来。
这是一次很棒的交流。
好的，非常感谢。
非常感谢你，雅津。
