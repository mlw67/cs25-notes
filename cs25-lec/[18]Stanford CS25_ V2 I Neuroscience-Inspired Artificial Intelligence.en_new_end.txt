Hello.
It's fun to be here.
So the work I'm presenting today, title of it is Attention Approximates Sports Distributed Memory.
And this was done in collaboration with Genghis Peledon and my PhD advisor is Gabriel Kreiman.
So why should you care about this work?
We show that the heuristic attention operation can be implemented with simple properties of high dimensional vectors in a biologically plausible fashion.
So the transformer and attention, as you know, are incredibly powerful, but they were heuristically developed.
And the softmax operation in attention is particularly important, but also heuristic.
And so we show that the intersection of hyperspheres that is used in sports distributed memory closely approximates the softmax and attention more broadly, both in theory and with some experiments on train transformers.
So you can see SCM sports to read memory as preempting attention by approximately 30 years, it was developed back in 1988.
And what's exciting about this is that it meets a high bar for biological possibility.
Hopefully I have time to actually get into the wiring of the cerebellum and how you can map each operation to part of the circuit there.
So first, I'm going to give an overview of sports distributed memory.
Then I have a transformer attention summary, but I assume that you guys already know all of that.
We can get there and then decide how deep we want to go into it.
I'll then talk about how actually attention approximates SDM, interpret the transformer more broadly, and then hopefully there's time to go into SDM's biological possibility.
Also, I'm going to keep everything high level visual intuition and then go into the math, but stop me and please ask questions literally whenever.
Okay.
So sports distributed memory is motivated by the question of how the brain can read and write memories in order to later retrieve the correct one.
And some considerations that it takes into account are high memory capacity, robustness to query noise, biological possibility, and some notion of fault tolerance.
SDM is unique from other associated memory models that you may be familiar with, like Hopfield networks, in so much as it is sparse.
So it operates in a very high dimensional vector space and the neurons that exist in this space only occupy a very small portion of possible locations.
It's also distributed.
So all read and write operations apply to all nearby neurons.
It is actually as a side note, Hopfield networks, if you're familiar with them are a special case of sparse distributed memory.
I'm not going to go deep into that now, but I have a blog post on it.
Okay.
So first we're going to look at the write operation for sparse distributed memory.
We're in this high dimensional binary vector space.
We're using Hamming distance as our metric for now.
We'll move to continue with later.
And we have this green pattern which is represented by the solid dot and the hollow circles are the hypothetical neurons.
Also think of everything quite abstractly and then we'll map the biology later.
So this pattern has a red radius, which is some Hamming distance.
It activates all of the neurons within that Hamming distance.
And then it's here, I just note that each of those neurons are now storing that green pattern and the green pattern has disappeared.
So I'm keeping track of its location with this kind of fuzzy hollow circle.
That'll be relevant later.
So we're writing in another pattern, this orange one.
And note here that neurons can store multiple patterns inside of them.
And formally, this is actually a superposition or just a summation of these high dimensional vectors.
Because they're high dimensional, you don't have that on a crosswalk, so you can get away with it.
But for now, you can just think of it as like the neuron can store multiple patterns.
Finally, we have a third pattern, this blue one.
We're writing it in another location.
And yeah, so again, we're keeping track of the original pattern locations, but they can be triangulated from the nearby neurons that are storing them.
And so we've written in three patterns.
Now we want to read from the system.
So I have this pink star xi.
It appears it has a given, it's represented by a given vector, which has a different location than space.
It activates nearby neurons again.
But now the neurons output the patterns that they stored previously.
And so you can see that based upon its location, it's getting four blue patterns, two orange and one green.
And it then does a majority rule operation where it updates towards whatever pattern it seems to us.
So in this case, because blue is actually a majority, it's just been updated completely towards blue.
Again, I'll formalize this more in a bit.
But this is really to give you intuition for the core operations of STM.
So the key thing to relate this back to attention is actually to abstract away the neurons that are operating under the hood, and just consider the circle intersecting.
And so what each of these intersections between the pink lead circle and each of the right circle means is that intersected the neurons that both stored that pattern that was written in, and are now being read from by the query.
And the size of that intersection corresponds to how many patterns the query is then going to read.
And so formally, we define the number of neurons in this circle intersection as the cardinality between number of neurons and pattern number of neurons and query and their intersection.
Okay, are there any questions?
Like at a high level before I get more into the math?
I don't know if I can check.
Is it easy for me to check zoom?
Nah, sorry, zoom people.
I'm not going to check.
Okay.
But the neurons is randomly distributed in this.
Yes, yeah.
And there's later there's more recent work that they can learn and update their location to cut tile manifold.
But in this, you can assume that they're randomly initialized binary high dimensional vectors.
Okay, so this is the full STM update rule.
I'm going to break it down.
So the first thing that you do, so this is this is for reading, to be clear.
So you've already written patterns into your neurons.
So the first thing you do is you weight each pattern by the size of its circle intersection.
So the circle intersection there for each pattern, then you sum over all of the patterns that have been written into this space.
So you're just doing a weighted summation of them.
And then there's this normalization by the total number of intersections that you have.
And finally, because at least for now, we're working in this binary space, you map back to binary, just seeing if any if the values are greater than half.
Okay, how familiar are people with attention?
I looked at like the previous talks you've had, they seem quite high level.
Like, can you guys write the attention equation for me?
Is that like, can I get thumbs up if you can do that?
Yeah.
Okay, I'm not like, I'll go through this.
But I'll probably go through it faster than otherwise.
So when I first made this presentation, this was the state of the art for transformers, which was like, alpha pole.
And so it's kind of funny, like how far things have come now, I don't need to tell you that transformers are important.
So, yeah, I'm going to work with this example.
Well, okay, I'm going to work with this example here, the cat sat on the blank.
And so we're in this setting, we're predicting the next token, which is the word math.
And so there are kind of four things that the attention operation is doing.
The first one up here is it's generating what are called keys, values and queries.
And again, I'll get into the math in a second.
I'm just trying to keep it high level first.
And then we're going to compare our query with each of the keys.
So the word the, which is closest to the word we're next predicting is our is our query.
And we're seeing how similar is each of the key vectors.
We then, based upon that similarity, do this softmax normalization, so that all of the attention weights, some of the one and then we sum together their value vectors to use to propagate to like the next layer or uses our prediction.
And so, at a high level, you can think of this as like the query word the is looking for nouns and their associated verbs.
And so hypothetically, it has a high similarity with words like cat and sat, or their keys.
So this then gives large weight to the cat and sat value vectors, which get moved to the next part of the network.
And the cat value vector hypothetically contains a superposition of other animals like mice, and maybe words that rhyme with Matt.
And so, and the sat vector also contains things that are sat on including that.
And so what you actually get from the value vectors of paying attention to cat and sat are like three times Matt plus one times mouse plus one time sofa.
This is again, like a totally hypothetical example.
But I'm trying to make the point that you can extract from your value vectors, things useful for predicting the next token by paying attention to specific keys.
So and I guess, yeah, another thing here is like what you pay attention to.
So cat and sat might be different from what you're actually extracting, you're paying attention to your keys, but you're getting your value vectors out.
Okay, so here is the full attention equation, the top line, I'm separating out the projection matrices, w subscript v and q, and the second one, I've just collapsed them into like the equation you only see.
And yeah, so breaking this apart, the first step here is we compare we do a dot product between our query vector and our keys.
This should actually be a small.
And so, yeah, we're doing this dot product between them to see get a notion of similarity.
We then apply the softmax operation, which is an exponential over a sum of exponentials.
The way to think of the softmax is it just makes large values larger.
And this will be important for the relation to SCM.
So I'll spend a minute on it.
At the top here, I have like some hypothetical items indexed from zero to nine, and then the values for each of those items.
In the second row, I just do like a normal normalization of them.
And so the top item goes to a 30 % value.
But if I instead do a softmax, and it depends on the beta coefficient of the softmax, but the value becomes 0 .6.
So it just it makes your distributions peakier is kind of one way of thinking of it.
And this is useful for attention because you only want to pay attention to the most important things or the things that are nearby and kind of ignore stuff further away.
And so once we've applied our softmax, we then just do a weighted summation of our value vectors, which actually get extracted and propagate to the next layer.
Okay, so here's the full equation.
I went through that a little bit quickly.
I'm happy to answer questions on it.
But I think half of you know it, half of you don't.
Okay, so how does transformer attention approximate sparse distributed memory, this 30 year old thing that I've said is biologically plausible?
So yeah.
So I'm going to get to that at the end.
Yeah.
I think the attention equation I'm showing here was developed.
I mean, attention is all you need was the highlight, but Benjio has a paper from 2015 where it was actually first written in this way.
Correct me if I'm wrong, but I'm pretty sure.
That's why I was asking the question because like, no, it's a good question.
Like you show that like two different methods that could be classified as like attention proposals, right?
Or like the same, like one of them is like, yeah.
Yes, exactly.
So I'll show that SDM has really nice mappings to a circuit in the cerebellum at the neural level.
And then there's right now it's this link to attention.
And I guess you make a good point that there are other attention mechanisms.
This is the one that has been dominant, but I don't think that's just a coincidence.
Like there's been a bunch of computing your softmax is expensive and there's been a bunch of work like the Lin former, et cetera, et cetera, that tries to get rid of the softmax operation.
And it's just done really badly.
Like there's a bunch of jokes on Twitter now that it's like a black hole for people to like, try and get rid of softmax and you can't.
And so it seems like this and like other versions of it, transformers just don't scale as well in the same way.
And so there's something important about this particular attention equation.
But like that goes the other way, right?
Which is like, this is really important than like SDM is like actually like this.
So the thing that I think is important is that you have this exponential waiting where you're really paying attention to the things that matter and you're ignoring everything else.
And that is what SDM approximates.
There might be equations, but the point I was just trying to make is like the softmax does seem to be important and this equation does seem to be very successful and we haven't come up with better formulations for.
Yeah, no, that's a great question.
Okay.
So it turns out that sparse distributed memory, as you move your query and your pattern away from each other, so you pull these circles apart, the read and write circles, the number of neurons that are in this intersection in a sufficiently high dimensional space decays approximately exponentially.
And so on this right plot here, I'm pulling apart, the x -axis is me pulling apart the blue and the pink circles.
And the y -axis is on a long scale, the number of neurons that are in the intersection.
And so to the extent that this is a linear plot on a long scale, it's exponential.
And this is for a particular setting where I have 64 dimensional vectors, which was used in UB2.
It holds across a lot of different settings, particularly higher dimensions, which are now used for bigger transformers.
Okay.
So I have this shorthand for the circle intersection equation.
And what I'll show is how the circle intersection is approximately exponential.
So we can write it in two constants, C, circle one and circle two.
With the one outside, because you're normalizing softmax's exponential over some of the exponentials, that would cancel.
The thing that matters is C2, and you can approximate that nicely with the beta coefficient that's used in the softmax.
And so, yeah, I guess as well, I'll focus first on the binary original version of SDM, but then we also develop a continuous version.
Okay.
So yeah, the two things that you need for the circle intersection and the exponential decay to work are you need to map it to attention is you need some notion of continuous space.
And so you can use this equation here to map Hamming distances to discretize percent similarity values, where the hats over the vectors are L2 normalizations.
And you can then write the circle intersection equation on the left as this exponential with these two constants that you need to learn.
And then rewrite this by converting C2 to C. You can write this as a beta coefficient.
Let me get to some plots.
Yeah.
So you need the correct beta coefficient, but you can fit this with a log linear regression in a closed form.
I want to show a plot here.
Yeah.
Okay.
So in the blue is our circular section for two different Hamming distances, both using 64 dimensional vectors.
And the orange is our actual softmax attention operation where we fit the beta coefficient.
So that it will, it will, the Hamming distance used by attention is equivalent to the Hamming distance.
So that the main plot is the normalized weights.
So just summed up and then divided.
So there's number one.
And then I have log plots here.
And you can see that in not log space, the curves agree quite nicely.
You can see that for the higher dimensional, sorry, the larger Hamming distance, the log plot, you see this drop off here where the circle intersection stops being exponential.
But it turns out this actually isn't a problem because the point at which the drop, the exponential breaks down, you're at approximately 0 .20 here.
And you're basically paying negligible attention to any of those points.
And so in the regime where the exponential really matters, this approximation holds true.
Yeah.
Yeah.
Yeah.
Yeah.
No, I just wanted to actually like show up here to get some question before.
Yeah.
So all we're doing here is we're just, we're in a binary space with original XM.
And we're just using this mapping to go time similarity.
And then what you need to do is just have the data and you can view your data coefficient and attention is determining how cheeky things are.
And this relates directly to the Hamming distance of your circles that you're using for read and write on version.
And so, yeah, to like mathematically show this now on this slide, I'm not using any tricks.
I'm just rewriting attention using the SCM notation of patterns and queries.
So this little box down here is doing that mapping.
And this is the money slide where we're updating our query.
And on the left, we have our attention equation written in SCM notation.
We expand our submax.
And then the main statement is that this is closely approximated by if we swap out our exponential with the SCM, the corner section equation.
So and again, the two things that you need for this to work are one, your attention vectors, your keys and queries need to be able to be normalized by the hats on them.
And then you want, if you decide a given Hamming distance for SCM, and I'll get into what Hamming distance are good for different things, then you need to have a data coefficient that relates to it.
But again, that's just how many things are you trying to pay attention to?
So yeah, just as a quick side note, you can write SCM using continuous vectors, and then not need this mapping to go send similarity.
And so here I have the plots again, but with this, and I've added the orange of the green of flex, but I've added the continuous approximation here too.
And what's nice about the continuous version is you can actually then write sparse distributed memory as a multilayered perceptron with slightly different assumptions.
And I'm not going to talk about that now, but this is featured in sparse three memories of continual learner, which is, was added to the additional readings.
And it'll be in, sorry, this shouldn't say ICML, this should say iClear.
It's just been accepted to iClear for this year.
Okay, so do train transformers use these beta coefficients that I've said are similar to those for SDM.
And so it shouldn't be surprising that depending on the hemic distance you set, SDM is better for certain things.
For example, you just want to store as many memories as possible, and you're assuming that your queries aren't noisy, or you're assuming your queries are really noisy, so you can't store as much, but you can retrieve from a long distance.
And if attention of the transformer is implementing sparse distributed memory, we should expect to see that the beta coefficients that the transformer uses correspond to these good instances of SDM.
And so we have some weak evidence that that's the case.
So this is the key query normalized variant of attention, where you actually learn your beta coefficient.
Normally in transformers you don't, but you don't L2 norm your vectors, and so you can kind of have this like effective beta coefficient.
So in this case, it's the cleaner instance where actually learning beta.
And this was trained on a number of different translation tasks.
We take the learning coefficients across layers and across tasks, and plot them as they hit the ground.
And the red dotted lines correspond to three different notions of sparse distributed memory that are optimal for different things.
And again, this is weak evidence in so much as to derive the optimal SDM data coefficients or corresponding kind of distances, we need to assume random patterns in this high natural space.
And like obviously real world data isn't random.
However, it is nice to see one, all of the beta coefficients fall within the bounds.
And two, they skew towards the max very noise, which makes more sense if you're dealing with like complicated real world data, where the next data points you'd see might be out of distribution based on what you seen in the past, the maximum capacity variant assumes no variables at all.
And so it's like, how many things can I pack in, assuming that my, the questions I'm asking the system are perfectly formed?
Okay, just talking a little bit about transformer components more broadly.
So I've mentioned that you can write the feed forward layer as a version of SDM that has like sort of notion of longer term memory.
There's also layer norm, which is crucial in transformers.
And it's not quite the same, but it can be related to the L2 normalization that's required by SDM.
There's also the key query normalization variant that explicitly does this L2 normalization.
And it does get slightly better performance, at least on the small tests that they did.
I don't know if this would scale to larger models.
And so I guess this work is interesting in so much as like the biological plausibility, which I'm about to get to.
And then the links to transformers, it hasn't to date improved transformer architectures.
But that doesn't mean that this lens couldn't be used or be useful in some way.
So yeah, I list a few other way things that SDM is related to that could be used to funnel in.
And actually, in the new work where SDM is continual learner, we kind of expand the circuit, look at components of it, particularly inhibitory interneurons, implement those in a deep learning model, and then becomes much better at continual learning.
So that was kind of a fun way of actually using this link to get better bottom line performance.
Okay, so a summary of this section is basically just the intersection between two hyperspheres approximates an exponential.
And this allows SDM's read and write operations to approximate attention, both in theory and our limited tests.
And so kind of like big picture research questions that could come out of this is, first, is the transformer so successful, because it's performing some key cognitive operation.
The cerebellum is a very old brain region used by most organisms, including fruit flies, maybe even cephalopods through like divergent, but now convergent evolution.
And then given that the transformer has been so successful empirically, is SDM actually the correct theory for cerebellar function?
And that's still an open question.
As we learn more and more about the cerebellum, there's nothing that yet disproves SDM as working there.
And I think it's all going on a limb and say it's like one of the more compelling theories for how the cerebellum is actually working.
Yeah.
And so I think this work kind of motivates looking at more of these questions, both of these questions more seriously.
Okay.
Do we have time?
Cool.
So here's the circuit that implements SDM.
At the bottom, we have patterns coming in for either reading or writing.
And actually, I break down each of these slides.
Okay.
Yeah.
So first, we have patterns that come in.
And every neuron here, these are the dendrites of each neuron.
And they're deciding whether or not they're going to fire for the input that comes in.
Then, if the neuron does fire, and you're writing in that pattern, then you simultaneously, and I'm going to explain to you, this is crazy, the brain doesn't do this, and I'm going to hopefully take it done.
You not only need to have the thing, but the pattern that activates neurons, but you need to have a separate line that tells the neuron what to sort.
And just like you have this difference between keys and values, where they can be different vectors, representing different things.
Here, you can have a key that comes in and tells the neuron when activated, and the value for what it should actually like soar and then later.
This is called a hetero -associative mapping.
And then, once you're reading from the system, you also have your query come in here, activate neurons, and those neurons that output whatever they soar, and the neurons vector is this particular column that it's stored.
And as a reminder, it's storing patterns in superposition.
And then it will dump whatever it's stored across these output lines.
And then you have this G majority bit operation that convert to a zero or one, decide if the neuron is going to fire or not.
And so here is the same circuit, but where I overlay cell types and so on.
And so I'll come back to this slide, because most people probably aren't familiar with cerebellar circuitry.
Let me just get some water.
Okay, so the way that the cerebellum is pretty homogenous, and that follows the pattern throughout, also comes back 70 % of all neurons in the brain are in the cerebellum.
They're small, so you wouldn't know it.
But the cerebellum is very underappreciated.
And there's a bunch of evidence that it has closed loop systems with most higher order processing now.
If your cerebellum is damaged, you are more likely to have autism, etc, etc.
So it does a lot more than just fine motor coordination, which a lot of people have assumed in the past.
Okay, so inputs come in through the MOS sliders here.
They interface with granule cells.
This is a major up projection where you have tons and tons of granule cells.
Each granule cell has what are called parallel fibers, which is incredibly long and thin axons that branch out in this T structure.
And then they're hit by the Purkinje cells, which will receive up to 100 ,000 parallel fiber inputs.
It's the highest connectivity of any neuron in the brain.
And then the Purkinje cell will decide whether or not to fire and send its outputs downwards here.
So that's the whole system where patterns come in and they're on the side of the fiber or not.
And the way that they then output their own.
You then have a separate right line, which is the climbing fibers.
So the climbing fibers come up, and they're pretty amazing in that these connections here you can go over them as important.
One that really matters is that they're not very strong in there.
One that really matters is it goes up and wraps around individual Purkinje cells.
And the mapping is close to one to one between climbing fibers and Purkinje cells, at least a very strong action detector.
And so they're connected to us here.
As in the stuff off the side?
Yeah, right.
It's two lines.
They're connected to us.
Oh, so they're separate neurons coming from in separate areas.
Purkinje is close to your go into the cerebral nuclei, kind of in the core of the sarvone.
And that then feeds into the LMS, like back to higher order brain regions, or like down to muscle movement, etc.
You will think of the cerebellum as kind of like a fine tuning lookup table, where like you've already decided the muscle movement you want to do, but the cerebellum will then like do a bunch of adjustment, so that it's like much more accurate.
But it seems like this also applies to like next word prediction.
Like we have an MRI data for this.
A neuroscientist once made a dirty little secret of fMRI is that the cerebellum lights up for everything.
So okay, going back to this circuit here, then.
Yeah, time scales are the operating yet?
I mean, how long is the information stored and retrieved?
Do we have any idea about this?
Like, just like a couple milliseconds, or like information worker system?
So the main theory is that you have updating through time dependent plasticity, where you're finding fiber will either which is doing the one right in will fire either just before or just after your granul cells fire.
And so that then updates the contingency cell synapses for long term depression or potentiation.
So whatever time scale that's happening on the fiber makes very large after the pencils, or at least a very large after the pencils.
And so I do think you could get pretty fast and epic updates.
And they're also persistent for a long time.
I think so.
Yeah, the science has been staying for like the rest of your life.
Yeah.
So what's really unique about this circuit is the fact that you have these two orthogonal inputs, where you have the muscle fibers bringing information in to decide if the neurons can fire or not, but then the totally separate fiber lines that can update specific neurons and what they're storing will later upload.
And then the contingency cell is so important as kind of doing this cooling across every single neuron.
And each neuron, remember, storing a vector this way.
And so the contingency cell is doing element wise summation, and then deciding whether it fires or not.
And this allows for you to store your vectors in superposition.
And then later, you know, it's not this is all this the theory of SDM maps quite well to the Marr and Hollis theories of solar velar function, which are still quite dominant, if anyone's familiar with this.
Yeah, so the analogy of the neuron in the SDM you introduced before that, like, each neuron of the contingency cell, instead of each neuron is a grandness.
And then yeah, so the the location of the neuron, those hollow circles, corresponds to the grandness of them right here, where the patterns that pop in correspond to the activations and modifiers.
And then the efforts, postsynaptic connections are with the contingency cell.
So that's actually it's what it's storing is in the set of connections with the end prokigi cells at that interface.
And then the prokigi cell does the majority bit operation and deciding if it wants to fire or not.
Are we into questions?
Yeah, yeah, I think we're basically into into question time.
So yeah, thanks a lot.
I have a question.
I don't know anything about SDM, but it seems as understood it's very good for long term memory.
And I am curious, what's your hypothesis of what we should be doing for short term memory?
Because it seems that, so if you have this link of transformers, having long term memory, what's good for short term memory?
Because for me, it seems like we are doing this in the prompt context right now.
But how would we incorporate these two directly?
Yeah, yeah.
So this work actually focuses more on the short term memory, where it relates to the attention operation.
But you can rewrite SDM, it's almost more natural to interpret it as a multilayer perceptron that does like a softmax activation across its or a top activation across it some neurons.
It's like a little bit more complicated than that.
But yeah, so so the Yeah, the most interesting thing here is the fact that I just had a bunch of neurons.
And in activating nearby neurons in this high dimensional space, you get this exponential weighting, which is the softmax.
And then because it's an associative memory where you have keys and values, it is attention.
And yeah, I guess like, the thing I most want to drive home from this is like, it's actually surprisingly easy for the brain to implement the attention operation, the attention equation, just using high dimensional vectors and activating.
So it's good for sure.
Yes, if you were to actually use SDM for attention.
Yeah, so let me go all the way back real quick.
This is important.
There are kind of two ways of viewing SDM.
And I don't think you were here for the talk.
I think I saw you come in a bit later, which is totally fine.
But Oh, cool, cool, cool.
Yeah.
Okay.
So, so there are two ways of looking at SDM.
There's the neuron perspective, which is this one here.
And this is actually what's going on in the brain, of course.
And so the only thing that is constant is the neurons, the patterns are ephemeral.
And then there's the pattern based perspective, which is actually what attention is doing.
And so here, you're abstracting away the neurons, we're assuming they're operating over the hood.
But what you're actually computing is the distance between the true location of your pattern and the query.
And there are pros and cons to both of these.
The pro to this is you get much higher fidelity distances, you know exactly how far the query is from the original patterns.
And that's really important when you're deciding what to update towards, like you really want to know what is closest and what is further away, and you will apply the exponential weighting correctly.
The problem is you need to store all your pattern notifications in that way.
And so this is why transformers have limited context windows.
The other perspective is this long term memory one, where you forget about the patterns, and you just look at where you just have your neurons that store a bunch of patterns in them in this noisy superposition.
And so you can't really you can tiny like really big pattern ones like not quite, it's all much noisier.
But you can store tons of patterns and you're not concerned by a complex window.
Or you can think of any penalty layer as storing like the entire data set in a noisy superposition of states.
Yeah, hopefully that kind of answers your question.
I think there's one here first.
Yeah.
So I guess my question is like, so I guess like, you kind of showed that like the modern self attention mechanism maps onto like this like SDM mechanism that like seems possible and like some of like the modern theories of like how the brain implement SDM.
And I guess my question is like, to what degree has that like been experimentally verified versus like you were like mentioning earlier, that like it might actually be easier to have done this using like a MLP layer in some sense than like a bunch of these like mechanisms.
And so like, how do experimentalists like actually distinguish between competing with hypotheses?
Yeah, like, for instance, like one thing that like I wasn't entirely clear about is like, like, even if like the brain could do attention, or like SDM, like that doesn't actually mean it would because like, it can't do back off.
Right?
Yeah.
Um, so like, I mean, how does this like get actually tested?
Totally.
Yeah.
So on the backdrop point, you wouldn't have to do it here because you have the climbing fibers that can directly like give training signals to the store.
So in this case, you it's like a supervised learning task for the funding partner knows what it wants to write in, how it should be updated, the protein B cell synapses.
But for your broader point, you basically need to do to test this, you need to be able to do real time learning.
The Drosophila mushroom body is basically identical to the servon.
And the fly in the brain data set has done most of the individual neuron connectivity.
So what you would really want to do is like in vitro, real time, super, like, super, super high frames per seconds, calcium imaging, and be able to see how synapses change over time.
And so for an associative learning task, like, here are sound moves left here, another sound move right or smells or whatever, present one of those trace, like figure out the small subset of neurons that fire, which we know is a small subset, that are fixed with the hand See how the synapses here update, and how the outputs of that correspond to changes in motor action.
And then extinguish that memory.
So right in a new one, and then watch it watch it go away again.
And like, our cameras are getting fast enough, and like our calcium and like voltage indicators are getting to be really good.
So hopefully, in the next like, three to five years, we can do some of those tests.
But I think that would be very definitive.
Yeah.
We have no questions.
I think there was one more and then I should go to one more.
In terms of how you map the neuron based on this final biological implementation, like, what is the, what is the range of your circle that you're mapping around?
I'm trying to understand how that is.
Yeah, so I wouldn't get confused with multi headedness.
Because that's different attention heads all doing their own attention operation.
It's funny, though, the circle has micro zones, which you can think of as like separate attention heads in a way.
I don't want to take that analogy too far and like, but but it is it is somewhat interesting.
So the way you relate this is in attention, you have your beta coefficient, that is an effective beta coefficient, because the vector norms of your queries aren't constrained.
That corresponds to a hemigresence.
And here that corresponds to the number of neurons that are on for any given input.
And the hemigresence you want, I had that slide before, the hemigresence you want to depends upon what you're actually trying to do.
And if you're not trying to store that many memories, for example, you get a higher hemigresence because you can get a higher fidelity calculation for the number of neurons in that noisy intersection.
Yeah.
Cool.
Yeah, thanks a lot.
So as a disclaimer, so as a disclaimer, before introducing the next speaker, the person was scheduled unfortunately, the council last minute due to faculty interviews.
So our next speaker has very graciously agreed to present at very last minute.
But we are very grateful to him.
So I'd like to introduce everybody to Will.
So Will is a computational neuroscience machine learning PhD student at the University College of London at their Gatsby unit.
So I don't know if anybody has heard about the Gatsby unit.
I'm a bit of a history buff, or history nerd, depending on how you phrase it.
The Gatsby unit was actually this incredible powerhouse in the 1990s and 2000s.
So Hinton used to be there, Zubin Garamani used to be there.
He's now in charge of Google Research.
I think they've done a tremendous amount of good work.
Anyways, and now I'd like to invite Will to talk about how to build a cognitive map.
Did you want to share your screen?
Yeah.
Okay.
Can you stand in front of, yeah, let me stop sharing.
Okay.
So I'm going to be presenting this work.
It's a, it's all about how a model that people in the group that I work with to study the hippocampal entorhinal system completely independently turned out to look a bit like a transformer.
So that's this paper that I'm going to talk about is describing that link.
So the paper that built this link is by these three people.
James is a postdoc, Harp at Stanford, Tim's a professor at Oxford and in London, and Joe's a PhD student in London.
So this is the problem that the, this model of the hippocampal entorhinal system, which we'll talk more about, is supposed to solve.
It's basically the observation.
There's a lot of structure in the world, and generally we should use it in order to generalize quickly between tasks.
So the kind of thing I mean by that is, you know how 2D space works because of your long experience living in the world.
And so if you start at this greenhouse and step north, so this orange one, then to this red one, then this pink one, because of the structure of 2D space, you can think to yourself, oh, what will happen if I step left?
And you know that you'll end up back at the green one because loops of this type close in 2D space.
Okay.
And this is, you know, perhaps this is a new city you've just arrived in.
This is like a zero shot generalization because you somehow realized that the structure applies more broadly and use it in a new context.
Yeah.
And there's generally a lot of these kinds of situations where there's structures that like reappear in the world.
So there can be lots of instances where the same structure will be useful to doing these like zero shot generalizations to predict what you're going to see next.
Okay.
And so you may be able to see how we're already going to start mapping this onto some kind of sequence prediction task that feels a bit transform -esque, which is you receive this sequence of like observations and in this case actions, movements in space.
And your job is given a new action step left here, you have to try and predict what you're going to see.
So that's the kind of sequence prediction version of it.
And the way we're going to try and solve this is based on factorization.
It's like you can't go into one environment and just learn from the experiences in that one environment.
You have to separate out the structure and the experiences you're having so that you can reuse the structural part, which appears very often in the world.
So yeah, separating memories from structure.
And so, you know, here's a separation of the two.
We have our dude wandering around this like 2D grid world.
And you want to separate out the fact that there's 2D space and it's 2D space that has these rules underlying it.
And in a particular instance in the environment that you're in, you need to be able to recall which objects are at which locations in the environment.
Okay.
So in this case, it's like, oh, this position has an orange house, this position has a green, sorry, orange, red and pink.
And so you have to bind those two.
You have to be like, whenever you realize that you're back in this position, recall that that is the observation you're going to see that.
Okay.
And so this model that we're going to build is some model that tries to achieve this.
Yeah.
New stars.
And so when you enter, imagine you enter a new environment with the same structure, you wander around and realize it's the same structure.
All you have to do is bind the new things that you see to the locations and then you're done, passed up.
You know, you know how the world works.
So this is what neuroscientists mean by a cognitive map is this idea of like separating it out and understanding the structure that you can reuse in new situations.
And yeah, this model that was built in the lab is a model of this process happening of the separation between the two of them and how you use them to do new inferences.
And this is the bit that supposed to look like a transport, but the general introduction and then we'll dive into it a little more now.
It makes sense though, the broad picture silence I'll assume is good.
So we'll start off with some brain stuff.
So there's a long stream of evidence from spatial navigation, but the brain is doing something like this.
I mean, I think you can probably imagine how you yourself are doing this already when you go to a new city or you're like trying to understand a new task that has instructions you recognize from previously, you can see how this is something you're probably doing.
But spatial navigation is an area in neuroscience, which had like a huge stream of discoveries over the last like 50 years.
And a lot of evidence of the neural basis of this computation.
So we're going to talk through some of those examples.
The earliest of these are psychologists like Tolman, who were showing that rats in this case can do this kind of path integration structure.
So the way this work is they got put at a start position here down at the bottom S and they got trained that this route up here got you a reward.
So this is the maze we had to run around.
Then they were asked, they were put in this new, the same thing, but they blocked off this path that takes this long winding route and given instead a selection of all these arms to go down.
And they look at which path the rat goes down.
And the finding is that the rat goes down the one that corresponds to heading off in this direction.
So the rat has somehow not just learned like a, you know, one option of this is it's like blind memorization of actions that I need to take in order to route around instead.
No, it's learning actually that the struck in embedding the reward and its understanding of 2d space and taking a direct route that even though it's never taken before there's evidence that rats are doing this as well as us.
And then a series of like neural discoveries about the basis of this.
So John O 'Keefe stuck an electrode in the hippocampus, which is a brain area we'll talk more about and found these things called place cells.
So what I'm plotting here is each of these columns is a single neuron and the mouse or rat, I can't remember, is running around a square environment.
The black lines are the path the rodent traces out through time and you put a red dot down every time you see this individual neuron spike.
And then the bottom plot of this is just a smooth version of that spike rate.
So that firing rate, which you can think of as like the activity of a neuron and neuron that works.
That's the analogy that people usually draw.
And so these ones are called place cells because they're neurons that respond in a particular position in space.
And in the 70s, this was like huge excitement, you know, and people have been studying mainly like sensory systems and motor output, and suddenly a deep cognitive variable place, something you never, you don't have a GPS signal, but somehow there's this like signal for what looks like position in the brain in very like understandable way.
The next step in the biggest step, I guess, in this chain of discovery is the Moser lab, which is a group in Norway.
They are a different area of the brain, the medial entorhinal cortex.
And so this is the hippocampal entorhinal system we're going to be talking about.
And they found this neuron called a grid cell.
So again, the same plot structure that I'm showing here, but instead these neurons respond not in one position of room, but at a trying like a hexagonal lattice of positions in a room.
Okay.
So this these two, I guess I'm showing to you because they like really motivate the underlying neural basis of this kind of like spatial cognition, and better embodying the structure of this space in some way.
Okay.
That's very surprising finding why, why neurons choosing to represent things with this hexagonal lattice.
It's like, yeah, provoked a lot of research since.
And broadly, there's been like many more discoveries in this area.
So there's place cells, I've talked to you about grid cells, cells that respond based on the location of not yourself, but another animal, cells that respond when your head is facing a particular direction, cells that respond to when you're a particular distance away from an object.
So like I'm one step south of an object, that kind of cell, cells that respond to reward positions, cells respond to vectors to boundaries, cells that respond to like all sorts, all kinds of structure that this, this, this pair of brain structures, the hippocampus here, this red area, and the entorhinal cortex, this blue area here, which is conserved across a lot of species are represented.
There's also finally one finding in this that's fun is they did an fMRI experiment on London taxi cab drivers.
And I don't know if you know this, but the London taxi cab drivers, they do a thing called the knowledge, which is a two year long test where they have to learn every street in London.
And the idea is that the tests go something like, oh, there's a traffic jam here and a road work here.
And I need to get from like Camden town down to Wandsworth in the quickest way possible.
What route would you go?
They have to tell you which route they're going to be able to take through all the roads and like how they would replant if they found a stop there.
So it's like intense.
You see them like driving around sometimes learning all of these like routes with little maps.
They're being made a little bit obsolete by Google maps, but you know, luckily they got them before that this experiment was done before that was true.
And so they've got here is a measure of the size of your hippocampus using fMRI versus how long you've been a taxi cab driver in months.
And the claim is basically the longer you're a taxi cab driver, the bigger your hippocampus, because the more you're having to do this kind of spatial reason.
So that's a big set of evidence that these brain areas are doing something to do with space.
But there's a lot of evidence that there's something more than that, something non -spatial going on in these areas.
Okay.
And we're going to build these together to make the broader claim about this like underlying structural inference.
And so I'm going to talk through a couple of those.
The first one of these is a guy called patient HM.
This is the most studied patient in medical history.
He had epilepsy and to cure intractable epilepsy, you have to cut out the brain region.
That's causing these like seizure like events in your brain.
And in this case, the epilepsy was coming from the guy's hippocampus.
So they bilaterally lesions, his path to the campus, they got cut out both of his hippocampus.
And it turned out that this guy then had terrible amnesia.
He never formed another memory again.
And it could only recall memories from a long time before the surgery happened.
Okay.
So experiments showed a lot of this stuff about how we understand the neural basis of memory, things like he could learn to do motor tasks.
So somehow the motor tasks are being analyzed.
For example, they gave him some very difficult motor coordination tasks that people can't generally do, but can with a lot of practice.
And he got very good at this eventually, and was as good as other people at learning to do that.
He had no recollection of ever doing the task.
So he'd go into the do this new task and be like, I've never seen this before.
I have no idea what you're asking me to do.
I need to do it.
Amazing.
Yeah.
So there's some evidence there that the hippocampus is involved in some parts of memory, which seems a bit separate to this stuff about space that I've been talking to you about.
The second of these is imagining things.
So this is actually a paper by Demis Asabis, who before he was deep mind head with a neuroscientist.
And here, maybe you can't read that.
I'll read some of these out.
You're asked to imagine you're lying on a white sandy beach in a beautiful tropical Bay.
And so the control, this bottom one says things like it's very hot and the sun is beating down on me.
The sound underneath me is almost unbearably hot.
I can hear the sounds of small wavelets laughing on the beach.
The sea is gorgeous, aquamarine color, you know, like so nice, lucid description of this beauty scene.
Whereas the person with a hippocampal damage says, as for seeing I can't really apart from just the sky, I can hear the sound of seagulls under the sea.
I could feel the grain of sand beneath my fingers.
And then like, yeah, the struggles are basically really struggles to do this.
Imagine and imagine some of the things really surprising.
So the last of these is this transitive inference task.
So transitive inference, A is greater than B, B is greater than C, therefore A is greater than C.
And the way they convert this into a rodent experiment is you get given two pots of food that have different smells.
And your job is to go to the pot of food, you learn which pot of food has sorry, which pot with the smell has the food.
And these are colored by the two pots by their smell and B, and the rodent has to learn to go to a particular pot.
In this case, the one that smells like A.
And they do two of these, they do A has the food when it's presented in a pair with B, and B has the food when it's presented in a pair with C.
And then they test what do the mouse do when presented with A and C a completely new situation.
And if they have a hippocampus, they'll go for A over C, they'll do transitive inference.
If they don't have one, they can't.
And so there's a much more broad set.
This is like, I've shown you how the campus is used for this spatial stuff that people have been excited about.
But there's also all of this kind of relational stuff, imagining new situations, some slightly more complex story here.
The last thing I'm going to do is how the entorhinal cortex as well.
So that's where if you remember hippocampus with these guys, entorhinal cortex with these grid cells, with how entorhinal cortex is apparently some broader stuff as well.
This is all motivation for the model, just trying to build all of these things together.
So in this one, this is called stretchy birds task.
Okay.
So you put people in an fMRI machine and you make them navigate, but navigate in bird space.
And what bird space means is it's a two dimensional space of images.
And each image is one of these birds.
And as you vary along the X dimension, the birds legs get longer and shorter.
And as you vary along the Y direction, the bird's neck gets longer and shorter.
Okay.
And the patients sit there, or subjects sit there and just watch the bird images change so it traces out some part in 2D space, but they never see the 2D space.
They just see the images.
And the claim is basically, and then they're asked to do some navigational tasks.
They're like, oh, whenever you're in this place in 2D space, you show Santa Claus next to the bird.
And so the participants have to pin that particular bird image, that particular place in 2D space to the Santa Claus.
And you're asked to go and find the Santa Claus again, using some non -directional controller.
And they navigate it like that.
And the claim is that these people use grid cells.
So entorhinal cortex is active in how these people are navigating this abstract cognitive bird space.
And the way you test that claim is you look at the fMRI signal in the entorhinal cortex as the participants head at some particular angle in bird space.
And because of the six fold symmetry of the hexagonal lattice, you get this six fold symmetric waving up and down of the entorhinal cortex activity as you head in particular directions in 2D space.
It was like evidence that the system is being used not just for navigation in 2D space, but any cognitive task with some underlying structure that you can extract, you use it to do these tasks.
Is there significance to bird space also being too linear?
Yes.
Yes.
Like, have people tried this with multiple dimensions of variability?
People haven't done that experiment, but people have done things like look at how grid cells rectify.
They've done things like 3D space, but not like cognitive 3D space.
They've done like literally like make they've done it in bats.
They stick electrodes in bats and make the bats fly around the room, look at how the grid cells respond.
Yeah.
But definitely the I think they've done it.
Ah, they've done it in sequence space.
So in this case, you hear a sequence of sound with hierarchical structure.
So it's like how there's months, weeks, days, and meals, something like that.
So like weeks have a periodic structure, months have a periodic structure, days have a periodic structure, and meals have a periodic structure.
And so you hear a sequence of sounds with exactly the same kind of structure as that hierarchy of sequences.
And you look at the representation in the entorhinal cortex through fMRI.
And you see exactly the same thing, that the structure is all represented in the pipe back.
Even more than that, you actually see in the entorhinal cortex, a array of length scales.
So one end of the entorhinal cortex, you've got very large length scale grid cells that are like responding to large variations in space, the other end of small ones.
And you see the same thing reconfitedly over there.
The like meals cycle that cycles a lot more quicker is represented in one end of the entorhinal cortex in fMRI.
And the months cycle is at the other end with a scale in between.
There's some evidence of that.
All right.
So I've been talking about MEC, the medial entorhinal cortex, another brain area that people don't look at as much is the LEC, the lateral length around cortex, but wouldn't be important for this model.
And basically the only that you shouldn't be aware of before we get to the model is that it seems to represent very high level, the similarity structure in the lateral entorhinal cortex seems to be like a very high level semantic one.
For example, you present some images and you look at how, you know, the visual cortex, things are more similarly represented if they look similar, but by the time you get to the lateral entorhinal cortex, things look more similar based on their usage.
For example, like an ironing board and an iron will be represented similarly, even though they look very different because they're somehow like semantically.
Okay.
So that's the role that the LEC is going to play in this model.
So yeah, basically the claim is, this is for more than just 2D space.
So the neural implementation of this cognitive map, which is for not only 2D space, which this cartoon is supposed to represent, also things, any other structure.
So some structures like transitive inference, this one is faster than that.
And it's faster than that.
Or family trees, like this person is my mother's brother and is therefore my uncle, those kind of things.
These like broader structural inferences that you want to be able to use in many situations.
So the same problem.
Great.
That was a lot of neuroscience.
Now I'm going to get onto the model that tries to summarize all of these things.
And that's going to be the model that will end up looking like a transport.
So yeah, we basically want this separation.
These diagrams here are supposed to represent a particular environment that you're wondering around.
It has an underlying grid structure and you see a set of stimuli at each point on the grid, which these little cartoons.
And you want to try and create a thing that separates out this like 2D structural grid from the actual experiences you're seeing and the mapping to the things I've been showing you is that this grid like code is actually the grid cells in the medial entorhinal cortex are somehow abstracting the structure.
The lateral entorhinal cortex encoding these semantically meaningful similarities will be the objects that you're seeing.
So it's just like, this is what I'm seeing in the world.
And the combination of the two of them will be the hippocampus.
So yeah, in more diagrams, you've got G, the structural code, the grid code, in MEC, in LEC.
Oh, is someone asking a question?
Since morning, so now it's lunchtime.
Sorry, I can't hear you if you're asking a question.
How do I mute someone if they're maybe type it in the chat if there is one.
Nice.
So yeah, we got the hippocampus in the middle, which is going to be our binding of the two of them together.
Okay.
So I'm going to step through each of these three parts on their own and how they do the job that I've assigned to them and then come back together and show the full model.
So lateral entorhinal cortex encodes what you're seeing.
So this is these images or the houses we were looking at before.
And that would just be some vector X, T that's different.
So a random vector different for everything.
The medial entorhinal cortex is the one that tells you where you are in space.
And it has the job of path integrating.
Okay.
So this means receiving a sequence of actions that you've taken in space.
For example, I went north, east and south and telling you where in 2D space that you are.
So it's somehow the bit that embeds the structure of the world.
And the way that we'll do that is this G of T, this vector of activities in this brain area will be updated by a matrix that depends on the actions you've taken.
Okay.
So if you step north, you update the representation with the step north matrix.
Okay.
And those matrices are going to have to obey some rules.
For example, if you step north and step south, you haven't moved.
So the step north matrix and the step south matrix have to be inverses of one another so that the activity stays the same and represents the structure of the body.
Okay.
So that's the world structure part.
Finally, the memory, because we have to memorize which things we found at which positions is going to happen in the hippocampus.
And that's going to be through a version of these things called the Hopfield networks that you heard mentioned in the last talk.
This is like a content addressable memory and it's biologically plausible.
The way it works is you have a set of activities, P, which are the activities of all these neurons.
And when it receives, so, and it just like recurrently updates itself.
So there's some weight matrix in here, W, and some non -linearity, and you run it for an entire, it's like settled into some dynamical system, it's settled into some attractive state.
And the way you make it do memory is through the weight matrix.
Okay.
So you make it like a sum of outer products of these chi mu, each chi is some memory, some pattern that you want to record.
And then it's, yeah, this is just writing it in there.
The update pattern is like that.
And the claim is basically that if P, the memory, the activity of the neurons, the hippocampal neurons is close to some memory, say chi mu, then this dot product will be much larger than all of the other dot products with all the other memories.
So there's sum over all of them will basically be dominated by this one term chi mu.
And so your attractor set network will basically settle into that one chi mu.
And maybe the preeminent of this kind of come later, you can see how this like similarity between points is yeah, power similarity, and then adding some, adding them up weighted by this pairwise similarity is the bit that's going to turn out looking a bit like attention.
And so some of the cool things you can do with these systems is like, here's a set of images that someone's encoded in a Hopfield network.
And then someone's presented this image to the network and asked it to just run to its like dynamical attractor minima.
And it recreates all of the memory that it's got stored in.
So like completes the recipe.
So that's our system.
I'm sorry, I'm trying to assess like where the way it works.
I've heard that like this interpretation is like the modern interpretation of the network.
This one's actually, which interpretation?
Sorry.
That like it's effectively a link.
Oh yeah.
Yeah, yeah, yeah.
It's only the link to transformers will basically only be through the fact, there's classic Hopfield networks, and then there's modern ones that were in like 2016 and the link between attention and modern is precise.
The link with like classic is not as perceptive.
I mean, yeah.
That's what I was expecting.
Yeah, yeah, yeah, yeah.
The modern Hopfield network is to continue with virtual.
With a change in the non -linearity, eh?
Because then you have to do the exponentiation thing.
We'll maybe get to it later and you can tell me something.
I'm sorry, I haven't mentioned it in real life.
No, no, no, no.
More questions are good.
We'll get, yeah.
We have a separate energy function and I think the exponential is in that.
So that's basically how our system is going to work.
But this Tommen -Eichenbaum machine, that's what the name of this thing is.
And so you, the patterns you want to store in the hippocampus, so these memories that we want to embed are a combination of the position and the input.
And like half of Homer's face here, if you then have decided you want to end up at a particular position, you can recall the stimulus that you saw there and predict that as your next observation, or vice versa.
If you see a new thing, you can infer, oh, I path integrated wrong, I must actually be there.
Assuming there's usually more than one thing in the world that might be in a different position.
So yeah, that's the whole system.
Does the whole Tommen -Eichenbaum machine make sense, roughly, what it's doing?
Okay, cool.
And basically this last bit is saying it's really good.
So what I'm showing here, this is on the 2D navigation task and it's a big grid.
I think they use like 11 by 11 or something and it's like wandering around and have to predict what it's going to see in some new environment.
And on here, this is the number of nodes in that graph that you've visited.
And on the y -axis is how much you correctly predict.
And each of these lines is based on how many of those type of environments I've seen before, how quickly do I learn.
And the basic phenomenon it's showing is over time, as you see more and more of these environments, you learn to learn.
So you like learn the structure of the world and eventually able to quickly generalize to the new situation and predict what you're going to see.
And this scales not with like the number of edges that you've visited, which would be the like learn everything option, you know, predict, because if you're trying to predict which data I'm going to see, given my current state and action, and in a dumb way, you just need to see all states in action for all edges.
But this thing is able to do it much more cleverly, because it only needs to visit all nodes and just memorize what is that position.
And you can see that it's learning curve follows the node number of nodes visited.
So it's doing well.
For neuroscience, this is also exciting is that the neural patterns of response in the brain in these like model regions match the ones observed in the brain.
So in the hippocampal section, you get place cell like activities, this is this hexagon is the grid of the environment that it's exploring and plotted is the firing rate of that neuron.
Whereas the ones in the medial inter -rinal cortex show this grid -like firing pattern.
This like example you compare operates on like discrete backspace in the center.
Yeah.
Do you have any thoughts about how that transfers to those things?
Do you think that we're just like macro -driven, like a very nicely discretized little space, or do you think it's like more complicated going on?
Yeah, I imagine there's something more complicated going on.
I guess this is like a super high - No, no, no.
Yeah.
Maybe you make arguments that they, as I was saying, there's these different modules that have operated different scales.
You can see this already here, like grid cells at one scale, grid cells at another scale.
And so you could imagine how that could be useful for like, one of them operates in the highest level and mixed those, one of them operates at the lowest level, you know, and like adaptable, they seem to scale up or down depending on your environment.
And so like an adaptable set of length scales that you can do super quickly, but that's quite speculative.
Okay.
Sorry.
Yeah.
So make sure I understand if you go up.
Okay.
One more.
Yeah.
So you have your, what's the key and what's the value?
Yeah.
So the - And how do networks are always auto instead of petro -associated?
So how are you - The memories that we're going to put in, so the patterns, let's say chi -nu, is going to be some like outer product of the position at a given time and the flattened.
So we take the outer product of those.
So every element in X gets to see every element of G, flatten those out, and that's your vector that you got in there.
Does that make sense?
Yeah.
Sorry, I should have put that out.
Yeah.
And then you do the same operation, except you flatten with an identity in the, let's say you're at a position you want to predict what you're going to see.
You set X to the identity.
You do this operation that creates a big vector from G.
You put that in and you let it run its dynamics and it recalls the pattern and you learn a network that traces out the X from that.
And the figures you show, if you go down a bit.
Yeah.
Yeah.
It's hard to see, but what's on the X axis and what's - Are you training a popular network with this flattened outer product of the - Okay.
Yeah.
The training that's going on is more in the structure of the world, because it has to learn those matrices.
All it gets told is which action type it's taken, and it has to learn the fact that stepping east is the opposite of stepping west.
So all of the learning of stuff is in those matrices, learning to get the right structure.
There's also, I mean, because the hot field network learning, the hot field network, reinitialize every environment and you're just like shoving memories in.
So it's less, like that's less the bit of the screen.
It's causing this certainly, but it's not causing this like shift up, which is as training progresses in many different environments, you get better at the task because it's learning the structure of the task.
And the link to, this is all just modern hot field networks.
The initial paper was actually classic hot field networks, but yeah, now the new versions of it are modern hot field networks.
Yeah.
Right.
And then in so much as modern hot field networks equal attention - This is the track box.
Yeah.
But then you're, okay.
And then you have some, there are some results looking at activations.
Well, these are recordings of the brain.
These are, no, these are actually in TEM.
So this is the left ones and neurons in the G section in the medial inter -ironal cortex part of TEM as you vary position.
And we're going to get, yeah, my last section is about how TEM is like, hopefully it will be clear the link between the two.
Okay.
We are happy with that though, hopefully.
Cool.
TEM is approximately equal to the transponder.
Yeah.
So you, you seem to know all of this, but I guess my notation, at least we can clarify that you've got your data, which is maybe you're like tokens coming in and you've got your positional embedding and the positional embedding will play a very big role here.
That's the E and together they make this vector H and these arrive over time.
Yeah.
And you've got your attention updates that you see some similarity between the key and the query, and then you add up weighted the values with those similarities.
We're all happy with that.
And here's the step version.
So the basic intuition about how these parts map onto each other is that the G is the positioning coding, as you may have been able to predict the X or the input tokens.
This guy when you put in the memory and you try and recall which memory it's most similar to, that's the attention part.
And maybe some get your attention, you compare the current GT to all of the previous GTs and you recall the ones with high similarity structure and return the corresponding X.
Maybe some differences.
So I think I'm going to go through this between how you would maybe like the normal transponder and how to make it map onto this the following.
So the first of these is that the keys and the queries are the same at all time points.
So there's no difference in the matrix that maps from tokens to keys and tokens to same matrix.
And it only depends on the positional encoding.
So you only recall memories based on how similar their positions are.
So yeah, this is key at time tau equals query at time tau equals some matrix applied only to the positional embedding at time tau.
Then the values depend only on this X part.
So it's some factorization of the two, which is the value at time tau is like some value matrix only applied to that X part.
That's the only bit you want to recall, I guess.
Is that right?
And then it's a causal transformer in that you only do attention at things that have arrived at time points in the path.
Make sense?
And finally, the perhaps like weird and interesting difference is that there's this path integration going on in the positional encodings.
So these E are the equivalent of the grid cells, the G from the previous bit, and they're going to be updated through this matrix that depends on the actions you're taking in the world.
Yeah, so that's basically the correspondence.
I'm going to go through a little bit about how the Hopfield network is approximately like doing attention over previous tokens.
So, yeah, I was describing to you before the classic Hopfield network, which if you remove the nonlinearity looks like this.
And the mapping, I guess, is like the hippocampal activity.
The like current neural activity is query.
The set of memories themselves are the key.
You're doing this dot product to get the current similarity between the query and the key.
And then you're summing them up weighted by that dot product.
All of the memories that are values.
So that's a simple version.
But actually, these Hopfield networks are quite bad.
They like in some senses, they tend to fail.
They have a like low memory capacity for n neurons.
They have something they can only embed like 0 .14 n memories, just like a big result from statistical physics in the 80s.
But it's okay.
People have improved this.
The reason that they're bad, it seems to be basically that the overlap between your query and the memories is too big for too many, too many memories.
You know, you basically like look too similar to too many things.
So how do you do that?
You like sharpen your similarity function.
Okay.
And the way we're going to sharpen it is through this function.
And this function is going to be soft.
So it's going to be like, oh, how similar am I to this particular pattern weighted, uh, exponentiated, and then over how similar am I to all the other ones.
That's our new measure of similarity.
And that's the minus sign of the modern Hopfield one.
Um, and then you can see how this thing, uh, yeah, it's basically doing the attention mechanism.
Um, it, and it's also, um, biologically plausible, uh, we'll quickly run through that is that you have some set of activity PT, this like, uh, neural activity, and you're going to compare that to each kind of you.
And that's through these memory neurons.
So there's a set of memory neurons, one for each pattern that you've memorized from you and the weights to this memory neuron will be at this time.
And then the activity of this neuron will be the stock product.
And then you're going to do a divisive normalization to, uh, to run this operation between these neurons.
So like to make them compete with one another and only recall the memories that are most similar through the most activated, according to this like softmax operation.
And then they'll project back to the PT and produce the output by summing up the memories weighted by this thing times the time you, which is the weights.
So then weights out to the memory neurons and back to them and back to their P the hippocampus are both kind of.
And so that's how you can like biologically plausibly run this modern hot field network.
And so, sorry, yeah.
Probably not.
Yeah.
Um, I guess somehow you have to have knowledge in, you know, in this case, it works nicely.
Cause we like wipe this for agents memory every time and only memorize things from the environment.
And so you need something that like gates it so that it only looks for things in the current environment somehow, uh, how that happens.
I'm not sure there are claims that there's this like, uh, just shift over time.
Uh, the claim is basically that like somehow as time passes, the representations just slowly like rotate or something.
And then they're also embedding something like a time similarity as well, because the closer in time you are, the more you're like in the same rotated thing.
Um, so maybe that's a mechanism to like, um, you know, pass a certain time.
You don't recall things, but the evidence and debate a lot around that.
Other mechanisms like it, I'm sure maybe context is another one actually we'll briefly talk about that.
You know, if you know you're in the same context, then you can send a signal like somehow in the prefrontal cortex, like work out what kind of setting in my head you can send that signal back and be like, Oh, make sure you attend to these ones that were in the same context.
So yeah, there we go.
Uh, Tim transformer.
That's the job.
Uh, it path integrates its positional encodings, which is kind of fun.
Uh, it computes similarity using these positional encodings, uh, and it only compares to past memories, but otherwise it looks a bit like a transform set up.
And here's the setup.
We are MEC LEC hippocampus and places, uh, some yet.
So here's a brief, the last thing I think I'm gonna say is that like, um, this extends 10 nicely because it allows it previously you have to do this outer product and flatten.
That's a very dimensionality is like terrible scaling with like, for example, if you want to do position, what I saw and the context signal, something out to them like outer product, three vectors and flatten that as a much, much bigger, you're scaling like thank you, right.
Rather than, uh, what you'd like to do is just like a three N and so this, uh, version of 10 with this new modern hot field network does scale nicely to adding a context input as just another input in what was previously this like modern hot field networks.
There's some, uh, so yeah, our conclusions is there's like proved somewhat interesting as a two way relationship, uh, from AI to the neuro side, we use this new memory model, this modern hot field network that has, uh, all of, you know, all of this bit is supposed to be in the hippocampus.
Whereas previously we just had these like memory bits in the classic hot field network in the campus.
So it makes kind of interesting predictions about different place cell structures in the hippocampus.
And it just sped up the code a lot, uh, from the neuro to AI, maybe there's some, a few things that are slightly different.
This like learnable, recurrent positioning coding.
So people do some of this.
I think they get like, cause it's an encoding to learn RNN updates them, but maybe this is like, uh, some motivation to try, for example, they don't do all the weight matrices and these weight matrices are very biased towards, uh, cause they're invertible generally and things like that.
They're very biased towards representing these very clean structures, like 2d space.
So I mean, interesting.
But the only thing is this is like one attention layer only.
And so like somehow by using nice expectations, making the task very easy in terms of like processing X and using the right positional encoding, you've got to solve the task with one of these also kind of nice.
And maybe it's like a nice interpretation is that you can go in and really probe what these neurons are doing in this network and really understand, you know, we know that the position encoding looks like grid cells.
We have a very deep understanding of why grid cells are a useful thing to have if you're doing this path integration.
So it's like, uh, hopefully helps like interpret all these things.
Oh yeah.
And if there was time, I was going to tell you all about grid cells, which are my hobby horse, but I don't think that's tough.
So I'll stop that.
Questions.
Okay.
Cool.
A very good question.
So in the very beginning, those grids are linked into one, or these ones, those ones.
Yeah.
That's one of your own response.
Yeah.
While it's wild.
Let me tell you more about the grid cell system.
Because you, cause you got electrodes stuck in here, right.
And they generally have like the classic measuring technique is a tetra, which is for wires.
Okay.
And they receive these spikes, which like electrical fluctuations as a result of a neuron firing.
And they can like triangulate that that particular spike that they measured because of the pattern of activity on the four wires has to have only come from one position.
So they can work out which neurons sent that particular spike.
But there's, so there's a set of neurons that have grid cell patterns.
Lots of neurons have patterns that are just translated versions of one another.
So the same grid, like shifted in space, that's called a module.
And then there are sets of modules, which are the same types of neurons, but with a lattice that's much bigger or much smaller.
And in Raps there's roughly seven.
So this is a very surprising crystalline structure of these seven modules, but in each module, each neuron is just translated by open of one another.
Which yeah, there's a lot of theory work about why that's a very sensible thing to do.
If you want to do path integration of work out where you are at the environment based on your like velocity signals.
Cool.
So just this thing that you said, this is really fascinating about the friendly thing.
This is a product of evolution or a product of learning.
Evolution.
Evolution.
It's like, it emerges like 10 days after in a baby rat's life after being born.
So suddenly, oh, suddenly that structure seems to be like very biased to being created.
Unclear, you know, we were talking about how it was being co -opted to encode other things.
And so it's debatable how flexible it is or how hardwired it is.
But it seemed, you know, the FMRI evidence suggests that there's some like more flexibility in the system.
Unclear quite how it's coding it, but it'd be cool to get neural recordings of it and see.
Cool.
Excellent.
Let's give a bigger round of applause.
