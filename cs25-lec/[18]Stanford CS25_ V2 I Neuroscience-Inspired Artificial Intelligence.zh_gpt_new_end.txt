你好。
在这里非常有趣。
今天我要呈现的工作，标题是“注意力近似体育分布式记忆”。
这是与成吉思·佩莱东和我的博士导师加布里埃尔·克雷曼合作完成的。
那么你为什么要关心这项工作呢？
我们展示了启发式注意力操作可以以高维向量的简单属性以生物学合理的方式实现。
正如你所知，transformer和注意力非常强大，但它们是经验性发展的。
注意力中的softmax操作特别重要，但也是一种启发式方法。
因此，我们展示了在体育分布式记忆中使用的超球体的交集在理论上和在一些训练transformer的实验中都近似于softmax和注意力。
所以你可以看到，SCM（Sports to Read Memory）在提前约30年左右就对注意力进行了开发，它是在1988年开发的。
关于这一点令人兴奋的是它符合生物可能性的高标准。
希望我有时间来深入讨论小脑的布线以及如何将每个操作映射到电路的一部分。
首先，我将概述运动分布式记忆。
然后我有一个Transformer注意力摘要，但我假设你们已经了解全部内容。
我们可以到那里然后决定我们想要深入研究多少。
我会谈谈如何实际上注意力近似于SDM，更广泛地解释Transformer，然后希望有时间深入探讨SDM的生物可能性。
此外，我会保持一切高层次的直觉，然后深入数学，但无论何时请停下来问问题。
好的。
所以，运动分布式记忆的动机是关于大脑如何读写记忆以便之后检索正确记忆的问题。
它考虑到的一些因素包括高记忆容量，对查询噪声的鲁棒性，生物可能性以及某种错误容忍度的概念。
SDM与其他你可能熟悉的相关记忆模型不同，比如Hopfield网络，因为它是稀疏的。
因此，它在一个非常高维的向量空间中运行，而存在于这个空间中的神经元只占可能位置的很小一部分。
它也是分布式的。
所以所有的读写操作都适用于所有附近的神经元。
实际上，作为一种旁注，如果你熟悉它们，霍普菲尔德网络是稀疏分布记忆的一个特例。
我现在不打算深入讨论这个问题，但我在博客上有一篇文章。
好的。
所以首先我们要看稀疏分布记忆的写操作。
我们处于这个高维二进制向量空间中。
我们现在将汉明距离作为我们的度量标准。
稍后我们将继续进行。
我们有这个绿色的模式，用实心点表示，而空心圆圈是假设的神经元。
也把一切都抽象地考虑，然后我们以后再将其映射到生物学中。
所以这个模式有一个红色的半径，这是一定的汉明距离。
它激活了所有在该汉明距离内的神经元。
然后，就在这里，我只是指出，这些神经元中的每一个现在都存储着那个绿色的模式，而绿色的模式已经消失了。
所以我正在用这种模糊的空心圆圈来跟踪它的位置。
这以后会相关的。
所以我们正在写入另一个模式，这个橙色的模式。
请注意，神经元内部可以存储多个模式。
从形式上看，这实际上是这些高维向量的叠加或求和。
因为它们是高维的，所以在人行横道上不会出现这种情况，所以你可以摆脱它。
但现在，你可以把它想象成神经元可以存储多种模式。
最后，我们还有第三个模式，这个蓝色的。
我们把它写入另一个位置。
是的，所以我们再次追踪原始图案的位置，但它们可以通过附近存储它们的神经元进行三角测量。
因此，我们已经写入了三个模式。
现在我们要从系统中读取。
所以我有这个粉红色的星星 xi。
看来它有一个给定，它由一个给定矢量表示，这个矢量的位置与空间不同。
它再次激活了附近的神经元。
但现在神经元输出的是它们之前储存的模式。
因此，你可以看到，根据它的位置，它得到了四个蓝色图案、两个橙色图案和一个绿色图案。
然后，它会进行多数规则运算，更新为我们所认为的模式。<EOS
所以在这种情况下，因为蓝色实际上是多数，它完全被更新为蓝色。
再次，稍后我会更正式地阐述这一点。
但这实际上是为了让你对STM的核心操作有直观的理解。
所以，与注意力相关的关键是抽象出在幕后操作的神经元，只考虑相交的圆。
因此，粉色主导圆与右圆的每个相交点意味着它们相交的神经元都存储了被写入的模式，并且现在由查询读取。
而这种相交的大小对应于查询将要读取的模式数量。
因此，从形式上讲，我们将该圆形相交中的神经元数定义为神经元数、模式数和查询之间的基数及其交集。
好的，有什么问题吗？
在我更深入讲解数学之前，总体上有什么问题吗？
我不知道我是否可以检查。
我是否可以轻松检查一下Zoom？
不，抱歉，Zoom的人们。
我不会去检查。
好的。
但是神经元是随机分布在这个圆中的。
是的，对。
而且后来有更近期的工作，他们可以学习并更新他们的位置来削减瓦片流形。
但在这个情况下，您可以假设它们是随机初始化的二进制高维向量。
好的，这就是完整的STM更新规则。
我要一步一步地解释。
首先，您要做的事情，这是为了阅读，明确一下。
所以你已经将模式写入你的神经元。
第一件事就是，你根据其圆形交集的大小给每个模式分配权重。
因此，每个模式的圆形交集，然后你对已写入此空间的所有模式求和。
所以你只是对它们进行加权求和。
然后，通过您拥有的所有交集的总数进行标准化。
最后，因为至少目前我们是在这个二进制空间中工作，您需要映射回二进制，只需查看任何值是否大于一半。
好的，人们对注意力有多熟悉呢？
我看了一下你们之前的讲座，它们似乎相当高级。
像，你们能为我写出注意力方程吗？
这有点像，如果你能做到，我能得到点赞吗？
是的。
好的，我不是特别像，但我会浏览一下。
但我可能会比以前快一些。
所以当我第一次做这个演示时，这是transformer的最新技术，有点像阿尔法级别。
所以这有点有趣，事情发展到现在，我不用告诉你transformer有多重要。
那么，是的，我会用这个例子来工作。
好吧，我会用这个例子来工作，猫坐在空白上。
所以我们在这个设置中，我们正在预测下一个标记，也就是单词"数学"。
那么有四件事情是注意力操作正在做的。
这里的第一件事是它生成所谓的键、值和查询。
再说一遍，我一会儿会进入数学。
我只是想先保持高层次。
然后我们将比较我们的查询与每个键。
所以单词"the"，它距离我们下一个预测的单词最近，是我们的查询。
我们正在看每个键向量有多相似。
然后，基于这种相似性，我们会进行软最大归一化处理，这样，所有的注意力权重、部分一维权重和它们的值向量相加，就可以传播到下一层或用于我们的预测。
因此，在高层次上，您可以将其视为查询词，即查找名词及其相关动词。
因此，假设它与 cat、sat 或它们的关键字具有高度相似性。
因此，这就给 cat 和 sat 值向量赋予了很大的权重，它们会被移动到网络的下一部分。
假设猫的值向量包含其他动物（如老鼠）的叠加，也可能包含与 Matt 押韵的单词。
因此，"坐 "向量也包含了包括 "坐 "在内的事物。
因此，你从关注猫和坐的价值向量中得到的实际结果就像是三次马特加一次老鼠加一次沙发。
这又是一个完全假设的例子。
但我想说的是，你可以从你的值向量中提取出一些有用的东西，通过关注特定的键来预测下一个标记。
所以，我想，是的，这里的另一件事就像是你所关注的东西。
所以cat和sat可能与你实际提取的不同，你正在关注你的键，但你却拿到了你的值向量。
好的，这就是完整的注意力方程式，顶部的行，我将投影矩阵w下标v和q分开，并且第二个，我将它们简化成了你只看到的方程式。
是的，所以，将这个拆分开来，这里的第一步是我们比较我们的查询向量和我们的键之间的点积。
这实际上应该是一个小数。
所以，是的，我们在它们之间做了这个点积，以便获得相似性的概念。
然后，我们应用了softmax操作，这是指数和的指数。
想想softmax的方式是它只会使大值变得更大。
这对于与SCM的关系将是重要的。
所以我会花一分钟来解释。
在这里顶部，我有一些假设的项目，从零到九进行索引，然后是每个项目的值。
在第二行，我只是对它们进行了正常的归一化。
所以，最高的项目变成了30%的值。
但如果我改用softmax，这取决于softmax的beta系数，但该值变为0.6。
所以这只是使你的分布更尖锐的一种思考方式。
而这对于注意力是有用的，因为你只想关注最重要的事情或者附近的事情，而忽略掉更远的东西。
所以一旦我们应用了 softmax，然后我们只是对我们的值向量进行加权求和，实际上这些值向量被提取出来并传递到下一层。
好的，所以这是完整的方程。
我讲得有点快。
我很乐意回答关于它的问题。
但我觉得你们有一半知道它，一半不知道。
好的，那么 Transformer 的注意力是如何近似稀疏分布内存的呢？这是我说的生物学上可行的 30 年前的东西。
是的。
我会在最后解释这个。
是的。
我认为我展示的注意力方程是经过发展的。
我是说，“注意力就是你所需要的”是亮点，但是 Benjio 在 2015 年有一篇论文，它实际上是首次以这种方式书写的。
如果我说错了，请纠正我，但我非常确定。
这就是我问这个问题的原因，因为这是个好问题。
就像你展示的那样，可能有两种不同的方法可以归类为注意力提案，对吧？
或者说，其中一个，是的。
是的，确切地说。
所以我会展示SDM在神经水平上与小脑中的电路有非常好的映射。
现在有这种联系到注意力。
我想你说得对，还有其他的注意力机制。
这是主导的一个，但我不认为这只是巧合。
就像一堆计算你的softmax很昂贵，还有一堆工作，像Lin前辈之类的，试图摆脱softmax操作。
但做得很糟糕。
就像现在Twitter上有很多关于这个的笑话，就像一个黑洞，让人们试图摆脱softmax但是做不到。
所以看起来像是这样以及其他版本，transformer的扩展性不是那么好。
所以这个特定的注意力方程有重要的地方。
但是反过来呢，对吧？
就像，这比如说SDM实际上是很重要的。
所以我认为重要的一点是，你有这种指数级的等待，你真的在关注重要的事情，而忽略其他一切。
而这正是SDM所近似的。
可能会有方程式，但我刚刚想表达的是，softmax似乎很重要，而且这个方程似乎非常成功，我们还没有提出更好的表达方式。
是的，这是一个很好的问题。
好的。
事实证明，稀疏分布记忆，当你将查询和模式彼此移开时，当你拉开这些圆圈，读和写的圆圈，这在足够高维空间中的神经元数量以指数方式衰减。
所以在这个右边的图中，我正在拉开，x轴是我拉开蓝色和粉色圆圈。
y轴上是一个长刻度，是在交叉点上的神经元数量。
因此，在这是一个长刻度上的线性图的程度上，它是指数的。
这是一个特定设置的情况，我使用了64维向量，这在UB2中使用过。
它在许多不同的设置中都成立，特别是在更高维度中，这些维度现在用于更大的transformers。
好的。
所以我有这个圆交集方程的简写。
我将展示的是圆交集近似为指数。
所以我们可以用两个常数C1、C2来写出它。
由于你在对一些指数进行softmax的归一化，那么这个常数就会抵消掉。
重要的是C2，你可以用softmax中使用的beta系数来很好地近似它。
所以，是的，我想首先专注于SDM的二进制原始版本，但后来我们也开发了一个连续版本。
好的。
是的，使圆交集和指数衰减起作用的两个要素是你需要将其映射到注意力，你需要一些连续空间的概念。
因此，你可以使用这个方程将汉明距离映射到离散化的百分比相似性值，向量上的帽子是L2范数。
然后，你可以将左侧的圆交集方程写成这个指数形式，其中有两个你需要学习的常数。
然后通过将C2转换为C进行重写。您可以将此表示为beta系数。
让我来看一些图。
是的。
所以您需要正确的beta系数，但是您可以使用对数线性回归在封闭形式中拟合这个。
我想在这里展示一个图。
是的。
好的。
所以在蓝色部分是我们的圆形截面，使用64维向量分别计算两个不同的汉明距离。
而橙色部分是我们实际的Softmax注意力操作，我们在其中拟合beta系数。
这样它将，它将，由注意力使用的汉明距离等同于汉明距离。
所以主要的图是标准化的权重。
所以只是加和然后除以。
所以这是第一点。
然后我这里有对数图。
您可以看到在非对数空间中，曲线相当一致。
您可以看到对于更高维度，抱歉，更大的汉明距离，在对数图中，您会看到这里的下降，其中圆形交点停止呈指数增长。
但事实证明，这实际上并不是问题，因为指数崩溃的点，您在这里大约是0.20。
而您基本上忽略了这些要点中的任何一个。
因此，在指数真正重要的情况下，这个近似是成立的。
是的。
是的。
是的。
是的。
不，我只是想真的在这里提出一些问题之前出现。
是的。
所以我们在这里所做的就是，我们只是在原始XM的二进制空间中。
我们只是使用这个映射来进行时间相似性。
然后你需要做的就是拥有数据，你可以查看你的数据系数和注意力是如何决定事物有多狡猾的。
这与您用于读写版本的圆的汉明距离直接相关。
所以，是的，为了在这张幻灯片上数学地展示这一点，我没有使用任何技巧。
我只是用模式和查询的SCM符号重写了注意力。
这里下面的这个小框正在进行那个映射。
这是我们更新查询的资金幻灯片。
在左边，我们有用SCM符号编写的我们的注意力方程。
我们扩展我们的子最大。
然后主要声明是，如果我们用SCM替换我们的指数，角落部分的方程式会被近似。
所以，再次，使这项工作需要的两个要素是，首先，您的注意向量，您的键和查询需要能够通过上面的帽子进行归一化。
然后，您想要的是，如果您决定SCM的给定汉明距离，并且我将介绍汉明距离对不同事物的适用性，那么您需要有一个与之相关的数据系数。
但是再次，这只是您想要关注多少事物呢？
所以是的，作为一个快速的侧记，您可以使用连续向量编写SCM，然后不需要进行相似性的映射。
所以在这里我再次有了这些图，但是有了这个，并且我已经添加了橙色的绿色的弯曲，但我也在这里添加了连续的近似值。
关于连续版本的好处是，您实际上可以将稀疏分布式内存编写为具有略有不同假设的多层感知器。
我现在不打算讨论这个，但这在稀疏三个连续学习者的记忆中是有特色的，这已经添加到了额外的阅读材料中。
它将出现在，抱歉，这不应该说ICML，这应该说iClear。
它刚刚被接受到了今年的 iClear。
好的，那么训练transformer使用这些我说的类似于 SDM 的 beta 系数，使用它们吗？
所以，这并不奇怪，根据你设置的半距离不同，SDM 在某些方面更好。
例如，你只想尽可能多地存储记忆，而且你假设你的查询不会有噪音，或者你假设你的查询非常嘈杂，因此你无法存储太多，但是你可以从很远的距离检索。
如果transformer的注意力正在实现稀疏分布式记忆，我们应该期望看到transformer使用的 beta 系数对应于这些良好的 SDM 实例。
因此，我们有一些弱证据表明情况如此。
所以这是关键查询规范化变体的注意力，你实际上学习了你的 beta 系数。
通常在transformer中，你不这样做，但是你不对你的向量进行 L2 范数，因此你可以有这种有效的 beta 系数。
所以在这种情况下，这是实际学习 beta 的更干净的实例。
而这是在多个不同的翻译任务上进行训练的。
我们将跨层和跨任务的学习系数绘制成它们落地时的图形。
而红色虚线对应于三种不同的稀疏分布记忆概念，这些概念对不同的事物都是最优的。
再次强调，这在多大程度上是弱证据，因为要推导出最优的SDM数据系数或相应的距离，我们需要假设在这个高自然空间中存在随机模式。
但是，很明显，现实世界的数据并不是随机的。
然而，很高兴看到，所有的 beta 系数都落在边界内。
而且，它们倾向于向最大的噪声方向偏移，如果你处理的是复杂的现实世界数据，这就更有意义了，在那里，你所看到的下一个数据点可能是基于过去所见的分布之外的，最大容量变体假设根本没有变量。
所以，就像，我能装进多少东西，假设我问系统的问题是完全形式化的？
好的，让我们稍微谈谈更广泛的transformer组件。
所以我提到过，你可以将前馈层编写为一种具有更长期记忆概念的SDM版本。
还有层归一化，在transformer中至关重要。
虽然不完全相同，但与SDM需要的L2归一化相关。
还有关键的查询归一化变体，明确执行这种L2归一化。
它确实在他们进行的小型测试中略有改善性能。
我不知道这是否能扩展到更大的模型。
所以我想这项工作之所以有趣，就像生物可信度一样，我即将谈到。
然后是与transformers的链接，迄今为止它没有改进transformer架构。
但这并不意味着这种观点不能被使用或在某种程度上是有用的。
所以是的，我列出了一些其他方式，SDM与之相关，可以用来汇集进来。
实际上，在SDM是持续学习者的新工作中，我们扩展了电路，看了看其中的组件，特别是抑制性内联神经元，将其实现在深度学习模型中，然后在持续学习方面变得更加出色。
所以这实际上是一种有趣的方式，利用这种联系来获得更好的底线性能。
好的，所以这一部分的总结基本上就是两个超球面的交集近似为指数。
而这使得SDM的读写操作在理论上和我们有限的测试中都能近似注意力。
所以，这里可能产生的大致研究问题是，首先，是Transformer如此成功，是因为它执行了一些关键的认知操作。
小脑是大多数生物，包括果蝇，甚至可能是头足类动物，通过发散进化但现在是收敛进化使用的一个非常古老的脑区域。
然后，考虑到Transformer在经验上非常成功，SDM实际上是否是小脑功能的正确理论呢？
这仍然是一个悬而未决的问题。
随着我们对小脑的了解越来越多，还没有证据证明SDM在那里起作用。
我认为可以大胆地说，这是一个更具有说服力的小脑实际工作方式的理论之一。
是的。
因此，我认为这项工作激励我们更认真地研究这些问题，更认真地研究这两个问题。
好的。
我们有时间吗？
好的。
这是实现SDM的电路。
在底部，我们有用于读取或写入的模式。
实际上，我会逐个解释这些幻灯片。
好的。
是的。
所以首先，我们有进来的模式。
每个神经元这里，这些是每个神经元的树突。
它们决定是否会针对进来的输入而触发。
然后，如果神经元确实触发，而你正在编写这种模式，那么你同时，我会向你解释，这很疯狂，大脑不会这样做，我希望能把它解释清楚。
你不仅需要拥有这个东西，还需要有激活神经元的模式，但你还需要有一条单独的线告诉神经元应该如何分类。
就像你有键和值之间的区别一样，它们可以是不同的向量，代表不同的东西。
在这里，你可以有一个键进来告诉神经元何时激活，以及应该实际上像什么，然后稍后。
这被称为异-联想映射。
然后，一旦你从系统中读取，你还有你的查询进来这里，激活神经元，那些输出任何它们激活的神经元，以及神经元向量是它所存储的特定列。
作为提醒，它是在叠加中存储模式的。
然后它将存储的任何内容转储到这些输出线上。
然后，您有这个 G 多数位操作，将其转换为零或一，决定神经元是否将发射。
所以这里是同样的电路，但我叠加了细胞类型等等。
所以我会回到这张幻灯片，因为大多数人可能不熟悉小脑的电路。
让我来喝点水。
好的，小脑的方式相当均匀，这种模式贯穿始终，还有脑中 70% 的神经元在小脑中。
它们很小，所以你可能不知道。
但小脑是非常被低估的。
有大量证据表明它现在与大多数高阶处理形成了封闭环系统。
如果你的小脑受损，你更有可能患有自闭症等等。
所以它做的事情远不止是精细的运动协调，在过去很多人都以为是这样。
好的，输入通过这里的 MOS 滑块进入。
它们与颗粒细胞接口。
这是一个主要的上投影，您有大量的颗粒细胞。
每个颗粒细胞都有所谓的平行纤维，这是非常长而细的轴突，呈T形分支。
然后它们被小脑皮质细胞击中，这将接收多达 100,000 条平行纤维输入。
这是大脑中任何神经元的最高连接性。
然后小脑皮质细胞将决定是否发射并将其输出向下发送。
这就是模式输入的整个系统，它们位于纤维的一侧或不在一侧。
然后它们以自己的方式输出。
然后你有一个单独的直线，这就是攀爬纤维。
攀爬纤维上来，它们相当惊人，这里的连接你可以把它们视为重要的。
一个真正重要的是它们在这里并不是很强大。
一个真正重要的是它上升并缠绕在单个小脑皮质细胞周围。
映射在攀爬纤维和小脑皮质细胞之间几乎是一对一的，至少是一个非常强大的动作检测器。
所以它们与我们这里连接在一起。
就像从侧面看到的东西？
是的，对。
是两条线。
它们与我们相连。
哦，所以它们是从不同的区域来的单独的神经元。
普金耶靠近你进入大脑核心的地方，有点像在Sarvone的核心。
然后，它再进入到LMS中，回到更高阶的大脑区域，或者向下到肌肉运动等。
你会认为小脑有点像一个精细调整的查找表，就像你已经决定了你想做的肌肉运动，但是小脑会进行一系列的调整，使得它更加精准。
但是似乎这也适用于下一个单词的预测。
就像我们有这方面的MRI数据。
一个神经科学家曾经透露过fMRI的一个小秘密，那就是小脑对一切都会有反应。
所以，好的，回到这个电路，然后。
是的，时间尺度是操作中的吗？
我的意思是，信息存储和检索需要多长时间？
我们有关于这个的任何想法吗？
就像，就像几毫秒，或者信息工人系统？
所以主要的理论是，你通过时间相关的可塑性进行更新，你正在寻找的纤维要么会在你的颗粒细胞发射之前发射，要么会在之后发射。
这样就会更新长期抑制或增强的应激细胞突触。
无论光纤上发生的时间尺度有多大之后，都会发生非常大的变化，或者至少是非常大的变化。
所以我认为你可以得到非常快速和史诗般的更新。
而且它们也能持续很长时间。
我也是这么认为的。
是的，这项科学已经保持了一生。
是的。
所以这个电路真正独特的地方在于，你有这两个正交输入，其中一个是肌肉纤维将信息带入来决定神经元是否能够发射，但另一个是完全分开的光纤线，可以更新特定的神经元和它们稍后会上传的内容。
然后，应激细胞对于每个单个神经元进行了这种冷却，非常重要。
而且每个神经元，记住，都是以这种方式存储一个向量。
所以应激细胞正在进行元素级的求和，然后决定是否发射。
这样可以让你将向量存储在叠加状态中。
然后后来，你知道，这不是所有这个SDM映射理论都很好地契合了马尔和霍利斯有关太阳V轮函数的理论，这些理论仍然相当占主导地位，如果有人熟悉的话。
是的，所以在你之前介绍的SDM中，神经元的类比就像每个条件细胞的神经元，而不是每个神经元都是一个大度。
然后，是的，神经元的位置，那些中空的圆圈，对应于这里的宏伟，模式的弹出对应于激活和修饰。
然后努力， postsynaptic 连接是与条件细胞一起的。
所以实际上它存储的是与该界面的端prokigi细胞的连接集。
然后prokigi细胞进行了大部分的位运算并决定是否要发射。
我们进入问题了吗？
是的，是的，我想我们基本上进入问题时间了。
那么，是的，非常感谢。
我有一个问题。
我对SDM一无所知，但听起来它对长期记忆非常有帮助。
我很好奇，对于短期记忆，你有什么假设呢？
因为看起来，所以如果你有这个transformers的链接，有长期记忆，对短期记忆有什么好处？
因为对我来说，现在看起来我们是在提示上下文中进行这个。
但是我们怎么直接将这两者结合起来呢？
是的，是的。
所以这项工作实际上更注重短期记忆，涉及到注意力操作。
但你可以重写SDM，把它理解为一个多层感知器，执行类似于对其一些神经元进行softmax激活或者对其进行顶部激活的操作，这似乎更自然。
比那更复杂一些。
但是，是的，是的。这里最有趣的事实是，我只是有一堆神经元。
在这个高维空间中激活附近的神经元，你会得到这种指数加权，这就是softmax。
然后，因为这是一种有键和值的关联记忆，它就是注意力。
而且，我想要从中传达的最重要的事情是，大脑实际上非常容易通过使用高维向量和激活来实现注意力操作，注意力方程式。
所以对短期记忆来说是好的。
是的，如果你真的要使用 SDM 进行注意力。
是的，所以让我快速回顾一下。
这很重要。
有两种方式来看待 SDM。
我觉得你之前没有在讨论中出现。
我想我看到你稍后进来了，这完全没关系。
但哦，好的，好的，好的。
是的。
好的。
所以，所以有两种看待 SDM 的方式。
有神经元的角度，就是这里的这个。
这实际上就是大脑中正在发生的事情。
所以唯一不变的是神经元，模式是短暂的。
然后有基于模式的视角，这实际上就是注意力在做的事情。
所以在这里，你抽象出了神经元，我们假设它们在幕后运行。
但你实际计算的是模式的真实位置与查询之间的距离。
这两者都有优缺点。
这样做的好处是你可以得到更高保真度的距离，你完全知道查询离原始模式有多远。
当你决定要更新什么时，这是非常重要的，就像你真的想知道什么是最接近的，什么是更远的，然后你会正确地应用指数加权。
问题在于你需要以那种方式存储所有的模式通知。
这就是为什么transformer有限的上下文窗口的原因。
另一个视角是长期记忆，你忘记了模式，只是看看你的神经元在这种嘈杂的叠加中存储了一堆模式的地方。
所以你真的不能太小，像真的很大的模式，就不太可能，一切都更加嘈杂。
但是你可以存储大量的模式，而且你不需要复杂的窗口。
或者你可以把任何一个惩罚层看作是将整个数据集存储在嘈杂的状态叠加中。
是的，希望这种回答能解决你的问题。
我想首先有一个问题。
是的。
所以我想我的问题是，所以我想，你有点展示了现代自我注意机制映射到这种似乎可能的SDM机制，以及一些关于大脑如何实现SDM的现代理论。
我想我的问题是，那种程度上已经有实验证明了这一点，与你之前提到的，可能实际上更容易使用某种意义上的MLP层来完成这个，而不是一堆这些机制。
那么，实验者们实际上如何区分不同的假设呢？
是的，比如，有一件事我不太清楚的是，即使大脑可以进行注意力，或者SDM，但这并不意味着它会，因为它无法回溯。
对吧？
是的。
那么，我是说，这样到底如何测试呢？
完全正确。
是的。
所以关于背景的观点，你不必在这里这样做，因为你有攀爬纤维，可以直接给存储器发送训练信号。
所以在这种情况下，对于基金伙伴来说，这就像是一个监督学习任务，知道它想要写什么，以及应该如何更新，蛋白质B细胞突触。
但对于你的更广泛的观点，你基本上需要做的是测试这个，你需要能够进行实时学习。
果蝇蘑菇体基本上与servon相同。
而在脑数据集中，这只蝇已经完成了大部分个体神经元的连接。
所以，你真正想要做的是，像离体实时、超级、超级高帧率的钙成像一样，能够看到突触随时间的变化。
所以对于一个关联学习任务，就像，这里的声音向左移动，另一个声音向右移动或者气味或其他什么东西，呈现其中一个追踪，像找出一小部分神经元是响应的，我们知道这是一个小的子集，这些与手固定的。看看这里的突触如何更新，以及这些输出如何对应于运动行为的变化。
然后消除那个记忆。
所以在新的一个上面，然后看着它再次消失。
就像我们的摄像头越来越快，而且我们的钙和电压指示剂也越来越好。
所以希望，在接下来的三到五年里，我们能做一些这样的测试。
但我认为那将是非常明确的。
是的。
我们没有问题。
我想还有一个问题，然后我应该再去一个地方。
就如何基于这个最终的生物实现映射神经元来说，就是说，你映射周围的圆的范围是什么？
我在尝试理解这是怎么回事。
是的，所以我不会与多头混淆。
因为那是不同的注意力头在进行各自的注意力操作。
有趣的是，这个圆有微区，你可以把它想象成某种方式的单独的注意力头。
我不想把那个类比太远，但它确实有点有趣。
所以你把这个联系起来的方式是，在注意力中，你有你的贝塔系数，那是一个有效的贝塔系数，因为你的查询的向量范数是没有约束的。
那对应于一个半突触。
而这里对应于任何给定输入上激活的神经元的数量。
而你想要的半突触，我之前有那张幻灯片，你想要的半突触取决于你实际想做什么。
如果你不想存储那么多记忆，例如，你会得到一个更高的半突触，因为你可以对那个嘈杂的交叉点中的神经元数量进行更高保真度的计算。
是的。
很酷。
是的，非常感谢。
那么作为免责声明，在介绍下一位演讲者之前，不幸的是，这个人因为学院的面试临时取消了议程。
所以我们的下一位演讲者非常慷慨地同意在最后一刻进行演讲。
但我们对他非常感激。
所以我想向大家介绍威尔。
威尔是伦敦大学学院的计算神经科学机器学习博士生，他在他们的盖茨比单位。
我不知道有没有人听说过盖茨比单位。
我有点对历史感兴趣，或者说是历史迷，取决于你如何说。
盖茨比单位实际上是在1990年代和2000年代的一个令人难以置信的强大力量。
辛顿曾经在那里，祖宾·加拉马尼曾经在那里。
他现在负责谷歌研究。
我认为他们已经做了大量的工作。
不管怎样，现在我想邀请威尔来谈谈如何建立认知地图。
你想分享你的屏幕吗？
是的。
好的。
你可以站在前面，是的，让我停止分享。
好的。
所以我将会呈现这项工作。
这是关于我与合作研究海马-内嗅系统的一个模型的一切，完全独立地变得有点像一个transformer。
所以我要谈论的论文描述了这个联系。
建立这个联系的论文是由这三个人撰写的。
詹姆斯是一名博士后，在斯坦福大学，蒂姆是牛津大学和伦敦的一名教授，乔是伦敦的一名博士生。
这是我们将要讨论的海马-内嗅系统模型要解决的问题。
基本上是观察到。
世界上有很多结构，通常我们应该利用这些结构来快速地在任务之间进行泛化。
我所说的那种东西是，你知道2D空间是如何工作的，这是因为你长时间在世界上生活的经验。
所以如果你从这个温室开始向北走，然后到这个橙色的，然后到这个红色的，然后到这个粉色的，因为2D空间的结构，你可以想到，如果我向左走会发生什么？
而且你知道你最终会回到绿色的那个，因为这种类型的循环在2D空间中闭合。
好的。
这可能是你刚到达的一个新城市。
这就像是一种零射击的泛化，因为你不知怎么地意识到这种结构更广泛地适用，并在新的上下文中使用它。
是的。
通常有很多这种情况，在世界上会出现类似的结构。
所以可能会有很多情况，同样的结构会对进行这些零射击的泛化有用，以预测接下来会看到什么。
好的。
所以你可能能够看到我们已经开始将这些映射到某种感觉有点类似变换的序列预测任务上，你会收到一系列观察结果，而在这种情况下是空间中的动作、移动。
你的工作是在给定一个新的动作步骤向左移动时，尝试预测你将会看到什么。
所以这就是它的序列预测版本。
我们将尝试解决这个问题的方式是基于因式分解。
就像你不能进入一个环境然后只从那一个环境的经验中学习一样。
你必须将结构和你正在经历的经验分开，以便你可以重用结构部分，它在世界中经常出现。
是的，将记忆与结构分离。
这样，你知道，这是两者的分离。
我们的小伙子在这个2D网格世界里游荡。
而你希望分开的是，这里有2D空间，而且它是有基础规则的2D空间。
在你所处的环境中的特定实例中，你需要能够回忆起物体在环境中的位置。
好的。
所以在这种情况下，就像，哦，这个位置有一个橙色的房子，这个位置有一个绿色的，抱歉，是橙色、红色和粉色。
所以你必须将这两者绑定起来。
你必须像是，每当你意识到你又回到这个位置时，记得那是你将要看到的观察结果。
好的。
所以我们要构建的这个模型是试图实现这一点的一些模型。
是的。
新的星星。
所以当你进入时，想象你进入了一个具有相同结构的新环境，你四处走动并意识到它是相同的结构。
你只需将你看到的新事物与位置绑定，然后完成，通过了。
你知道，你知道世界是怎么运转的。
这就是神经科学家所说的认知地图，即将其分开并理解可以在新情境中重复使用的结构的概念。
是的，实验室里建立的这个模型就是这个过程发生的模型，分离它们之间的过程以及如何使用它们进行新的推断。
这部分应该看起来像一个交通工具，但是先进行一般性的介绍，然后我们将深入了解一下。
虽然这可能有点说不清楚，但总体来说，我会假设这是有道理的。
所以我们将从一些大脑方面的内容开始。
在空间导航方面，有大量证据表明大脑正在执行类似这样的任务。
我的意思是，我认为你可能已经能够想象出自己在做这件事情了，当你去一个新的城市，或者尝试理解一个你之前从指导中认识到的新任务时，你可以看出这是你可能正在做的事情。
但是空间导航是神经科学的一个领域，在过去的50年里取得了大量发现。
有很多关于这种计算的神经基础的证据。
所以我们要讨论一些这些例子。
最早的是像Tolman这样的心理学家，他们展示了在这种情况下老鼠可以做这种类型的路径整合结构。
这项工作的方式是，他们被放置在这里的起始位置，底部S，他们被训练说，这条路线上去这里会得到奖励。
所以这是我们必须绕过的迷宫。
然后他们被问及，他们被放置在这个新的，同样的东西，但他们堵住了这条需要绕过这条长长的曲折路线的路径，而是给了他们一系列所有这些通道供选择。
他们看看老鼠走哪条路。
发现是老鼠选择与朝这个方向出发相对应的那一条路。
所以老鼠不仅仅是学会了，你知道，一个选项是，这就像是对需要采取的行动进行盲目记忆，以便绕过去。
不，它实际上是学会了将奖励嵌入到它对二维空间的理解中，并采取一条直接路线，即使它以前从未走过，有证据表明老鼠正在做到这一点，就像我们一样。
然后关于这个的神经发现一系列事情。
所以John O'Keefe在海马体中插入了电极，海马体是一个我们将更多地讨论的脑区，并发现了被称为位置细胞的东西。
所以我在这里绘制的每一列都是一个单独的神经元，而老鼠或大鼠，我记不清了，正在一个方形环境中奔跑。
黑线是啮齿动物随着时间追踪的路径，每次看到这个单个神经元尖峰时，您就放下一个红点。
然后这个底部的图就是那个尖峰率的平滑版本。
所以那个射频率，你可以把它想象成神经元的活动和神经元的工作。
这通常是人们绘制的类比。
所以这些被称为位置细胞，因为它们是在空间中特定位置上响应的神经元。
在70年代，这引起了巨大的兴奋，你知道，人们主要一直在研究感觉系统和运动输出，突然之间，一个深层的认知变量——位置，这是你从来没有的GPS信号，但是在大脑中有一个看起来像是位置的信号，以非常可理解的方式。
这个最重要的发现链中的下一步，我猜，是莫瑟实验室，这是挪威的一个研究小组。
他们研究的是大脑的不同区域，即内嗅皮层。
所以，我们要讨论的是海马内嗅系统。
他们发现了这种称为“网格细胞”的神经元。
所以，这里再次出现了我在这里展示的相同情节结构，但是这些神经元不是在房间的一个位置响应，而是在房间中尝试形成类似六边形格子的位置。
好的。
所以，我想向您展示这两个，因为它们确实激发了对这种空间认知的基础神经基础的研究，更好地体现了某种方式的空间结构。
好的。
这是一个非常令人惊讶的发现，为什么神经元选择用这种六边形格子来表示事物。
这就像，是的，自那时以来引发了大量的研究。
广义上说，在这个领域已经有了许多更多的发现。
所以有场所细胞，我已经和你们谈过了网格细胞，这些细胞根据不是你自己的位置，而是另一个动物的位置而响应，根据你的头朝向特定方向时响应的细胞，根据你距离物体特定距离时响应的细胞。
就像我在物体的南边一步，那种细胞，响应奖励位置的细胞，响应到边界的向量的细胞，响应各种各样结构的细胞，这一对脑结构，这里的海马体，这个红色区域，和这里的内嗅皮质，这个蓝色区域，跨越了许多物种都有所代表。
最后，这其中还有一个有趣的发现是他们对伦敦出租车司机进行了fMRI实验。
我不知道你是否知道，但是伦敦出租车司机他们有一个叫做“the knowledge”的东西，这是一个为期两年的考试，他们必须学习伦敦的每一条街道。
而这个想法是，考试大致是这样进行的，哦，这里堵车了，这里有路障。
我需要从像卡姆登镇到旺兹沃斯的最快路线。
你会选择哪条路线呢？
他们必须告诉你他们要通过所有道路的路线以及他们在那里停车时如何重新规划。
所以非常繁忙。
有时你会看到他们在地图上学习所有这些路线。
谷歌地图已经使他们有些过时了，但你知道，幸运的是，他们在这之前就得到了它们，这个实验在那之前就已经完成了。
所以，他们用fMRI测量你的海马体的大小与你当出租车司机的月份有关。
基本上，出租车司机做得越久，海马体越大，因为他们需要进行这种空间推理。
这是一大堆证据表明这些大脑区域与空间有关。
但是，有很多证据表明，这些区域中发生的事情不仅仅与空间有关。
好的。
我们要一起构建这些证据来证明这种潜在的结构推理。
所以我要讲解其中的一些。
这些证据中的第一个是一个名叫HM的病人。
这是医学史上研究最多的病人。
他患有癫痫，为了治疗难治性癫痫，你必须切除大脑的特定区域。
这就是在你的大脑中引起这些癫痫事件的原因。
而在这种情况下，癫痫是从这个家伙的海马体发作的。
所以他们双侧损害了他的海马体，切除了他的两个海马体。
结果发现，这个家伙后来患有严重的遗忘症。
他再也没有形成过新的记忆。
他只能回忆起手术发生很久以前的记忆。
好的。
实验证明了很多关于我们如何理解记忆的神经基础的内容，比如他能学会执行运动任务。
所以运动任务以某种方式被分析。
例如，他们让他做一些非常困难的运动协调任务，一般人无法完成，但通过大量练习可以做到。
最终，他变得非常擅长，并且在学习做这些任务方面与其他人一样出色。
他对自己做任务的经历毫无记忆。
所以他开始做这项新任务时会说：“我以前从未见过这个。
我不知道你让我做什么。
我需要做它。”
令人惊讶。
是的。
所以有一些证据表明海马参与了某些记忆部分，这似乎与我刚刚和你谈到的关于空间的内容有些不同。
第二个是想象事物。
这实际上是 Demis Asabis 的一篇论文，他之前是一位神经科学家，现在是深度之心的负责人。
也许你看不到这个。
我会读出其中一些。
要求你想象自己躺在一个美丽的热带海湾的白色沙滩上。
而控制组，这底部的一行说的是像这样，天气很热，太阳照在我身上。
在我下面的声音几乎让人无法忍受的炎热。
我可以听到小浪花在沙滩上笑的声音。
海水是华丽的，碧绿的颜色，你知道，像这样美丽场景的生动描述。
而海马受损的人则说，就看到天空，除此之外我真的看不到什么，我能听到海鸥在海下的声音。
我可以感觉到手指下的沙粒。
然后，是的，挣扎基本上是真的很难做到这一点。
想象和想象一些事情真的令人惊讶。
所以这些任务中最后一个就是这个传递推理任务。
传递推理，A大于B，B大于C，因此A大于C。
他们将这个转化为啮齿类实验的方式是给你两个有不同气味的食物罐。
你的任务是去找到那个有食物的罐子，你会学习到哪个罐子有，抱歉，哪个有食物气味的罐子。
它们通过气味和B来区分这两个罐子，并且啮齿类动物必须学会去特定的罐子。
在这种情况下，就是那个像A的气味的罐子。
他们做了两个这样的实验，当A和B一起出现时，A有食物，当B和C一起出现时，B有食物。
然后他们测试，当面对全新的A和C时，老鼠会怎么做。
如果它们有海马，它们会选择A而不是C，它们会进行传递推理。
如果它们没有，它们就不能。
因此，这是一个更广泛的集合。
这就像，我已经告诉你海马是如何用于人们一直以来都很激动的这些空间东西的。
但还有所有这种关系性的东西，想象新情况，一些略微更复杂的故事在这里。
我要做的最后一件事就是处理嗯托边缘皮层。
所以，如果你还记得海马体和这些家伙，嗯托边缘皮层和这些网格细胞，以及嗯托边缘皮层的一些更广泛的东西。
这都是模型的动机，试图将所有这些东西都结合起来。
所以在这个里面，这被称为有弹性的鸟任务。
好。
所以你把人放在一个 fMRI 机器里，让他们导航，但是在鸟的空间里导航。
而鸟的空间意味着这是一个二维图像空间。
每个图像都是这些鸟之一。
当你沿着 X 轴变化时，鸟的腿变得长短不一。
当你沿着 Y 轴方向变化时，鸟的脖子变得长短不一。
好。
病人坐在那里，或者受试者坐在那里，只是看着鸟的图像变化，所以它在二维空间中描绘出一些路径，但他们从不看到这个二维空间。
他们只看到这些图像。
而基本上的说法是，然后他们被要求执行一些导航任务。
他们会说，哦，无论何时你在二维空间的这个地方，你就会在鸟旁边看到圣诞老人。
所以参与者必须将那只特定的鸟图像固定在二维空间中的那个特定位置上，固定在圣诞老人身上。
然后，你被要求再次找到圣诞老人，使用一些非定向控制器。
他们就是这样导航的。
而声称这些人使用的是网格细胞。
因此，内嗅皮层在这些人如何导航这个抽象的认知鸟空间中是活跃的。
测试这一说法的方法是观察内嗅皮层的fMRI信号，当参与者在鸟空间的特定角度移动头部时。
由于六重对称的六角网格结构，当你在二维空间的特定方向上移动时，内嗅皮层的活动会呈现出六重对称的波动。
这就像是证据，表明这个系统不仅仅用于在二维空间中导航，而且用于任何具有一些基本结构的认知任务，你可以提取出这些结构并用它来完成这些任务。
鸟空间也线性地太有意义了吗？
是的。
是的。
比如，人们尝试过用多个变量维度来做这个实验吗？
人们还没有做过那个实验，但人们已经做过类似的事情，比如观察网格细胞是如何矫正的。
他们做过3D空间的事情，但不是认知3D空间的。
他们做过像字面上的，就像是在蝙蝠身上做的。
他们把电极插入蝙蝠身上，让蝙蝠在房间里飞来飞去，观察网格细胞的反应。
是的。
但我确信他们已经做到了。
啊，他们在序列空间中做到了。
所以在这种情况下，你听到的是具有分层结构的一系列声音。
就像有月份、星期、天和餐点一样。
所以像星期有周期结构，月份有周期结构，天有周期结构，餐点有周期结构。
于是你听到一系列声音，其结构与该层次序列的层次结构完全相同。
然后你通过fMRI观察内嗅皮质中的表示。
你会看到完全相同的情况，结构都在管子里得到了表示。
甚至更甚的是，你实际上会在内嗅皮质中看到一系列长度尺度。
所以在内嗅皮质的一端，你有非常大尺度的网格细胞，它们对空间中的大变化做出响应，而在另一端则是小尺度的。 
你会经常在那里看到同样的事情。
循环更快的类似餐周期在实验室中的内嗅皮层的一端表示出来。
月份周期在另一端，中间有一个范围。
有一些证据支持这一点。
好的。
所以我一直在谈论MEC，即内嗅皮层，人们不太关注的另一个脑区是LEC，即外嗅皮层，但对于这个模型来说不太重要。
基本上，在我们进入模型之前，你应该知道的唯一一件事是，外嗅皮层中的相似性结构似乎是一个非常高级的语义结构。
例如，你呈现一些图像，你看看，你知道，视觉皮层，如果它们看起来相似，它们的表示方式就更相似，但是当你到达外嗅皮层时，事物看起来更相似是基于它们的使用方式。
例如，像熨斗和熨斗板会被类似地表示，即使它们看起来非常不同，因为它们在某种程度上是语义上相似的。
好的。
所以这就是LEC在这个模型中将要发挥的作用。
所以，基本上的说法是，这不仅仅适用于2D空间。
所以这种认知地图的神经实现，不仅仅是为了2D空间，这幅卡通图所代表的，还有其他的事物，任何其他的结构。
所以一些像是推理传递这样的结构，这个比那个快。
而且它比那个快。
或者家谱，像这个人是我妈妈的哥哥，因此是我的叔叔，那样的事情。
这些广义的结构推理，你希望能够在许多情况下使用。
所以是同样的问题。
太好了。
那是很多的神经科学。
现在我要开始介绍试图总结所有这些内容的模型。
那将会是一个看起来像一个传输的模型。
所以，是的，我们基本上想要这种分离。
这些图表在这里是代表你周围特定环境的，你正在其中游荡。
它具有基础的网格结构，你在网格上的每个点看到一组刺激，就像这些小卡通一样。
你想尝试创建一个东西，将这个二维结构网格与你所看到的实际经验分离开来，而我向你展示的内容与这个网格的编码实际上是在内嗓皮层中的网格细胞，它们以某种方式将结构抽象化。
侧嗓皮层对这些语义上有意义的相似之处进行编码，这些相似之处将成为你所看到的对象。
所以，就像这是我在世界中看到的东西。
两者的结合将形成海马体。
是的，在更多的图表中，你有G，结构编码，MEC中的网格编码，LEC中的网格编码。
哦，有人在问问题吗？
从早上开始，现在是午餐时间。
抱歉，如果你在问问题，我听不见你。
如果有人要静音别人，应该在聊天中输入，如果有的话。
不错。
所以是的，我们在中间有海马体，它将是将它们两者绑在一起的地方。
好的。
所以我将逐个介绍它们三个部分，以及它们如何完成我分配给它们的任务，然后再回过头来展示整个模型。
所以，侧隔前皮质对你所看到的东西进行编码。
所以，这些图像或者是我们之前看到的房屋。
那就是一些不同的向量X，T。
所以，一种随机向量，每个都不同。
而腹侧前皮质则告诉你在空间中的位置。
它的工作是路径积分。
好的。
这意味着接收到你在空间中采取的一系列动作。
例如，我向北走、向东走、向南走，并告诉你在二维空间中的位置。
所以，它在某种程度上嵌入了世界的结构。
而我们将这样做的方式是通过这个G of T，这个大脑区域中的活动向量将通过一个依赖于你采取的行动的矩阵进行更新。
好的。
所以，如果你向北走，你就用向北走的矩阵更新表示。
好的。
而这些矩阵必须遵循一些规则。
例如，如果你向北走然后向南走，你就没有移动。
所以向北走的矩阵和向南走的矩阵必须彼此求逆，以使活动保持不变，并表示身体的结构。
好的。
那就是世界结构部分。
最后是记忆，因为我们必须记住我们在哪些位置找到了哪些东西，这将发生在海马体中。
而这将通过一种称为霍普菲尔德网络的东西来实现，你在上次讲话中听到过。
这就像一个内容可寻址的内存，而且它在生物上是可行的。
它的工作方式是你有一组活动P，这些活动是所有这些神经元的活动。
当它接收到，所以，它就像是再次更新自己。
所以这里有一些权重矩阵W，还有一些非线性，你让它运行一段时间，它就像是安定在某种动力系统中，安定在某种吸引态中。
你让它记忆的方式是通过权重矩阵。
好的。
所以你让它像这样成为这些χμ的外积之和，每个χ是你想要记录的一些记忆，一些模式。
然后它就是，是的，这只是把它写在那里。
更新模式就是这样。
而这个论断基本上是说，如果P，即内存，神经元的活动，海马神经元的活动接近某个记忆，比如说chi mu，那么这个点积将远远大于与所有其他记忆的所有其他点积。
所以总和所有这些将基本上被这个chi mu的这一项所主导。
因此，你的吸引子集网络基本上会稳定在那个chi mu上。
也许这种类型的突显稍后会出现，你可以看到这种点之间的相似性是一种权力相似性，然后通过这种两两相似性加权相加的方式，结果会看起来有点像注意力。
所以你可以用这些系统做一些很酷的事情，比如，这是一组图像，有人已经在Hopfield网络中对其进行了编码。
然后有人向网络呈现了这个图像，并要求它只是运行到它的动态吸引子最小值。
它重新创建了它存储的所有记忆。
就像完成了食谱一样。
所以这就是我们的系统。
对不起，我在试图评估它的工作方式。
我听说这种解释是网络的现代解释。
这个实际上是，哪种解释？
对不起。
那就像是一个有效的链接。
哦，是的。
是的，是的，是的。
它只是与transformer的链接基本上只通过这个事实，有经典的Hopfield网络，然后有现代的网络，就像2016年的那种，关于注意力和现代之间的链接是精确的。
与经典的链接并不那么感知。
我是说，是的。
这是我期望的。
是的，是的，是的，是的。
现代Hopfield网络是要继续虚拟的。
随着非线性的变化，对吧？
因为然后你就得做指数的那个事情。
也许我们以后会涉及到，你可以告诉我一些事情。
对不起，我在现实生活中没有提到它。
不，不，不，不。
更多的问题是好的。
我们会得到，是的。
我们有一个单独的能量函数，我认为指数是在那里。
所以基本上这就是我们系统将要运作的方式。
但是这个Tommen-Eichenbaum机器，这就是这个东西的名字。
所以，要存储在海马体中的模式，我们想要嵌入的这些记忆是位置和输入的组合。
就像霍默的脸的一半在这里，如果你决定要到达特定的位置，你可以回忆起你在那里看到的刺激，并预测它作为你的下一个观察，反之亦然。
如果你看到一个新的东西，你可以推断，哦，我路径积分错了，我实际上必须在那里。
假设世界上通常有不止一件事可能处于不同的位置。
所以，是的，这就是整个系统。
整个Tommen-Eichenbaum机器的操作大致有意义吗？
好的，很酷。
基本上，最后这部分是在说它真的很好。
所以我在这里展示的是在2D导航任务上，它是一个大网格。
我认为它们使用了11乘11之类的尺寸，它在周围游荡并且必须预测在某个新环境中它将看到什么。
在这里，这是图中你访问的节点的数量。
在y轴上是你正确预测了多少。
每条线都基于我之前看过这些类型环境的数量，我学得有多快。
它展示的基本现象是随着时间的推移，随着你看到越来越多这样的环境，你学会了学习。
所以你会学习世界的结构，最终能够快速地推广到新的情况，并预测你将会看到什么。
这并不是随着你访问的边的数量而扩展的，那将是学会所有的东西的选择，你知道，预测，因为如果你试图预测我将看到哪些数据，鉴于我的当前状态和动作，在一个愚蠢的方式，你只需要看到所有边的所有状态和动作。
但是这个东西能够更聪明地做到这一点，因为它只需要访问所有的节点，并记住那个位置是什么。
你可以看到它的学习曲线遵循访问的节点数量。
所以它做得很好。
对于神经科学来说，这也是令人兴奋的，因为这些模型区域中大脑的神经反应模式与大脑中观察到的模式相匹配。
因此，在海马区，你会得到类似位置细胞的活动，这是它正在探索的环境的六边形，绘制的是该神经元的发射率。
而在内侧中间-颞皮层中，显示出这种类似网格的放电模式。
这个例子就好比你在中心操作离散的退格键。
是的。
你对这个如何转移到那些东西有什么想法吗？
你认为我们只是像宏一样驱动，像一个非常好地离散化的小空间，还是你认为情况更复杂一些？
是的，我想有更复杂的事情正在发生。
我想这就像一个超级高级的-不，不，不。
是的。
也许你可以提出论点，正如我之前所说，有这些不同的模块，它们在不同的尺度上运作。
你已经可以在这里看到，比如一个尺度上的网格单元，另一个尺度上的网格单元。
所以你可以想象，这对于像，其中一个在最高层次运作并混合这些，其中一个在最低层次运作的时候可能会有用，你知道的，像是适应性的，它们似乎会根据你的环境进行缩放。
所以就像是一组可适应的长度尺度，你可以非常快速地完成，但这相当推测性的。
好的。
抱歉。
是的。
所以确保我理解如果你上升了。
好的。
再来一个。
是的。
所以你有你的，什么是关键，什么是值？
是的。
所以- 网络如何总是自动而不是石油相关的？
所以你是怎么样的 - 我们要放入的记忆，所以模式，比如说chi-nu，将会是某个给定时间位置的外积和扁平化的。
所以我们取这些的外积。
所以X中的每个元素都可以看到G中的每个元素，将它们展平，这就是你放进去的向量。
这样说得通吗？
是的。
抱歉，我应该把那个说出来。
是的。
然后你执行相同的操作，除了你用一个身份矩阵展平，在，比如说你在一个位置，你想要预测你将要看到的。
你把X设置为身份矩阵。
你执行这个操作，从G创建一个大向量。
你把它放进去，让它运行它的动态，它会回忆起这个模式，并学会一个网络，跟踪出X的轨迹。
你展示的图表，如果你往下看一点。
是的。
是的。
很难看到，但是X轴上是什么，什么是 - 你正在用这个扁平化的外积训练一个流行的网络 - 好的。
是的。
正在进行的培训更多地涉及到世界的结构，因为它必须学会那些矩阵。
它所得到的全部信息就是它采取的动作类型，它必须学会向东迈步是向西迈步的相反。
因此，所有的学习都在那些矩阵中进行，学会获得正确的结构。
还有，我是说，因为热场网络学习，热场网络，每个环境重新初始化，你就像是在推送记忆。
所以，就像屏幕的那部分就比较少了。
它当然引起了这个，但它并没有引起这种向上的移位，随着训练在许多不同的环境中进行，你在任务上变得更加娴熟，因为它正在学习任务的结构。
而这一切都与现代热场网络有关。
最初的论文实际上是关于经典热场网络的，但是，是的，现在的新版本是现代热场网络。
是的。
对。
然后，在现代热场网络中，注意力是相等的 - 这是跟踪框。
是的。
但然后你，好的。
然后你有一些，有一些研究在观察激活。
好的，这些是大脑的记录。
这些，不，实际上是在TEM中。
所以这是左侧的神经元在TEM中介-铁皮质部分的G节中，当你改变位置时。
我们将会得到，是的，我的最后一部分是关于TEM的，希望能清楚地表明两者之间的联系。
好的。
我们对此感到满意，希望如此。
很酷。
TEM大约等于传输器。
是的。
所以，你似乎对所有这些都很了解，但我想我的表示法，至少我们可以澄清你已经有了你的数据，也许你的数据就像是进来的标记，你有了你的位置嵌入和位置嵌入在这里会起到非常重要的作用。
这就是E，它们一起构成这个向量H，这些向量随时间到达。
是的。
然后你有你的注意力更新，你会看到一些键和查询之间的相似性，然后你会用那些相似性加权地添加值。
我们对此都很满意。
这是步骤版本。
所以关于这些部分如何映射到彼此的基本直觉是，G是位置编码，正如你可能已经能够预测X或输入令牌。
当你将这个记忆放入，然后尝试回忆起它与哪个记忆最相似，那就是注意力部分。
也许有些会引起你的注意，你会比较当前的GT与所有先前的GT，并回忆那些具有高相似性结构的，并返回相应的X。
也许会有一些差异。
所以我想我会通过这个过程，解释你可能会喜欢正常的传输器和如何使其映射到这个过程之间的差异。
所以其中之一是，关键和查询在所有时间点都相同。
因此，在从令牌到关键和从令牌到相同矩阵的映射中没有区别。
它仅取决于位置编码。
所以你只根据它们的位置有多相似来回忆记忆。
是的，在这里，关键在时间tau等于查询在时间tau等于仅应用于时间tau的位置嵌入的某个矩阵。
然后值仅取决于这个X部分。
所以它是两者的一些因式分解，而时间tau处的值就像是仅应用于那个X部分的某个值矩阵。
那是你想要回忆的唯一部分，我猜。
是这样吗？
然后它是一个因果transformer，你只在已到达路径时间点的事物上进行注意力。
有道理吗？
最后，也许是奇怪而有趣的区别是，位置编码中正在进行路径集成。
所以这些E相当于前面一部分的网格单元G，它们将通过这个矩阵进行更新，这个矩阵取决于你在世界中所采取的行动。
是的，基本上就是这样对应的关系。
我将稍微介绍一下Hopfield网络如何近似地像是在对先前的记号进行注意力。
所以，是的，我之前向你描述了经典的Hopfield网络，如果去除非线性的话，看起来是这样的。
我想映射就像是海马活动。
像是当前的神经活动就是查询。
记忆集本身就是键。
你正在做这个点积来得到查询和键之间的当前相似性。
然后你将它们加权求和。
所有的记忆都是值。
所以这是一个简单版本。
但事实上，这些Hopfield网络非常糟糕。
在某些意义上，它们往往失败。
对于n个神经元，它们的内存容量很低。
它们只能嵌入大约0.14n个记忆，就像80年代统计物理学的一个重要结果。
但没关系。
人们已经改进了这一点。
它们之所以糟糕，似乎基本原因是你的查询和记忆之间的重叠对于太多太多的记忆来说太大了。
你知道，你基本上看起来太像太多的东西。
那么你该怎么做呢？
你会像磨削你的相似度函数。
好的。
我们将通过这个函数来磨削它。
这个函数将是软的。
所以它会是，哦，我对这个特定模式有多么相似的加权指数，然后除以我对其他所有模式有多么相似。
这是我们新的相似度度量。
这就是现代Hopfield的负号。
嗯，然后你可以看到这个东西，嗯，是基本上在执行注意力机制。
嗯，它也是，嗯，在生物学上是可行的，我们会快速地浏览一下，你有一些活动PT，这就像，嗯，神经活动，然后你要将其与每种类型进行比较。
而这是通过这些记忆神经元来实现的。
所以有一组记忆神经元，每个模式对应一个你所记忆的模式，神经元的权重将会在这个时刻。
然后这个神经元的活动将是点积。
然后你会进行除法归一化，来对这些神经元之间进行这个操作。
就像让它们相互竞争，并且只回忆那些通过最激活的、根据这个 softmax 操作的最相似的记忆。
然后它们将会投射回到 PT 并通过将记忆按照这个乘以时间的权重加起来来产生输出。
所以权重到记忆神经元然后回到它们和回到它们的 P 海马体都是一种。
这就是你如何可以像生物学上可行地运行这个现代的热门领域网络。
所以，抱歉，是的。
可能不是。
是的。
嗯，我想你必须在某种程度上对这个有所了解，你知道，在这种情况下，它运作得很好。
因为我们每次都会清除这个代理的记忆，只记住来自环境的东西。
所以你需要一些像门一样的东西，这样它只会在当前环境中寻找东西，不知何故，以某种方式实现这一点。
我不确定是否有人声称随着时间的推移会发生这种情况。
这种说法基本上是，随着时间的推移，表示法会慢慢地旋转或类似的事情。
然后它们还会嵌入一些时间相似性，因为你越接近时间，你就越像在同一个旋转的东西里。
那么也许这是一种机制，可以像通过某个特定的时间。
你不会回忆起事情，但是有很多证据和辩论围绕着这个问题。
其他类似的机制，我相信也可能有，实际上我们会简要地谈谈这一点。
你知道，如果你知道你在同样的情境中，那么你可以发送一个信号，就像在前额叶某个地方，想象出我头脑中的一种设定，然后你可以发送那个信号回来，然后说，哦，确保你关注那些在同一个情境中的东西。
所以，是的，就是这样。
嗯，Tim transformer。
那就是这份工作。
它通过路径集成了它的位置编码，有点有趣。
它使用这些位置编码来计算相似性，然后它只与过去的记忆进行比较，但除此之外，它看起来有点像一个变换设置。
这是设置的情况。
我们是MEC，LEC，海马体和地方，有一些。
所以这是一个简要说明，我认为我要说的最后一件事是，这样做可以很好地扩展10，因为它允许你以前必须进行这个外积和展平。
那维度太可怕了，比如，如果你想做位置，我看到的和上下文信号之类的事情，就像是外积，三个向量，然后展平，那就会变得更大得多，你的缩放就像是，谢谢你，对吧。
与其说, 嗯, 你想要做的就像一个三N, 所以这个, 嗯, 版本的10和这个新的现代热场网络确实能够很好地扩展到将上下文输入作为先前这个现代热场网络中的另一个输入。
有一些, 嗯, 是的, 我们的结论是, 从AI到神经方面有一种证明相当有趣的双向关系, 我们使用了这个新的内存模型, 这个现代热场网络, 它有, 你知道的, 所有这一切都应该在海马体中。
而以前我们只是在经典的热场网络中有这些像记忆位。
所以它对海马体中的不同位置细胞结构做出了一些有趣的预测。
而且它只是加速了代码, 从神经到AI, 也许有一些略有不同的东西。
这种可学习的, 循环定位编码。
所以人们做一些这样的事情。
我认为他们得到了一些, 因为它是一种编码来学习RNN更新它们, 但也许这就像, 嗯, 一些动力去尝试, 例如, 他们没有做所有的权重矩阵, 而这些权重矩阵非常倾向于, 因为它们通常是可逆的, 诸如此类。
他们非常偏向于代表这些非常干净的结构，比如二维空间。
所以我的意思是，有趣。
但唯一的问题是，这只有一个注意力层。
所以通过使用良好的期望，使任务在处理X方面非常容易，并使用正确的位置编码，你必须用其中一个解决任务，也有点好。
也许一个很好的解释是，你可以深入研究这个网络中这些神经元在做什么，并真正理解，你知道，我们知道，位置编码看起来像是网格细胞。
我们非常深刻地理解了为什么网格细胞是一种有用的东西，如果你在做这种路径积分。
所以这就像，希望有助于解释所有这些事情。
哦是的。
如果有时间的话，我本来打算告诉你所有关于网格细胞的事情，那是我的爱好，但我觉得那没必要。
所以我会停下来。
问题。
好的。
很酷。
一个非常好的问题。
所以在最开始的时候，那些网格都连接到了一个，还是这些，那些。
是的。
这是你自己的回答之一。
是的。
虽然很疯狂。
让我告诉你更多关于网格细胞系统的信息。
因为你，因为你的电极被卡在这里，对吧。
而它们通常使用的经典测量技术是四极，即四根电极。
好的。
它们接收到这些尖峰，就像神经元放电的电流波动一样。
他们可以通过四根电极上的活动模式来三角定位他们测量到的特定尖峰，因为这个模式只能来自于一个位置。
因此，他们可以确定是哪些神经元发送了这个特定的尖峰。
但是，有一组神经元具有网格细胞模式。
很多神经元的模式只是彼此平移的版本。
所以，相同的网格，空间中平移，这被称为一个模块。
然后，有一系列模块，这些模块是相同类型的神经元，但网格大小要么大得多，要么小得多。
在 Raps 中大约有七个这样的模块。
所以，这是这七个模块的非常令人惊讶的晶体结构，但在每个模块中，每个神经元只是相对于另一个的平移。
嗯，有很多理论工作关于为什么这样做是很明智的事情。
如果你想进行路径积分以确定基于你的速度信号在环境中的位置。
好的。
所以刚才你说的这件事，关于友好的事情，这真的很有趣。
这是进化的产物还是学习的产物。
进化。
进化。
就像，在小老鼠出生后的第十天突然出现。
所以突然之间，哦，突然之间这个结构似乎非常倾向于被创建。
不清楚，你知道，我们当时在讨论它是如何被利用来编码其他事物的。
因此，它有多灵活或者有多固化是有争议的。
但似乎，你知道，FMRI的证据表明系统中有一些更灵活性。
不太清楚它是如何编码的，但能够通过神经记录来观察将是很酷的事情。
好的。
很棒。
让我们鼓掌更热烈一些。
