好的，各位，谢谢你们来参加我们的第二堂课。
今天我们很荣幸地邀请到了Faish Shah。
他是Google DeepMind的一名高级研究科学家，在那里他从事机器人团队的工作。
他实际上在这里获得了博士学位，与Stanford Vision and Learning Lab的Sylvio Salvariz以及Leonidas Pitis合作。
他的使命是构建智能的具身代理，这些代理可以与复杂且非结构化的真实世界环境进行交互，应用于家庭机器人等领域。
最近，他一直在探索使用基础模型来进行机器人的决策制定和动作生成。
那么现在我将把话题交给Bing。
大家好。
我非常高兴能够来到这里，也很高兴能够回到这里。
我两年前从这里毕业，现在是Google DeepMind的一名研究科学家。
我在机器人团队工作，今天我将讨论低层次的具身智能，使用基础模型。
所以这绝对是一个有趣的话题，我将介绍什么是具身智能，什么是低层次的具身智能，以及我们如何利用基础模型加速它们的构建。
好的，那么我们为什么要研究具身智能呢？
所以，具象智能是人工智能的一个组成部分，对于人工通用智能来说，这是一个重要的里程碑。
它有很多用例。
比如，我们都希望有一个家庭机器人，可以全天候在家里，为我们清洁房屋，整理凌乱的房间，为我们做饭，或者照顾年迈的家庭成员。
但我们还没有完全达到那个阶段。
事实上，我们离那还相当遥远。
这是因为我们的智能主要存在于虚拟世界中。
所以我们有一些人工智能代理可以帮助我们起草电子邮件或写出雄辩的论文，但它们在与人类生活的混乱真实世界、非结构化、复杂环境进行交互方面并不十分擅长。
所以，只是为了给你们举几个混乱的真实世界以及对机器人可能是多么敌对的例子，我想给你们展示我们的一个机器人犯下的一个奇怪的错误。
任务是将可乐罐放在水槽里，看看机器人会做什么。
机器人拿起了可乐罐并打开了水龙头。
所以这有点危险，但也很有趣，对吧？
因为我们从来没有想到它会做出这样的事情。
它只是从随机噪声开始打开水龙头，水就开始流出来了。
所以，要使一个代理有这种类型的物理智能，它需要理解其行为的影响，以及所谓的世界模型。
所以人们一直在抱怨，迄今为止的语言模型没有世界模型。
所以它不理解几何，不理解物体之间的空间关系或者行为的影响，基本上不理解根据物理定律物体将如何移动。
所以我们还没有完全达到那个水平。
在另一个案例中，这是我们的机器人，它准备递送一罐饮料，或者实际上是扔掉一罐饮料。
但是正如你所看到的，我们有这种预编程的行为，将手臂藏在身后。
这样做的话，罐子就是倒过来的。
所以如果罐子里有液体，液体就会溢出并损坏机器人。
这是另一个现实世界非常复杂的例子。
而且还有很多事情要建模。
为了使我们的机器人具有这种环境智能，它确实需要理解环境的许多非常微妙的细节，并理解物理、物理定律的影响。
我们该如何做到呢？
实现具身智能有许多方法。
实际上，在我的博士研究期间，我一直对创造互动环境的想法很着迷，基本上，让代理探索这个互动环境，基本上创造足够复杂的环境。
所以，如果代理需要在这样的环境中生存，它必须发展智能。
所以这是一种生态视角的感知和代理，由美国心理学家詹姆斯·J·吉布森推广。
所以他有一句著名的话，不要问你的头脑里有什么，而是你的头脑在什么里面。
所以人类学会了这种具身智能。
人类能够轻松地操纵物体，一是因为进化，二是因为童年经验。
我们一直在玩这个玩具，我们一直在与这个玩具互动并观察物理效果，这样我们就学会了。
同样，我们可以给机器人一个安全的游乐园，让它们在这些环境中探索、与环境互动、玩耍并观察行为的效果，从而有效地理解如何操纵这些物体。
所以我一直在开发这个仿真环境，其中之一就是被称为吉布森环境的，它已经发表在CVPR上。
主要是为了忠实地模拟视觉世界，并在一定程度上模拟物理世界。
因此，我们构建了这个环境，这是从许多房屋中扫描得到的环境，然后可以在其中生成一个代理，本例中是一个类人代理，代理可以学会在这个环境中行走或奔跑，并模拟所有这些感知信息。
因此，我们可以为这个代理创建一个感知行动循环。
类似地，我们可以将其他类型的代理放入这个环境中。
在这种情况下，是一个小推车，我们还可以把四足动物或这只蚂蚁放入这个环境中。
因此，我们基本上创建了一个环境，在这个环境中我们可以模拟代理的感知，然后我们可以创建一个神经网络来将感知映射到行动，通过这种方式我们实现了某种形式的物理智能。
这主要用于导航和运动。
这还不够。
在这种情况下，环境是一个整体网格。
正如你所看到的，代理会撞到墙上然后弹回来。
这个环境中没有关节。
它没有模拟环境的完整复杂性。
我们的代理可以做的事情相当有限。
这就是为什么我们创建了其他的模拟环境，其中之一是 iGibson 环境，也被称为 Interactive Gibson。
那么我们所做的是，再次扫描大量真实世界的房屋，然后将它们转换为 CAD 资产，基本上是可交互的网格资产。
在这种情况下，我们有一个模拟代理进入环境，然后关闭所有抽屉。
所以我们能够做到这一点是因为我们对世界的复杂性进行了更多的建模。
我们不仅仅模拟视觉世界。
我们开始更多地模拟物理，基本上模拟环境中的自由度，我们的代理不仅仅可以在环境中导航。
所以我们可以走得更远，我们甚至可以模拟更多的自由度，我们的代理可以发展出更复杂的行为，比如卸载洗碗机，找到一个碗，或者拿出碗放在桌子上。
因此，随着环境复杂性的提高，我们能够在模拟中学到更复杂的技能。
这是实现具体智能的一种方式，即构建足够复杂的模拟环境。
这不仅仅是我的研究，整个计算机视觉领域都正在经历一场范式转变。
之前我们专注于互联网人工智能。
我们整理了许多互联网数据集，用于研究分类、分割和检测等问题。
基本上所有这些都是计算机视觉问题。
现在我们更多地关注具身体的人工智能，这就是在问题中添加行动维度。
我们正在研究视觉导航、操作、重新排列、具身体的问答、遵循指令，而模拟器在某种意义上取代了原始数据集的角色。
有一件事情是不会改变的，那就是数据仍然非常重要。
我们仍然依赖大量数据来学习这种智能行为，无论是来自静态数据集还是模拟器。
所以在模拟中学习可能需要大量的交互。
举个例子，我们创建了这个 iGibson 环境，我们想要学习一种被称为穿过关闭的门进入房间的行为。
这是一个相当简单的行为，我可以在屏幕右上角展示。
所以，代理需要停在门前。
它需要停在正确的距离。
如果它停得太靠近门，它就无法伸出手臂。
如果距离太远，它就无法打开门。
然后它基本上就打开了门。
让我再播放一遍。
打开是一扇门。
当有足够的空间时，它就会进入门内。
然而，要学会这种类型的行为需要大约 50,000 次试错或者 1.25 百万次环境交互。
这是因为我们正在使用无模型的强化学习。
智能体正在探索这个环境。
它确实可以在任何地方停下来。
它宁愿在任何地方停下来。
所以我们给它一个奖励函数去走进房间，但是它很少会偶然发现这种行为。
我想争论一下，有了基础模型，我们可以做很多不同的事情，对吧？
那么你现在做什么呢？
你只需问一个 HIV-T，你怎么通过一扇关闭的门进入房间？
它会说，打开门，走过门口。
所以这是对问题的粗略简化。
当然，问题并不是那么简单。
但我只是想说的是，我们可以从基础模型中借鉴很多语义先验知识。
所以如果我们真的喜欢数据，如果我们真的需要大量的数据，基础模型就是整个数据的压缩版本，它是一个知识库，您可以查询它，以加速机器人技术的发展。
当然，模拟和真实世界的数据仍然非常非常重要，但也许我们可以兼顾两全其美。
我们可以使用基础模型加上有限量的模拟或真实世界的数据。
这就是我今天要谈论的内容。
那么在基础模型加上机器人技术方面我们处于什么地步？
所以我们在谷歌DeepMind团队一直在试点基础模型加上机器人技术。
因此，我们开发了先进的规划、高级规划算法。
首先之一叫做Palm Seikan。
这是一个可以解析用户命令的算法。
所以这里有一个演示，这里有一个场景，这里有一个用户命令。
我把可乐洒在桌子上了，你会怎么处理并给我拿点东西来帮忙清理？
它正在查询一个大型语言模型，该模型给出的分数用蓝色突出显示，还有一个Horton分数。
Horton分数将告诉您在给定状态下某个动作是否可能。
它在增强语言模型，只提供可能的内容。
因此，从本质上讲，它是在使用语言模型进行语义规划，但同时也考虑到了它能做什么。
因此，它并不只是输出语言模型，或者倾向于幻觉。
它不会产生幻觉。
它只会为你提供机器人可能做的事情，以及机器人可以采取的行动。
而机器人正在做的事情，就是推动长远任务的进展。
此外，每项任务都由一个低级策略来执行。
在这里，它并不完全符合表格的要求，因为我们还没有将其添加到低级技能中。
但想象一下，如果有很多技能来清洁桌子，它就会完成整个任务。
这里使用的低级策略是什么？
这里使用的低级策略是机器人transformer 1 RT1。
这是我们团队自制的transformer。
从本质上讲，我们收集了大量的人类演示数据。
我们放置一个transformer，并在这个专家轨迹的大型数据集上对其进行训练。
它能够完成大约 700 项任务，成功率高达 97%。
它还具有有趣的泛化行为。<EOS
它可以在之前从未见过的新厨房中运行，这表明在机器人技术中应用基础模型有成功的配方。
所以，大概就是我们在基础模型加机器人领域所处的位置。
我将谈论一些将这个提升到新水平的新工作。
实际上，我的队友泰德在今年年初就基础模型加机器人进行了演讲。
也就是这门课，CS25。
我强烈推荐它。
它在YouTube上可以找到。
昨晚我实际上就看了，以免我重复一些内容。
但他基本上提到的是我们团队在构建这些机器人基础模型方面的进展。
我们曾经有过很多小小的偏差，现在我们总算找到了一个方法。
所以在2021年到2022年，我们是如何通过示范来扩展到许多任务的。
我们如何收集大量的数据？
实际上，大约有10万次示范，我们尝试了不同的方法。
我们尝试了行为克隆，我们尝试了模仿学习，再加上强化学习，还有其他一些方法，或者将它们与Seikan这样的语言模型相结合。
在2022年到2023年，这是关于我们如何利用基础模型来加速机器人技术的。
我们确实看到了利用基础模型来加速机器人技术的大量应用，无论是在高层规划还是低层控制上，可能更倾向于高层规划。
所以，如果这个配方奏效了，那么这个配方实质上就是将大规模、多样化的离线数据集与高容量的架构（比如Transformer）相结合，并利用语言作为通用粘合剂。
这将是构建机器人技术基础模型的配方。
好的，如果这个配方奏效了，我们接下来该怎么做？
我们接下来该做什么？
实质上，我们只需要将一切按数量级扩展，完成并解决机器人技术问题。
猜猜看？
那就是我们所做的。
所以，这就是讲座的结束。
也许我们可以把这个说得简单一点。
这只是一个玩笑。
那是不可能的。
所以我们仍然在路上，继续解决低层次的具身智能。
当我告诉人们可以利用基础模型来做机器人技术时，他们的反应可能是，主要是进行高层次推理。
它并不擅长进行低级操作。
这是有原因的。
比如其中一个原因是莫拉维克悖论。
莫拉维克悖论观察到在人工智能和机器人领域，与传统假设或我们的直觉相反，推理需要非常少的计算，但感知运动控制和感知技能需要巨大的计算资源。
这是因为作为生物体，我们通过进化获得了感知运动技能。
这是非常不同的。
因此，我们可能无法进行推理或进行大规模计算，但这种感知运动控制对我们的生存至关重要。
所以它基本上已经学会了我们的DNA。
但在机器人领域，情况有些不同。
所以芯片非常擅长推理和计算，但它们并不是非常擅长。
它们没有经历过这个世界。
它们没有获得在现实世界中执行任务所必需的感知运动技能。
这里有一个例子。
当计算机击败卡斯帕罗夫，基本上是国际象棋的人类冠军时，并不是一个机器人手臂在移动棋子。
它可以在国际象棋中击败人类冠军，但仍然有人需要移动棋子。
同样，在AlphaGo时刻，当莉莎被AlphaGo击败时，仍然有人在为她们移动棋子，而不是机器人在做这件事。
因此，这显示了推理是艰难的事情很容易，而简单的事情很难。
还有一件事阻止我们更广泛地在机器人领域使用基础模型，那就是训练数据的偏见。
像基础模型或大型语言模型的训练数据大多是与语言相关的任务。
因此，也许并不奇怪它知道如何清理厨房，因为也许有wikiHow文章教你如何清理厨房或以程序化的方式做某事。
但没有wikiHow文章教你如何将手指向左移动5厘米，因为人们就是不这样说，人们不写下这样的事情。
因此，在大型语言模型训练Copra中，这种低层控制数据非常有限。
因此，在将基础模型引入更低层次时，我们面临着许多挑战。
这就是我所说的低层次具体智能。
到目前为止，有任何问题吗？
另外，我想使这更加互动，所以如果有任何问题，请随时与我互动。
好的，如果没有，我们可以继续。
使用大型语言模型进行低级控制有一些挑战。
正如我刚才提到的，第一件事是缺乏数据。
所以我们只有大约 100,000 个人类示范数据的剧集，而且需要大约 13 台机器人花费 17 个月的时间来收集。
所以这是一项巨大的努力。
相反，大型语言模型是在千亿级令牌的数量级上进行训练的。
较小的 POM 训练了 7800 亿令牌，而较大的模型被训练。
按照毛茸茸动物规则，您将需要对其进行 1.35 万亿令牌的训练。
因此，在机器人领域，我们能够获得的数据量与在大型语言模型中获得的数据量之间存在巨大差异。
所以我们将始终受到机器人数据的限制。
也许我们可以在其他方面进行扩展。
也许我们可以保持机器人数据不变，然后在其他方面进行扩展。
比如，也许我们可以扩展文本和图像的蛋白质混合，或者图像和文本对。
也许我们可以构建这个蛋糕，而机器人数据只是其上的一颗樱桃。
我们可以非常好地扩展基础。
我今天要谈论的一些工作实际上重用了 RT1 数据。
我们不会为 RT2 收集新数据，但我们希望能够用同样数量的数据做更多的事情。
第二个挑战与第一个挑战有些相关。
语言模型缺乏低层控制接口。
如果你问一个语言模型，怎样让一个机器狗站起来两只脚？
它会告诉你很多听起来合理、听起来可信的事情。
它会告诉你机器狗的躯干直立，平衡于两只后脚之上，并且两脚间距站立在肩膀宽度处。
这很好，这些都很好，但我们无法把它应用到机器人身上。
另一方面，也许我们可以要求语言模型编写直接控制机器人的控制代码。
但通常需要你创建一个对语言模型友好的 API。
如果我直接要求它提供我的关节角度来使机器人直立，它不会给出正确的结果，因为它没有足够的上下文。
所以，基本上，大型语言模型不懂机器人语言。
我们真的能找到正确的机器人语言吗？
我们能找到大型语言模型和机器人控制之间的接口吗？
还是我们可以把机器人的动作看作是另一种语言？
这就是我们想要找出的答案。
所以在今天的议程中，我将讨论基于基础模型的低级具体智能。
它分为两个部分，它正在解决我刚刚提到的两个挑战。
第一部分是关于模型整合、联合缩放和积极迁移。
所以我必须把它们放在一部分，因为它们在某种程度上是相关的。
第二部分是开发大型语言模型的新接口。
那么，模型整合是什么意思呢？
模型整合，是的，问题。
是的，我正要问，为什么你不能使用精细调整在LLM中生成特定的低级代码？
你需要一些文本描述或其他吗？
是的。
是的，这是一个很好的问题。
所以问题是，为什么我们不能对语言模型进行微调，直接输出低级代码或机器人动作呢？
所以我将要谈论RT2，它在某种程度上做了类似的事情。
这是一个经过微调的语言模型，可以输出动作作为一种语言，来输出我们的动作表示。
对此有一些不利之处。
比如，你需要收集额外的数据来微调语言模型。
所以要么我们可以对其进行微调，要么我们可以在找到正确的接口后，使用零-shot语言模型，我将在第二部分中稍微谈谈这个问题。
所以在不进行微调的情况下。
所以模型的整合，基本上，我们可以在一个模型中进行高层次的推理和低层次的控制。
联合扩展不仅是我们扩展机器人数据，这是昂贵的，我们还扩展预训练数据。
或者我们已经从预训练的视觉语言模型开始了。
积极的转移是模型受益于互联网规模的语言、视觉和视觉语言领域的多样联合训练。
所以这是Tad在之前的讲话中绘制的轴线的延续。
所以我们可以看到有一个趋势。
因此，这个可视化基本上突出了我们团队的一些工作。
而每项工作，每列基本上都是一个能够进行高层次推理和低层次控制的机器人系统。
以前，我们需要为每个事物单独建模。
在 Seikan 的最初版本中，计划由一个大型语言模型完成，而 affordance 则由 Qt opt 完成，就像是用 Sim2Real 训练的策略。
因此，低级策略是 RoboticTransformer1。
因此，每个模型都在执行其专门的任务，您需要使用不同类型的数据对每个模型进行训练。
后来，我们有了 Q -Transformer，它统一了离线强化学习方法，利用了 Transformer 结构。
因此，这是一个高容量的架构。
它可以在正面数据和负面数据上进行训练。
有了这个，我们能够收集一个也能理解 affordances 的策略。
因此，我们可以统一低级策略和 affordances，但是规划仍然是一个大型语言模型。
然后我们有了 PAWME，这是一个视觉语言模型，也是一个在视觉语言领域训练的大型语言模型。
因此，PAWME 可以在一个模型中完成规划和 affordance，但是低级别仍然使用 RT1。
最后，我们将所有内容统一起来。
就像今天我要谈论的RT2一样，它可以在一定程度上进行高层规划，生成可供利用性并执行低层策略。
模型整合背后是任务的整合。
我们可以将每个任务表示为一个视觉加文本到文本的任务。
这是任务的一个非常通用的表示。
有了这个，你可以用大量数据来训练它，并且你可以看到积极的转移。
基本上，学习可供利用性也可以告诉你如何完成任务，就像当你把所有任务都整合在一起时，任务之间会有转移。
所以要理解这种联合扩展和理解模型整合，我们需要稍微了解一下POM-E。
所以POM-E是具有体现的多模态语言模型。
它基于POM架构。
所以POM是一个大型语言模型，我们对架构进行了一些改进，使其能够理解多模态输入。
所以它基本上是一个能够接受多模态输入的模型。
在大型语言模型中，每个单词都被标记化，然后将这些单词的嵌入送入一个大型语言模型中。
所以在Palmini中，我们所做的是不使用单词，而是可以使用多模态标记。
所以多模态标记可以来自视觉，可以来自视觉transformer，即VIT，也可以来自机器人的感官数据。
所以每个多模态标记，然后我们将其映射到文本嵌入空间。
我们基本上训练了多模态标记与文本嵌入空间之间的线性仿射变换。
然后我们也可以将多模态标记视为单词。
所以基本上我们有一个语言模型作为一个坚实的基础，然后我们开始适应它以理解多模态标记。
所以这非常有趣，因为它不需要大量的适应或微调就能理解多模态输入。
它自然地对齐到多模态输入，比如图像。
我将展示一些它能做的例子。
而且我们可以像训练大型语言模型一样进行训练。
所以基本上我们可以重用相同的基础设施和训练算法以及一切来训练这个PAL-ME。
我们在这个过程中发现的另一件事是正迁移，稍后我会分享。
所以我想在这里也要提一下POM-E是我们迄今为止探索过的最大的模型之一。
它有5620亿个参数，这是通过连接POM、5400亿个参数和22亿个VIT得到的。
我们发现这些模型具有许多新出现的能力。
也就是说，在训练期间我们没有预料到，但实际上我们可以提示这些模型并要求它做一些有趣的事情。
我们还探索了使用神经场景表示，基本上是一种以对象为中心的表示，输入到PalmE中。
因此，以对象为中心的表示将一个令牌分配给每个对象。
我们发现，这种表示对于机器人规划任务非常有帮助，因为传统的VIT表示是基于网格的，并且它没有对轻对象及其关系的完全理解。
我们在论文中对规模性能和灾难性遗忘性能以及所有其他有趣的实验进行了广泛研究，请参阅论文以了解更多。
所以这里我只是展示了一些有趣的定性例子或我们发现的Tommy的一些新出现的能力。
首先，我们发现这个模型具有一定的推理能力。
您可以给它一张图片，然后问它一些需要一点推理的问题。
而您可以用让我们一步一步地思考来提示这一点，这是一种用于引发大型语言模型推理的技术。
但在这里，在多模态语言模型中，您也可以做同样的事情。
我猜人们这些天也在尝试使用GPT-4V进行实验。
您还可以提示它一步一步地思考或逐行计数。
但在这里，这是在GPT-4V之前，我们能够使用一些有趣的提示引发推理，例如我们可以问它，在这张照片中，猫多还是狗多？
让我们一步一步地思考。
而POMI发现狗和猫的数量相等。
在右边给出一张图片，我能骑自行车沿街而行吗，是还是不是？
让我们一步一步地思考。
回复是，不要进入，其次，除了自行车，不要进入，是的。
所以它正在进行这种谨慎的推理，将符号理解和文本理解混合在一起。
对我来说，当我第一次看到这个时，这真是令人惊讶。
我没想到一个多语言模型居然能够做到这一点。
而且我们还尝试了一件传统上对语言模型来说非常困难的事情，那就是讲一个笑话。
语言模型能理解笑话，但有时它却做不到。
当谈到笑话的妙处时，它无法告诉你一个笑话，因为它只是试图制造出一些看起来像笑话的东西，但又是合理的。
而当谈到笑话的妙处时，它真的不知道该说什么。
所以在这里，我给它一张图片，然后让它想出一个描述，然后再想出一个笑话。
这样可以引导语言模型循序渐进地思考。
描述是一只驴背着一只狗、一只猫和一只公鸡。
而笑话是，一个背着公鸡的驴叫什么？
一个“鸡尾巴”（音译，意为：鸡的尾巴，与“提神剂”谐音）。
这真是太有创意了。
就像当我看到这个时，我感到惊喜，我在网上搜索，但找不到另一个像这样的笑话，所以这实际上是Pommy的原创笑话，最后我们看到了这个模型有一些大规模推理能力，基本上我给它提供了一份比萨店的混乱菜单，然后问它我和朋友买一份比萨要付多少钱，我们来循序渐进地思考一下，它正在弄清楚有一份比萨，价格是$9.99，然后告诉你价格。
在一些答案中，它甚至计算了税，但税是幻想出来的，所以这不起作用。
好的，让我们谈谈正向迁移。
所以除了PAOMI能够做的惊人的事情之外，它还具有有趣的正向迁移行为。
当我们在单一领域训练PAWMI，当我们仅在单一的机器人任务上训练它时，性能并不是特别出色，但当我们将所有数据汇聚在一起，同时还包括互联网规模的视觉语言任务，比如字幕或视觉问答，它就能够表现得更好。
所以这表明将所有数据混合在一起并联合训练是重要的，互联网规模的数据可以作为一个正则化项，使你不会忘记这些表示。
而这些表示反过来对机器人非常有用。
这就是正向迁移的结果。
而且我们开始在我们的其他研究中看到越来越多的正向迁移。
是的？
那么你有多少数据要进行集体训练，比如在模拟中还是在现实世界中？
我认为在帕洛阿尔托进行排序的那些东西非常令人印象深刻。
是的。
是的，这是一个非常好的观点。
所以这些都是规划数据，像高层次的规划。
那么也许让我们只谈论两件事情。
首先，排序结果，低级策略仍然使用传统的控制器。
所以它使用的策略叫做 Lava。
而且这个策略是在 68,000 个剧集上进行训练的。
高级规划可能比你想象的要简单，因为它只需要告诉，它给低级策略下达命令。
所以基本上只需要说，把红色的块放到左上角，再把另一个红色的块放到左上角。
所以这其实是一个相当标准的自回归任务，就像语言建模任务一样。
我唯一需要做的事情就是确定哪些任务还没有完成。
所以例如，如果块已经在角落了，就不应该再调用策略将其移动到角落。
所以这更像是解析状态并理解状态。
所以这个高级策略只需要大约 50 到 100 个演示来学习。
所以它非常参数高效。
至于未来，这是一个非常好的问题，实际上。
在未来，很多这些任务可以在上下文中教授。
所以也许我们只需向大型语言模型演示一次，然后它就知道如何做了。
大型语言模型如何知道应该使用哪个低级策略？
是的，这也是通过人类演示来实现的。
因此，低级别的人类可以通过遥操作机器人执行特定任务来展示低级策略。
但在高层次上，它也可以直接提供低级策略。
想象一下，你的控制界面是通过文本实现的。
然后作为人类，你也可以指导低级策略完成任务。
然后，这个过程可以用来训练大型语言模型。
所以这是关于排序模块的内容。
切线部分更有趣，因为规划步骤实际上是由Palm生成的。
因此，我们实质上将Palm和这个属性模型提炼成了Palm E。
这就更有趣了。
这就像是利用AI数据来自举自己。
其中有大约3000个剧集，虽然不算多，但它能够学习复杂的规划行为、重新规划行为、错误恢复，我将在幻灯片中展示。
因此，借助Palmi作为高级规划器，我们能够将大米片从抽屉中取出。
然后有一个转折，就是我将和机器人互动。
当它被放到柜台上时，我将它放回抽屉。
当再次拿起它，然后我再次放回去。
因此，它能够理解状态。
它能够理解我的任务尚未完成。
我无法进行下一个任务。
现在，在我不再乱弄它之后，它能够关闭抽屉并拿起备用芯片。
所以汤米能够将可供性和规划结合在一个模型中，并进行复杂的推理来观察环境。
有趣的是，我们可以使用完全相同的模型检查点进行块排序。
所以这是相同的模型检查点。
它不仅能够思考如何将一袋薯片带给用户，还可以对块进行排序。
它还能够应对对抗性扰动，比如用户再次将块放在中间，它能够从中恢复过来。
所以这些都来自同一个模型。
它还能讲笑话。
是的，这就是视觉语言模型的力量。
现在我们想深入一层。
这些都是用于规划或高级推理的视觉语言模型。
我们能否将它们用于低级控制？
原来我们是可以的。
这就是RG2的工作，它是一个视觉语言动作模型，将网络知识转化为机器人控制。
那它能做什么呢？
当被要求时，捡起这个已经灭绝的动物。
桌子上有各种各样的物体。
它会捡起那个恐龙。
因此，它可以将这个已灭绝的动物与恐龙联系起来，并执行捡起恐龙的动作。
所以它实际上在进行这种新型的推理，同时在一个模型中进行操控。
顺便说一句，这个机器人以前从未见过这些东西，至少在机器人的训练数据中是这样。
它可能在互联网目录中见过这些东西，对吧？
但在机器人的训练数据中，它从未见过这些。
所以我们现在需要评估这些机器人，这是相当有趣的。
当我们评估语言模型以防止数据污染时，每次都需要给它新的问题，否则它可能已经在训练中记住了。
当我们评估这些机器人时，我们实际上会去杂货店购买所有这些玩具，以确保它以前没有见过。
随着我们进行更多的评估，可能会有一些复制。
但是正如你所看到的，它能够理解拾起这个恐龙玩具。
我们是如何做到这一点的呢？
因此，我们从一个在互联网上训练的视觉语言模型开始 - 规模化数据。
然后我们还将其与机器人行为数据相结合，这就是RT1数据，我们得到了RT2。
我们可以更深入地探讨一下RT2。
首先，什么是视觉语言模型？
视觉语言模型是一个接受图像和文本然后输出文本的转换器。
因此，在Google内部，有一个名为Pali的视觉语言模型，它是一种编码器 - 解码器类型的架构。
它基本上具有一个VIT来理解图像，然后是一个transformer编码器和一个transformer解码器。
它们包含了视觉和语义来理解世界。
在机器人技术中，我们必须处理大量的这两者。
问题是，我们能否利用视觉语言模型中的知识并将其应用于机器人技术？
另一方面，我们有RT1。
如果您想了解更多关于RT1的信息，您可以听Tad在本CS25的上一集中的介绍。
所以他对RT1进行了详细介绍。
但是 RT1 的关键是，如果你站得足够远，它也是一种将视觉语言转化为行动或其他模型。
它接收人类的指令，接收当前的摄像头图像，摄像头图像经过一个高效的影片网络，被标记为 81 个标记，然后传递到一个标记学习器，将所有信息压缩成八个标记。
然后有一个transformer块利用了很多自我-注意力层，然后生成动作。
动作也被标记化。
机器人有七个自由度。
基本上，抗-因素有六个自由度，作为位置和旋转，夹持器可以打开和关闭。
还有另一个维度代表是否终止这一集。
终止意味着我的任务已经完成了。
我们将每个维度离散成 256 个箱，然后在这些箱上进行交叉熵损失。
这就是 RT1 架构的要点。
它与一种视觉语言模型非常相似，只是输出标记不同。
因此，我们可以直接使用一个大型预训练的视觉语言模型作为策略。
我们可以使用 Polly 或 Polyme 作为策略。
一个问题是，当使用预训练的视觉语言模型时，我们如何处理动作。
这里是我们使用的动作表示。
这里的机器人动作有八个维度，正如我提到的，有终止、位置变化和旋转变化。
我们将所有东西离散化为 256 个箱子。
我们也尝试了其他替代表示方法，但它们并不像这个天真的表示方法那样好。
是的。
是的。
是的。
哦，影片有效网络是一个预训练的卷积神经网络。
它用于对图像进行标记。
我们这样做的原因是通过一些消融研究，我们可以以不同的方式对图像进行标记。
我们可以用 ResNet 对图像进行标记。
我们可以将所有东西标记为 ResNet，也可以使用影片有效网络进行标记。
影片，它的意思是它还考虑了语言嵌入，并将其附加到 ResNet 的中间层。
所以基本上你有一些新的字母组合被编码到图像中。
是的。
然后经过这个神经网络，这个基础层。
没错，没错。
重新排列，然后进行转换。
所以我会提及输出，比如，用于动作树。
没错。
在代码中的动作是什么？
动作不在代码中。
所以动作在文本中，基本上就是这里显示的内容。
这就是动作。
它由八个数字组成。
每个数字的范围是从0到255。
也许关于电影 ResNet 的另一个注解。
关于我们是如何对图像进行标记化以及如何结合视觉信息和语言信息的。
有许多方法可以做到这一点。
这不是唯一的方法。
有早期融合和晚期融合。
还有交叉注意力。
基本上你可以仅仅对图像进行标记化，然后你可以使用交叉注意力来结合图像和文本表示。
所以在这里我们使用了这个模型。
这是用于机器人技术的 RT1。
所以我们确实考虑了许多因素，比如延迟。
这就是为什么我们使用了这个电影 RustNet，因为它运行速度非常快，而且可以输出有限数量的标记，我们可以进一步用标记学习器进行压缩。
是的。
是的。
是的。
这是自回归的吗？
就像，它看到的每张图片，但然后它只是每张图片之间。
对，所以它是自回归的，是的。
每次我们使用长达六步的历史。
所以每次你看到这张图片，就在它之前看到大约两秒的历史，这将成为你的输入。
是的，如果你对RT1有更多问题，我建议观看前一集。
这里是关于RT2的一切。
所以我们可以转换数字串。
这将是我们的转换器输出，它是一个视觉语言模型。
我们尝试过其他替代方案，比如浮点数。
浮点数对语言模型分词器不太友好，因为它有这些小数点。
我们还尝试了人类语言，比如左或右。
这是更语义化的表示，但它们不能直接在机器人上执行，这是这种方法的限制。
所以如果我们致力于这个动作表示，它只是一串数字，我们基本上得到一个视觉语言动作模型。
我们尝试了不同的变体，包括polyX。
这是一种路径语言图像模型。
有5十亿参数的变体和5十五亿参数的变体。
我们还尝试了POME，它有12十亿参数。
我们用来训练这个RT2的过程是通过协同微调。
协同微调是将互联网规模的数据和机器人数据放在一起。
然后我们对这些混合数据进行微调，以便保留互联网规模的知识。
也许这也是我们的数据太小，不够多样化的一个特征。
所以如果你只是在机器人数据上进行微调，它会很快过拟合，忘记所有这些蛋白质混合物。
也许这是一个规模的动态。
所以我们将会看到。
在推理时，我们如何做到这一点？
基本上，我们再次自回归地执行此操作。
我们有一个任务的指示。
我们将其格式化为一个问答任务。
机器人应该做些什么来完成某个任务？
任务是人类给机器人的一个字符串，让机器人去实现。
它还有当前的观察结果，这是机器人的观察结果，相机图像，RGB图像，经过VIT传递，然后通过大型语言模型，然后输出一系列令牌。
所以我们利用约束解码来确保它始终有八个数字。
因为否则，我们无法去-标记它。
对于语言模型来说，错过一个数字是很容易的，对吧？
因此，我们确实有一些机制，比如约束解码和波束搜索，来确保格式正确。
在我们得到一个包含八个数字的字符串之后，我们将其解码为Δt和Δr，即反-因子Δ后。
机器人可以直接在机器人上运行这个。
在机器人上运行之后，我们重复这个过程。
我们得到另一个新图像，通过这个过程，得到一个新的动作。
然后我们重复这个过程，直到解码出终止信号。
所以有些人可能担心这会比较慢。
事实上，它确实很慢，因为它有120亿个参数或50亿个参数。
我们无法在机器人上运行，所以我们在TPO集群上运行，机器人正在查询TPO集群以获取数字并将其应用在机器人上。
对于120亿个参数，我们实际上可以以10赫兹的速度运行。
所以速度相当快。
对于所有这些模型，我们至少可以以三赫兹的速度运行。
对于控制机器人来说，这已经足够了。
我们看到许多新兴的技能，这些技能并没有经过训练，也不在训练集中。
基本上，正如我刚才提到的，我们正在探究RT2能做什么。
实际上，我们不知道。
所以我们正在努力弄清楚RT2能做什么。
因此，我们测试了许多新任务，比如把草莓放到正确的碗里，或者把香蕉移到德国，只是为了测试它对符号或旗帜的理解。
选择一种陆地动物。
有一匹马。
还有一只章鱼。
我们基本上测试了语义推理以及低水平的操作技能。
我们将任务分为简单的理解与推理，人类识别和平均水平。
我们发现，在没有在互联网规模数据上训练的RT1中，在这些新出现的评估任务中表现相当差。
而在RT2变体中，它在互联网数据和我们的机器人数据上进行了共同微调，我们在这些任务中表现得更好。
还有一个规模的效应。
因此，拥有55亿个多边形的RT2表现比有12亿个多边形的要好，尽管它们在域内任务中表现相似。
但是泛化效果是有趣的。
似乎随着规模的扩大，你可以更好地进行泛化。
这里有一些机器人完成这些任务的视频，比如把香蕉移到一个数字上，把草莓放入正确的碗中，把魔方移到水瓶上，但我说的是中文，把香蕉移到德国国旗上。
所以它能够完成所有这些非常有趣的任务。
就定量评估而言，我们还发现 RT2 策略对未见过的物体、未见过的背景和未见过的环境相当稳健。
这里还有另一个积极迁移的证据。
因此，与仅在机器人上微调相比，与 VQA 数据一起微调的性能更好。
如果你从零开始在机器人数据上进行训练，它几乎不起作用。
它几乎不起作用，因为它对机器人数据进行了过度拟合，而我们的机器人数据只是太小了。
所以我们确实需要进行共同微调，或者至少是微调，以便保留其网络技能知识。
这也是人们如何开发特定领域视觉语言模型的配方。
所以你可以从一个非常通用的视觉语言模型开始，然后在你的领域上进行微调，或者你可以与你的特定领域数据一起进行共同微调。
这很可能是人工智能每个垂直领域都会遇到的问题。
我们也可以在其他平台上进行测试，就像这展示了一些跨领域的情况。
在移动方块的二维环境中，RT2，PolyE3b在性能上优于先前的模型。
在大型语言模型中，我们有这种思维推理的链条，这是一种在大型语言模型中引导推理的方法，您可以通过逐步引导来进行零短板推理。
我将为您提供推理的示例。
基本上是解码更多信息，然后得出结论。
我们也可以在RT2中使用类似的过程。
因此，在RT2 Palm E中，我们可以不直接解码动作，而实际上解码一个计划，然后将其与动作一起追加。
这为语言模型提供了理解问题或以不同方式解析问题的机会。
这也为我们提供了对事物进行一些推理的机会。
例如，如果您说，给我拿杯饮料，它可能会说，拿起7 -up罐，因为桌子上有一罐7 -up。
所以我们使用一个大型语言模型合成了几百个这样的例子，只是通过增强指令，然后对RT2进行了一些微调，只进行了几百个步骤。
所以它介于完全微调和上下文学习之间，它能够进行一些推理。
一些有趣的推理任务包括，我需要钉一根钉子，场景中的哪个对象可能有用。
在场景中，有一个耳机，有一块石头，还有一个便条。
机器人会说，石头，然后生成拿起石头的动作。
所以有趣的是，它能够用RT2进行这种推理。
这里是一些RT2 PAWME推理链的演示。
任务是拿起与所有其他对象不同的东西。
它拿起巧克力，因为这是一种零食，其他东西都是饮料。
我也可以说一种不同的语言。
计划是将其翻译成一种熟悉的语言，即英语，然后执行任务。
链式推理也存在潜在的失败案例。
所以这里我说移动绿色物体在一起。
正如你所看到的，机器人在两个绿色物体之间振荡，因为有两个相当的计划。
它可以把罐头移到薯片袋子里，或者把薯片袋子移到罐头旁边。
它在两个计划之间摆动，直到某个动作将其带到一个物体，并且它会承诺其中一个计划而不是另一个。
虽然不总是保证能够成功，但是这相当有趣。
而且有趣的是，我们再次测试了操纵策略，就像我们测试人类、动物或孩子的智能一样，因为它们变得越来越先进。
总而言之，我们有了视觉、语言和行动模型，能够提高泛化能力。
它可以完成新任务并操作新物体。
它还可以进行思维推理，并通过扩展并使用互联网规模的数据来训练视觉语言模型本身，或者通过使用更大或更高质量的互联网规模数据来训练它，我们可以实现更好的机器人控制，这相当令人惊讶，因为机器人领域一直以来发展相当缓慢，并受到硬件的限制，受到许多不同事物的限制，受到运行的限制。
但现在看来，我们可以搭上基础模型领域的发展，无论他们做什么都会传导到我们领域。
未来的发展方向将是增加运动多样性，扩展思维链推理能力等等。
所以，这里有另一个正迁移的例子，你可能最近已经见过了。
到目前为止，我一直在谈论不同的规模化，对吧？
我一直在谈论不要扩展机器人数据，而是扩展其他数据。
这是因为机器人数据很难收集，目的不是为了避免收集机器人数据，而是要制定一个配方，使你可以在有限的机器人数据上做更多事情。
然而，我们团队和整个机器人领域还在努力扩大机器人数据收集的工作，这就是所谓的OpenX Embodiment。
模型链被称为RTX Robotics Transformer X。
它基本上包括22种类型的具象化和572种技能，以及60个数据集汇总在一起。
因此，这将是我们用来研究正迁移和研究这种联合扩展的最终数据集。
已经有正迁移的证据了。
所以我们从所有这些实验室汇总了所有数据，并找到了一个共同的动作表示，我们可以用来训练一个机器人transformer。
而且我们已经发现，这个联合训练的模型可以胜过每个实验室开发的特定任务模型。
所以将所有数据汇总在一起是有一些好处的。
所以扩展机器人数据也非常重要。
因此，这部分的总结是我们正在进行模型整合。
我们现在可以在一个模型中进行高层推理和低层控制。
而低层控制部分让我感到兴奋，因为它远离了传统语言模型的领域。
它是如此不同，并且显示出我们可以推广比我们过去认为的更多的东西。
而且我们可以扩展视觉语言模型的预训练，以及扩展机器人数据。
我们观察到越来越多的正面转移模型受益于跨互联网规模、语言、视觉和视觉语言领域的多样联合培训。
好的，我注意到我们快要用完时间了，所以我会很快地浏览一下第二部分，我认为这也很有趣。
这是为了找到语言模型的新接口。
但我只会从非常高的层次上讨论。
所以语言模型，正如我们所看到的，如果我们找到了动作表示，可以直接输出动作标记。
所以我们可以将动作视为语言模型的另一种语言。
所以语言模型可以进行翻译。
所以它应该也能够生成动作。
但这需要精细调整。
我们能否在不进行精细调整的情况下做到呢？
或者我们能否生成更具表现力的动作，超出了精细调整的范围？
所以这就是找到正确接口的问题。
所以以前，我们已经确定语言模型没有动作接口。
如果它有一个动作接口，那么它的效果就不是很好。
那么语言和很多动作之间最好的接口是什么？
我会认为语言模型和动作之间最好的接口是奖励函数。
奖励函数是普适的。
它已经被用于强化学习。
它也是动作的重新参数化。
什么是动作？
让我们看看如果我想拿起这个瓶子。
我可以说，那么，什么是技能？
技能是我的观察和我的动作之间的映射。
因此，我的观察和动作之间的映射可以被视为一种技能。
但是技能可以有另一种定义，即一组约束和一组目标。
因此，拿起瓶子意味着瓶子在我的右手上，并且瓶子不在支撑表面上。
那就是拿起的意思。
而我是如何拿起它并不重要。
这更多地是对技能的广义定义。
它在不同技能之间更具可转移性。
约束和目标可以被表示为奖励。
因此，我们可以要求语言模型生成这些奖励函数，然后有一个优化器。
它可以是强化学习，也可以是模型预测控制，以优化这些奖励，然后在机器人上运行。
那么奖励翻译器中有什么呢？
它是，让我们打开一个盒子。
因此，奖励翻译器基本上是一个两阶段的过程。
它使用相同的语言模型，使用两个不同的提示。
因此运动描述基本上描述了运动。
所以刚才我们发现语言模型可以输出机器狗应该如何站起来的描述，但它无法实现。
但是动作描述仍然合理。
这仍然有意义。
它给出了正确的东西。
所以我们只是生成了这个动作描述。
然后我们有一个奖励翻译器，奖励编码器，将这个动作描述翻译成代表奖励函数的一段代码。
这些奖励函数不能直接在机器人上执行。
但是它可以通过我们的优化过程来学习如何实现这些奖励函数。
所以我们正在使用奖励作为语言模型和低级控制器之间的接口。
而对于低级控制器，我们使用的是Mojoco MPC，这是一种模型预测控制算法。
它基本上是一个黑盒控制器。
它对许多轨迹进行抽样，并找到优化您奖励的轨迹之一。
我们在一个机器狗上进行了测试，基本上是一个四足机器人，还有一个葡萄糖操纵器。
所以葡萄糖操纵器有六七自由度的手臂和一个手。
它无法控制，因为它有这么多自由度。
所以这是非常具有挑战性的。
所以为了展示一些例子，我省略了运动描述部分。
我只输出了奖励代码部分。
所以似乎语言模型能够生成正确的奖励函数，使机器人像人一样站立在两条后腿上。
好的，现在我们有点更有雄心壮志了。
我们知道它能够站起来。
我们能让机器人这样站立的同时做一个月球步吗？
月球步是迈克尔·杰克逊的动作，这非常具有挑战性。
我们如何让机器人做到呢？
所以它生成了运动描述并生成了奖励代码。
但是动作不太正确，不完全是我们想要的。
使用语言模型和奖励函数的好处是你可以指导机器人。
你可以回去解释出了什么问题，然后要求语言模型修复它。
所以现在我们可以说你很有耐心。
你说月球步意味着机器人应该向后走，同时脚摆动得好像它们在向前移动。
这样一个很好的解释。
向我的同事致敬。
并且正确修改你的答案并使其以每秒0.5米的速度行走。
而且在你非常耐心并给出正确指示之后，它能够修改奖励、运动描述符，并生成正确的奖励集以实现这一点。
现在你可以通过使用语言作为接口来教会机器人跳月步。
总有一天，我们也能在真正的机器人上做到这一点。
是的？
因此，在前一节中，我们展示了如何渲染所有的人工状态数，并将它们限制在人工状态数中。
在这里，你如何防止它在某种程度上出现幻觉的编程？
对，这是一个很好的问题。
在这项工作中，我们并不是在以编程方式防止它的幻觉。
我们有一套系统提示，或一套规则，来解释API。
毕竟，奖励函数需要能够由优化器编译。
然后，是的，所以我们确实需要一些检查。
而且，如果它无法编译，我们可以将错误消息直接提供给语言模型。
它不必传播到运动描述符。
它可以留在奖励解码器。
如果有错误，请修复它。
所以在那之后，它应该能够修复它。
我们也可以使用这个框架将多个任务链接在一起。
我们可以说，打开一个抽屉，拿苹果，放进抽屉，然后关闭抽屉，它就能做到。
我们尝试过，仅仅使用奖励编码器是不够好的。
其实我们的两阶段提示非常，非常有帮助。
我认为这对其他领域也是一种启发。
当您的领域与语言领域差异太大时，也许最好找到一个中间表示，并要求语言模型在那个中间表示下解释，然后再直接进入一个更加晦涩的表示。
最后，我们想要将这个转移到现实世界，但是有一个挑战。
在模拟中，它可能会产生太灵巧的动作，比如这个动作在现实世界中是不可能做到的。
所以我们添加了一些额外的正则化项来稳定运动。
我们还对真实机器人进行了一些状态估计，以便它们了解方块的位置。
然后我们可以在模拟中抓取动作，然后在现实世界中实现它。
这里是现实世界中的一些执行情况。
你可以说，拿起魔方，它会生成拿起魔方和二极管的动作。
所以这与RT2相当不同。
动作非常流畅。
速度相当快。
比3赫兹快得多。
所以这里，它可以达到10赫兹甚至30赫兹。
所以它与人类的脚相当。
好的，那就是一种语言Q奖励。
还有一件事我想谈谈，就是找到一个新接口。
所以，很多时候，我们一直把语言模型看作是一个语义引擎，一个语义机器。
它理解语义。
所以，例如，你说学生拿出书，你会说书，就像语言模型能够推理出这样一个序列。
但是如果你只是给它一些晦涩的数字，你能做什么呢？
实际上这是一个低层接口，我们可以向语言模型打开低层接口，并要求它执行机器人任务。
所以在这篇论文中，大型语言模型作为通用模式机，我们探索使用大型语言模型的低级接口，基本上要求它推理不同的序列，这是令人惊讶地非常有效。
它可以解决像 ARC 挑战和 PCFG 这样的任务，甚至可以进行序列改进。
所以我会稍微深入一下序列改进，因为这与机器人学非常相关。
序列改进是指你用状态动作和奖励元组提示语言模型，然后只需用更高的奖励提示它，看看它能否生成达到更高奖励的动作。
所以它在上下文中进行了强化学习或类似强化学习的事情。
这真是令人惊奇。
以前你需要专门的算法来收集数据，重播缓冲区来进行这种强化学习，但现在我们可以通过利用语言模型的低级接口，直接在语言模型上下文中构建所有内容。
有了这个，我们实际上可以做一些像点击器训练这样的事情。
所以如果你对点击器训练不太熟悉，这是如何训练狗的方法。
你可以有一只狗，当它做对了事情，你就用点击声给它一个奖励。
另一方面，点击器训练是给予代理奖励的一种方式。
我们现在可以使用点击器训练来训练机器人。
在这里，机器人正在探索，但当它做对或朝着正确方向时，我会给予点击。
随着时间的推移，它将能够推动备用芯片，这是训练的目标。
因此，您可以执行整个决策变换的操作，但纯粹在上下文中，只需提供一堆模式并要求其弄清楚这个序列的规律性。
通过这种方式，它可以生成新的动作以改进先前的序列。
好了，对于语言模型，我们可以找到更适合教授低级技能的新接口。
奖励是语言模型和低级控制的桥梁，我们可以充分利用它作为通用接口，并实时优化。
有时，它表现优于直接生成动作，因此真的激励我们使用奖励函数作为接口。
在语言模型作为一般模式机器的情况下，我们可以将语言模型用于推理低级事务。
我们可以要求其推理低级事务。
同时机器人学作为一个领域，充满了序列转换、序列完成和序列改进任务。
因此，我们真正可以研究语言模型的较低级别机制。
而这次演讲的关键要点是，我们看到了基础模型的应用越来越多，不仅仅是在机器人技术的语义推理方面，更多的是在灵巧性、生成动作和机器人的较低级别具体智能方面。
我们需要重新思考机器人和transformer的扩展规律。
如何在有限的数据量下进行扩展呢？
我们在RT2中提出了一种新的扩展机器人模型和数据的方法，表明您可以用同样的数据做更多的事情。
基本上使用RT1数据加上互联网数据，您就可以概括地允许更多的事情。
在RTX中显示，您可以用更多的数据做更多的事情。
收集更多的机器人数据也有好处，到处都有积极的传输。
第二部分，关于语言模型的新接口，我认为机器人领域值得考虑开发新的、更低级别的语言模型接口，这有助于学习低级技能。
就这样，我想结束我的演讲。
如果你觉得有趣的话，有很多参考资料供你查阅。
特别感谢我的团队，Google DeepMind机器人团队。
因此，我们正处在为机器人开发基础模型的前沿，并请期待未来更多的内容。
谢谢。
是的？
你提到浮点数对于我们大多数时间与模型都很困难。
但如果你只是生成动作标记本身，就像在一个示例中没有岩石或其他任何东西一样，为什么不只是在transformer后附加一个线性层，它可以直接从这里生成你可以键入的数字呢？
是的，问题是，如果我们这些大型语言模型难以理解数字，为什么我们不使用一个线性层直接输出动作呢？
我认为语言模型确实很难理解数字，但有时我们仍然希望它能从预训练混合中带来一些知识。
迁移。
比如，如果我有一个新层，那么这个新层在预训练中是不存在的。
那我怎么能指望它进行迁移呢？
我认为这是一个有趣的问题。
但与此同时，我并不认为使用原始数字是正确的接口。
我们可能可以进行一些动作表示学习来学习一种表示。
语言模型可以输出那种表示。
所以我们仍在努力弄清楚什么是正确的表示。
所以在我们以前没有尝试过的表示中，比如十进制数、浮点数、额外的标记之间，我们发现只使用数字或额外的标记就足够了。
是的。
你是如何使用场景和一切都是关于使用你的工具的？
这是相当不同的。
就像我说的，两者实际上都在探索有前景的市场的不同方向。
是的，我认为两个方向都值得探索。
生成的不同优点，比如直接生成动作。
我认为它借用了语言建模的自回归性质，并且与许多其他任务非常契合，比如视觉问答。
局限性在于当你生成动作时，它受到严格的规范化。
你能生成那些超出分布范围的灵巧动作吗？这有点困难吗？
奖励语言实际上带来了传统机器人学的一页，这是基于优化或模型预测控制的。
所以，你也可以更容易地考虑安全约束。
它可以生成更多样化的动作。
也许一个方法是使用奖励语言系统生成大量数据，然后将它们提炼成一个Transformer，因为这样你就能赋予你的大型语言模型所有这些其他可取之处…
奖励语言本身，我不知道它有多可扩展。
就像是，我们没有对语言模型进行微调。
所以也许你被限制在什么，你完全取决于语言模型的训练数据。
语言模型可以做一次“月球步”是因为它知道什么是“月球步”。
它大致知道如何做那个。
但如果你想要扩展到完全新的事物，也许你可以使用语言去开拓你的数据生成，然后将其放入其他策略中。
那么Google接下来追求的是什么方向？
所以这就像是，语言是你观察正确方向的方法，或者说扩展你如何做，如何处理等等？
是的，我认为这是一个很好的问题。
所以关于扩展的部分,那只是一个玩笑。
但我是相当认真的。
这实际上是一个有前途的方案。
所以，大家都相信扩展规则的力量。
所以只要给它更多的数据，给它更多的计算，你会看到有趣的能力出现。
你对GPT2有什么看法？
GPT2是一个很大的进步。
你认为为了使机器人有这样的飞跃，你认为差距会在哪里？
是的，我仍然认为我们还不够，就像，我们还没有足够的数据。
我认为这可能仍然是最大的瓶颈。
所以我们正在试图找到更多的方法来利用有限的数据。
我们正在尝试收集更多的数据。
我认为我们需要一些时间来积累足够的数据。
目前，我说我们对积极转移有一些生机。
但是在语言模型中，人们不再谈论积极转移，因为这已经是司空见惯了，对吧？
你随处可见。
而机器人还没有达到那个阶段。
是的。
你的团队对安全和对齐性考虑了多少？
是的。
你现在是不是，就依赖于从大型语言模型中产生的伦理观？
它不会，你知道的，告诉你去杀人来实现这个目标？
是的，这是一个很好的问题。
实际上，我们非常非常重视安全，因为开发语言模型的其他领域，并没有直接影响物理世界。
但在这里，它可能对人类和环境造成潜在的危害。
而且Gary Marcus之前对我们的工作发表了评论，假如你说“拿出碗来，喂猫，然后放进洗碗机”。
结果，它把猫放进了洗碗机。
如果误解了，实际上，它将产生灾难性的失败案例。
我们通过设计硬件和软件安全层来谨慎处理安全问题。
还有一些宪法安全措施即将推出。
我现在不能透露太多细节，但很快，我们将发布一些工作。
是不是有点像，如果有人在，就别互动？
不，不，不。
我认为这要复杂一些，也更详细一些。
但我们确实非常重视安全。
在我们的一些实验中，实际上，机器人的手指可能会断裂，因为它无法对环境施加足够的力。
所以这只是确保安全的又一种方式。
我们能否有一些可视化的简化或类似的东西，以发现电压发生器如何发挥作用。
也许这有点像企业，但在一些大型模型中可能是这样。
是的，是的。
是的，我认为这是可能的。
并安装伍德兰，这将带您到演示的结束。
感谢您的精彩演讲。
酷，酷。
