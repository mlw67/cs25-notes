Hi everyone and thanks for coming to our CS35 lecture today.
So today we're honored to have Jim Ban from NVIDIA where we're talking about generalist agents in open -ended worlds.
He's a senior AI research scientist at NVIDIA, where his mission is to build generally capable AI agents with applications for gaming, robotics, and software automation.
His research spans Foundation models, multi -modal AI, reinforcement learning, and open -ended learning.
He obtained his PhD degree in computer science from here at Stanford, advised by Professor Pei Pei Liu.
And previously he did research internships at OpenAI, Google AI, as well as Mila Quebec AI Institute.
So yeah, we're giving it up for Jim.
Yeah, thanks for having me.
So I want to start with a story of two kittens.
It's a story that gave me a lot of inspiration over the career, over my career.
So I want to share this one first.
You know, back in 1963, there were two scientists from MIT, Held and Hein.
They did this ingenious experiment where they put two newborn kittens in this device and the kittens have not seen the visual world yet.
So it's kind of like a merry -go -round where the two kittens are linked by a rigid mechanical bar.
So their movements are exactly mirrored.
And there's an active kitten on the right -hand side, and that's the only one able to move freely and then transmit the motion over this link to the passive kitten, which is confined to the basket and cannot really control its own movements.
And then after a couple of days, Held and Hein kind of take the kittens out of this merry -go -round and then did visual testing on them.
And they found that only the active kitten was able to develop a healthy visual model loop, like responding correctly to approaching objects or like visual cliffs, but a passive kitten did not have a healthy visual system.
So I find this experiment fascinating because it shows the importance of having this embodied active experience to really ground a system of intelligence.
And let's put this experiment in today's AI context.
We actually have a very powerful passive kitten, and that is ChaiGBT.
It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.
And because of this, its knowledge is kind of abstract and ungrounded.
And that partially contributes to the fact that ChaiGBT hallucinates things that are just incompatible with our common sense and our physical experience.
And I believe the future belongs to active kittens, which translates to generalist agents.
They are the decision makers in a constant feedback loop, and they're embodied in this fully immersive world.
They're also not mutually exclusive with the passive kitten.
And in fact, I see the active embodiment part as a layer on top of the passive pre -training from lots and lots of other people.
And yet, have we achieved generalist agents?
Back in 2016, I remember, it was like spring of 2016.
I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention to the lecture.
I was watching a board game tournament on my laptop.
And this screenshot was the moment when Arthur Go versus Lisa Dao, and Arthur Go won three matches out of five and became the first ever to beat a human champion at a game of Go.
I remember the adrenaline that day.
I've seen history unfold.
Oh my God, we're finally getting to AGI.
And everyone's so excited.
And I think that was the moment when AI agents entered the mainstream.
But when the excitement fades, I felt that even though Arthur Go was so mighty and so great, it could only do one thing and one thing alone.
And afterwards, in 2019, there were more impressive achievements like OpenAI 5 beating the human champions at a game of Dota and Arthur Stark from DeepMind beat StarCraft.
But all of these with Arthur Go, they all have a single kind of theme, and that is to beat the opponent.
There is this one objective that the agent needs to do.
And the models trained on Dota or Go cannot generalize to any other tasks.
It cannot even play other games like Super Mario or Minecraft.
And the world is fixed and have very little room for open -ended creativity and exploration.
So I argue that a journalist agent should have the following essential properties.
First, it should be able to pursue very complex, cementally rich and open -world objectives.
Basically, you explain what you want in natural language, and the agent should perform the actions for you in a dynamic world.
And second, the agent should have a large amount of pre -trained knowledge instead of knowing only a few concepts that's extremely specific to the task.
And third, massively multitask.
A journalist agent, as the name implies, needs to do more than just a couple of things.
It should be, in the best case, infinitely multitask, as expressive as human language can dictate.
So what does it take?
Correspondingly, we need three main ingredients.
First is the environment.
The environment needs to be open -ended enough because the agent's capability is upper bounded by the environment complexity.
And I'd argue that Earth is actually a perfect example because it's so open -ended, this world we live in, that it allows an algorithm called natural evolution to produce all the diverse forms and behaviors of life on this planet.
So can we have a simulator that is essentially a lo -fi Earth, but we can still run it on the lab clusters?
And second, we need to provide the agent with massive pre -training data because exploration in an open -ended world from scratch is just intractable.
And the data will serve at least two purposes.
One as a reference manual on how to do things, and second, as a guidance on what are the interesting things worth pursuing.
And GPT is only at least up to GPT -4.
It only learns from pure text on the web, but can we provide the agent with much richer data, such as video walkthrough or multimedia, wiki documents, and other media forms?
And finally, once we have the environment and the database, we are ready to train foundation models for the agents.
And it should be flexible enough to pursue the open -ended tasks without any task -specific assumptions, and also scalable enough to compress all of the multi -modal data that I just described.
And here, language, I argue, will play at least two key roles.
One is as a simple and intuitive interface to communicate a task, to communicate the human intentions to the agent, and second, as a bridge to ground all of the multi -modal concepts and signals.
And that train of thought landed us in Minecraft, the best -selling video game of all time.
And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world, and in the game, you can basically do whatever your heart desires.
And what's so special about the game is that, unlike AlphaGo, StarCraft, or Dota, Minecraft defines no particular objective to maximize, no particular opponent to beat, and doesn't even have a fixed storyline.
And that makes it very well -suited as a truly open -ended AI playground.
And here, we see people doing extremely impressive things in Minecraft.
Like this is a YouTube video where a gamer built the entire Hogwarts castle block by block, by hand, in the game.
And here's another example of someone just digging a big hole in the ground and then making this beautiful underground temple with a river nearby.
It's all crafted by hand.
And one more, this is someone building a functioning CPU circuit inside the game, because there is something called Redstone in Minecraft that you can build circuits out of it, like logical gates.
And actually, the game is Turing -complete.
You can simulate a computer inside a game.
Just think about how crazy that is.
And here, I want to highlight a number that is 140 million active players.
And just to put this number in perspective, this is more than twice the population of the UK.
And that is the amount of people playing Minecraft on a daily basis.
And it just so happens that gamers are generally happier than PhDs.
So they love to stream and share what they're doing.
And that produces a huge amount of data every day online.
And there's this treasure trove of learning materials that we can tap into for training generalizations.
Remember that data is the key for foundation models.
So we introduce MindDojo, a new open framework to help the community develop generally capable agents using Minecraft as a kind of primordial soup.
MindDojo features three major parts, an open -ended environment, an international knowledge base, and then a generous agent developed with a simulator and massive data.
So let's zoom in the first one.
Here's a sample gallery of the interesting things that you can do with MindDojo's API.
We feature a massive benchmarking suite of more than 3 ,000 tasks.
And this is by far the largest open -source agent benchmark to our knowledge.
And we implement a very versatile API that unlocks the full potential of the game.
Like for example, MindDojo supports multimodal observation and a full action space like moving or attack or inventory management.
And then it can be customized at every detail.
Like you can tweak the terrains, the weather, block placement, monster spawning, and just anything you want to customize in the game.
And given the simulator, we introduce around 1 ,500 programmatic tasks, which are tasks that have grown to success conditions defined in Python code.
And you can also explicitly write down like sparse or best reward functions using this API.
And some examples are like harvesting different resources, unlocking the tech tree, or fighting various monsters and getting rewarded.
And all these tasks come with language prompts that are templated.
Next, we also introduce 1 ,500 creative tasks that are freeform and open -ended.
And that is in contrast to the programmatic task I just mentioned.
So for example, let's say we want the agent to build a house, but what makes a house a house?
It is ill -defined.
And just like image generation, you don't know if it generates a cat correctly or not.
So it's very difficult to use simple Python programs to give these kinds of tasks reward functions.
And the best way is to use foundation models trained on internet skill knowledge so that the model itself understands abstract concepts like the concept of a house.
And finally, there's one task that holds a very special status called play -suit, which is to beat the final boss of Minecraft, the Ender Dragon.
So Minecraft doesn't force you to do this task.
As we said, it doesn't have a fixed storyline, but it's still considered a really big milestone for any kind of beginner human players.
I want to highlight it is an extremely difficult task that requires very complex preparation, exploration, and also martial skills.
And for an average human, it will take many hours or even days to solve, easily over like 1 million action steps in a single episode.
And that will be the longest benchmarking task for policy learning ever created here.
So I admit I am personally a below average human.
I was never able to beat Ender Dragon.
And my friends laugh at me and I'm like, okay, one day my AI will avenge my poor skills.
That was one of the motivations for this project.
Now let's move on to the second ingredient, the internet skill knowledge base part of Minecraft Gojo.
We offer three datasets here, the YouTube, Wiki, and Reddit.
And combined, they are the largest open -ended agent behavior database ever compiled to our knowledge.
The first is YouTube.
And we already said Minecraft is one of the most streamed games on YouTube and the gamers love to narrate what they are doing.
So we collected more than 700 ,000 videos with 2 billion words in the corresponding transcripts.
And these transcripts will help the agent learn about human strategies and creativities without us manually labeling things.
And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft -specific Wikipedia that basically explains everything you ever need to know in every version of the game.
It's crazy.
And we scraped 7 ,000 Wiki pages with interleaving multimodal data like images, tables, and diagrams.
And here are some screenshots.
This is a gallery of all of the on and attack patterns.
And also the thousands of crafting recipes are all present on the Wiki.
And we scraped all of them.
And more like complex diagrams and tables and embedded figures.
Now we have something like GPD4V.
It may be able to understand many of these diagrams.
And finally, the Minecraft subreddit is one of the most active forums across the entire Reddit.
And players showcase their creations and also ask questions for help.
So we scraped more than 300 ,000 posts from Minecraft Reddit.
And here are some examples of how people use the Reddit as a kind of stack overflow for Minecraft.
And we can see that some of the top -loaded answers are actually quite good.
Like someone is asking, oh, why doesn't my wheat farm grow?
And the answer says, you need to light up the room with more torches.
You don't have enough lighting.
Now, given the massive tasks, wheat and internet data, we have the essential components to build journalists' agents.
So in the first mind -dozer paper, we introduced a foundation model called Mind Clip.
And the idea is very simple.
I can explain in three slides.
Basically for our YouTube database, we have time -aligned videos and transcripts.
And these are actually the real tutorial videos from our data set.
You see on the third clip, as I raise my axe in front of this pig, there's only one thing that you know is going to happen.
Actually, someone said this, a big YouTuber of Minecraft.
And then given this data, we train Mind Clip in the same spirit as OpenAI Clip.
So for those who are unfamiliar, OpenAI Clip is a contrastive model that learns the association between image and its caption.
And here it's a very similar idea.
But this time it is a video text contrastive model.
And we associate the text with a video snippet that runs about 8 to 16 seconds each.
And intuitively, Mind Clip learns the association between the video and the transcript that describes the activity in the video.
And Mind Clip outputs a score between 0 and 1, where 1 means a perfect correlation between the text and the video, and 0 means the text is irrelevant to the activity.
So you see this is effectively a language prompted foundation reward model that knows the nuances of things like forests, animal behaviors, and architectures in Minecraft.
So how do we use Mind Clip in action?
Here's an example of our agent interacting with a simulator.
And here the task is to obtain wool.
And as the agent explores in the simulator, it generates a video snippet as a moving window, which can be encoded and fed into Mind Clip along with an encoding of the text prompt here.
And Mind Clip computes the association.
The higher the association is, the more the agent's behavior in this video aligns with the language, which is a task you want it to do.
And that becomes a reward function to any reinforcement learning algorithm.
So this looks very familiar, right?
Because it's essentially RL from human feedback or RLHF in Minecraft.
And RLHF was the cornerstone algorithm that made chat GPT possible.
And I believe it will play a critical role in Jonas agents as well.
I'll quickly gloss over some quantitative results.
I promise there won't be many tables of numbers here.
For these eight we show the percentage success rate over 200 test episodes.
And here in the green circle is two variants of our Mind Clip method.
And in the orange circles are the baselines.
So I'll highlight one baseline, which is that we construct a dense reward function manually for each task using the MindDojo API.
It's a Python API.
And you can consider this column as a kind of oracle, the upper bound of the performance, because we put a lot of human efforts into designing these reward functions just for the tasks.
And we can see that Mind Clip is able to match the quality of many of these, not all of them, but many of these manually engineered rewards.
It is important to highlight that Mind Clip is open vocabulary.
So we use a single model for all of these tasks instead of one model for each.
And we simply prompt the reward model with different tasks.
And that's the only variation.
One major feature of Foundation Model is strong generalization out of the box.
So can our agent generalize to dramatic changes in the visual appearance?
So we did this experiment where during training, we only train our agents on a default terrain at noon on a sunny day, but we tested zero shot in a diverse range of terrains, weathers, and day -night cycles.
And you can customize everything in MindDojo.
And in our paper, we have numbers showing that Mind Club significantly beats an off -the -shelf visual encoder when facing these kinds of distribution shifts out of the box.
And this is no surprise, right?
Because Mind Clip was trained on hundreds of thousands of clips from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.
And I think that is just a testament to the big advantage of using internet -scale data, because you get robustness out of the box.
And here are some demos of our learned agent on various tasks.
So you may notice that these tasks are relatively short, around like 100 to 500 time steps.
And that is because Mind Clip is not able to plan over very long time horizons.
It is an inherent limitation in the training pipeline, because we could only use 8 to 16 seconds of the video.
So it's constrained to short actions.
But our hope is to build an agent that can explore and make new discoveries autonomously, all by itself, and it keeps going.
And in 2022, this goal seems quite out of reach for us.
MindDojo was June 2022.
And this year, something happened, and that is GB4, a language model that's so good at coding and long -horizon planning, so we just cannot sit still.
We built Voyager, the first large language model -powered lifelong learning agent.
And when we set Voyager loose in Minecraft, we see that it just keeps going.
And by the way, these video snippets are from a single episode of Voyager.
It's not from different episodes, it's a single one.
And we see that Voyager is just able to keep exploring the terrains, mine all kinds of materials, fight monsters, craft hundreds of recipes, and unlock an ever -expanding tree of diverse skills.
So how do we do this?
If we want to use the full power of GB4, a central question is how to streamify things, converting this 3D world into a textual representation.
We need a magic box here.
And thankfully, again, the crazy Minecraft community already built one for us, and it's been around for many years.
It's called MindFlayer, a high -level JavaScript API that's actively maintained to work with any Minecraft version.
And the beauty of MindFlayer is it has access to the game states surrounding the agent, like the nearby blocks, animals, and enemies.
So we effectively have a ground truth perception module as textual input.
And at the same time, MindFlayer also supports action APIs that we can compose skills.
And now that we can convert everything to text, we are ready to construct an agent on top of GB4.
So on a high level, there are three components.
One is a coding module that writes JavaScript code to control the game plot, and it's the main module that generates the executable actions.
And second, we have a code base to store the correctly written code and look it up in the future if the agent needs to recall the skill.
And in this way, we don't duplicate efforts, and whenever facing similar situations in the future, the agent knows what to do.
And third, we have a curriculum that proposes what to do next, given the agent's current capabilities and also situation.
And when you wire these components up together, you get a loop that drives the agent indefinitely and achieves something like lifelong learning.
So let's zoom in the center module.
We prompt GD4 with documentations and examples on how to use a subset of the MindFlayer API.
And GD4 writes code to take actions given the current assigned task.
And because JavaScript runs a code interpreter, GD4 is able to define functions on the fly and run it interactively.
But the code that GD4 writes isn't always correct, right?
Just like human engineers, you can't get everything correct on the first try.
So we developed an iterative prompting mechanism to refine the program.
And there are three types of feedback here.
The environment feedback, like what are the new materials you got after taking an action or some enemies nearby.
And the execution error from the JavaScript interpreter, if you wrote some buggy code, like undefined variable, for example, if it hallucinates something.
And another GD4 that provides critique through self -reflection from the agent state and the world state.
And that also helps refine the program effectively.
So I want to show some quick example of how the critic provides feedback on the task completion progress.
So let's say in the first example, the task is to craft a spyglass and GD4 looks at the agent's inventory and decides that it has enough copper, but not enough amherst as a material.
And the second task is to kill three sheeps to collect food.
And each sheep drops one unit of wool, but there are only two units in inventory.
So GD4 reasons and says that, okay, you have one more sheep to go.
Now, moving on to the second part, once Voyager implements a skill correctly, we save it to our persistent storage.
And you can think of the skill library as a code repository written entirely by a language model through interaction with a 3D world.
And the agent can record new skills and also retrieve skills from the library facing similar situations in the future.
So it doesn't have to go through this whole program refinement that we just saw again, which is quite inefficient, but you do it once you save it to disk.
And in this way, Voyager kind of bootstraps its own capabilities recursively as it explores and experiments in the game.
And let's dive a little bit deeper into how the skill library is implemented.
So this is how we insert a new skill.
First, we use GBD 3 .5 to summarize the program into plain English.
And summarization is very easy and GBD 4 is expensive.
So we just go for a cheaper tier.
And then we embed this summary as the key and we save the program, which is a bunch of code, as the value.
And we find that doing this makes retrieval better because the summary is more semantic and the code is a bit more discrete and you insert it.
And now for the retrieval process.
When Voyager is faced with a new task, let's say craft iron pickaxe, we again use GBD 3 .5 to generate a hint of how to solve the task.
And that is something like a natural language paragraph.
And then we embed that and use that as the query into the vector database and we retrieve the skill from the library.
So you can think of it as a kind of in -context replay buffer in the reinforcement learning literature.
And now moving on to the third part, we have another GBD 4 that proposes a task to do given its own capabilities at the moment.
And here we give GBD 4 a very high level kind of unsupervised objective.
That is to obtain as many unique items as possible.
That is our high level directive.
And then GBD 4 takes this directive and implements a curriculum of progressively harder challenges and more novel challenges to solve.
So it's kind of like curiosity exploration where it is not novelty search in a prior literature, but implemented purely in context.
If you're listening to Zoom, the next example is fun.
Let's go through this example together.
Just to kind of show you how Voyager works, the whole complicated data flow that I just showed.
So the agent finds itself hungry.
It only has one out of 20 hunger bar.
So it knows, GBD 4 knows that it needs to find food ASAP.
And then it senses there are four entities nearby, a cat, a villager, a pig, and some wheat seeds.
And now GBD 4 starts a self -reflection.
Like, do I kill the cat and villager to get some meat?
That sounds horrible.
How about the wheat seeds?
I can use the seeds to grow a farm, but that's going to take a very long time until I can generate some food.
So sorry piggy, you are the one being chosen.
So GBD 4 looks at the inventory, which is the agent state.
There's a piece of iron in inventory.
Voyager recalls a skill from the library that is to craft an iron sword and then use that skill to start pursuing, to start learning a new skill.
And that is hunt pig.
And once the hunt pig routine is successful, GBD 4 saves it to the skill library.
That's roughly how it works.
Yeah.
And putting all of these together, we have this iterative prompting mechanism, the skill library, and an automatic curriculum.
And all of these combined is Voyager's no gradient architecture, where we don't train any new models or fine tune any parameters, and allows Voyager to self bootstrap on top of GBD 4, even though we are treating the underlying language model as a black box.
It looks like my example worked and they started to listen.
So yeah, these are the tasks that Voyager picked up along the way.
And we didn't pre -program any of these.
It's all Voyager's idea.
The agent is kind of forever curious and also forever pursuing new adventures just by itself.
So to quickly show some quantitative results, here we have a learning curve where the X axis is the number of prompting iterations and the Y axis is the number of unique items that Voyager discovered as it's exploring an environment.
And these two curves are baselines, React and Reflexion.
And this is AutoGPT, which is like a popular software repo.
Basically, you can think of it as combining React and a task planner that decomposes an objective into sub goals.
And this is Voyager.
We're able to obtain three times more novel items than the prior methods and also unlock the entire type tree significantly faster.
And if you take away the skill library, you see that Voyager really suffers.
The performance takes a hit because every time it needs to kind of repeat and relearn every from scratch and it starts to make a lot more mistakes and that really degrades the exploration.
Here, these two are the bird -eye views of the Minecraft map.
And these circles are what the prior methods are able to explore given the same prompting iteration budget.
And we see that they tend to get stuck in local areas and kind of fail to explore more.
But Voyager is able to navigate distances at least two times as much as the prior works.
So it's able to visit a lot more places because to satisfy this high -level directive of obtaining as many unique items as possible, you've got to travel.
If you stay at one place, you will quickly exhaust the interesting things to do.
And Voyager travels a lot, so that's how we came up with the name.
So finally, one limitation is that Voyager does not currently support visual perception because GvD4 that we used back then was text -only.
But there's nothing stopping Voyager from adopting like multimodal language models in the future.
So here we have a little proof of concept demo where we ask a human to basically function as the image captioner.
And the human will tell Voyager that as you're building these houses, what are the things that are missing?
Like you place a door incorrectly, like the roof is also not done correctly.
So the human is acting as a critic module of the Voyager stack.
And we see that with some of that help, Voyager is able to build a farmhouse and a nether portal, but it has a hard time understanding 3D spatial coordinates just by itself in a textual domain.
Now, after doing Voyager, we're considering like where else can we apply this idea of coding in an embodied environment, observe the feedback, and iteratively refine the program.
So we came to realize that physics simulations themselves are also just Python code.
So why not apply some of the principles for Voyager and do something in another domain?
What if you apply Voyager in the space of this physics simulator API?
And this is Eureka, which my team announced just like three days ago, fresh out of the oven.
It is for robot dexterity at superhuman level.
And it turns out that GPT -4 plus reinforcement learning can spin a pen much better than I do.
I gave up on this task a long time ago from childhood.
It's so hard for me.
So Eureka's idea is very simple and intuitive.
GPT -4 generates a bunch of possible reward function candidates implemented in Python.
And then you just do a full reinforcement learning training loop for each candidate in a GPU accelerator simulator.
And you get a performance metric, and you take the best candidates and feed back to GPT -4, and it samples the next proposals of the candidates and keeps improving the whole population of the reward functions.
That's the whole idea.
It's kind of like an in -context evolutionary search.
So here's the initial reward generation where Eureka takes as context the environment code of NVIDIA's ISOC SIM and a task description and samples the initial reward function implementation.
So we found that the simulator code itself is actually a very good reference manual because it tells Eureka what are the variables you can use, like the hand positions, like here, the fingertip position, the finger test, the rotation, angular velocity, et cetera.
So you know all of these variables from the simulator code, and you know how they interact with each other.
So that serves as a very good in -context instruction.
So Eureka doesn't need to reference any human return reward functions.
And then once you have the generated reward, you plug it into any reinforcement learning algorithm and just train it to completion.
So this step is typically very costly and very slow because reinforcement learning itself is slow.
And we were only able to scale up Eureka because of NVIDIA ISOC chain, which runs a thousand simulated environment copies on a single GPU.
So basically you can think of it as speeding up reality by a thousand X. And then after training, you will get the performance metrics back on each reward component.
And as we saw from Voyager, GPT -4 is very good at self -reflection.
So we there's a software trial reminding you to activate a license.
Yeah.
So Voyager reflects on it and then proposes mutations on the code.
So here the mutations we found can be very diverse, ranging from something as simple as just changing a hyperparameter in the reward function weighting to all the way to adding completely novel components to the reward function.
And in our experiments, Eureka turns out to be a superhuman reward engineer, actually outperforming some of the functions implemented by the expert human engineers on NVIDIA's ISOC -CM team.
So here are some more demos of how Eureka is able to write very complex rewards that lead to these extremely dexterous behaviors.
And we can actually train the robot hand to rotate pants, not just in one direction, but in different directions along different 3D axes.
I think one major contribution of Eureka different from Voyager is to bridge the gap between high level reasoning and low level model controls.
So Eureka introduces a new paradigm that I'm calling hybrid gradient architecture.
So recall Voyager is a no gradient architecture.
We don't touch anything and we don't train anything, but Eureka is a hybrid gradient where a black box inference only language model instructs a white box learnable neural network.
So you can think of it as two loops, right?
The outer loop is gradient free.
It's driven by GD4, kind of selecting the reward functions.
And the inner loop is gradient based.
You train like a full reinforcement learning episode from it to achieve extreme dexterity by training a special neural network controller.
And you must have both loops to succeed to deliver this kind of dexterity.
And I think it will be a very useful paradigm for training robot agents in the future.
So these days when I go on Twitter or X, I see AI conquering new lands like every week, chat, image generation and music, they're all very well within reach.
But Mindojo, Voyager and Eureka, these are just scratching the surface of open -ended journalist agents.
And looking forward, I want to share two key research directions that I personally find extremely promising and I'm also working on it myself.
The first is a continuation of MindClip, basically how to develop methods that learn from internet scale videos.
And the second is multimodal foundation models.
Now that GD4v is coming, but it is just the beginning of an era.
And I think it's important to have all of the modalities in a single foundation model.
So first, about videos.
We all know that videos are abundant, right?
Like so many data on YouTube, way too many for our limited GPUs to process.
They're extremely useful to train models that not only have dynamic perception and intuitive physics, but also capture the complexity of human creativity and human behaviors.
It's all good, except that when you are using video to pre -training body nations, there is huge distribution shift.
You also don't get action labels and you don't get any of the groundings because you are a passive observer.
So I think here is a demonstration of why learning from video is hard, even for natural intelligence.
So little cat is seeing boxers shaking their head and it thinks maybe shaking head is the best way to do fighting.
Right.
This is why learning from video is hard.
You have no idea why...
This is too good.
Let's play this again.
You have no idea why Tyson is doing this, right?
The cat has no idea.
And then it associates this with just the wrong kind of policy.
But for sure, it doesn't help the fighting, but it definitely boosts the cat's confidence.
Why learning from video is hard.
Now I want to point out a few kind of latest research in how to leverage so much video for generalizations.
There are a couple of approaches.
The first is the simplest, just learn kind of a visual feature extractor from the videos.
So this is R3M from Chelsea's group at Stanford.
And this model is still an image level representation, just that it uses a video level loss function to train more specifically time contrasted learning.
And after that, you can use this as an image backbone for any agent, but you still need to kind of fine tune using domain specific data for the agent.
The second path is to learn reward functions from video.
And MindClip is one model under this category.
It uses a contrastive objective between the transcriber and video.
And here, this work, VIP, is another way to learn a similarity -based reward for goal condition tasks in the image space.
So this work, VIP, is led by who's also the first author of Eureka.
And Eureka is his internship project with me.
And the third idea is very interesting.
Can we directly do imitation learning from video, but better than the cat that we just saw?
So we just said the videos don't have the actions.
Right.
We need to find some ways to pseudo -label the actions.
And this is video training of VPT from OpenAI last year to solve long range tasks in Minecraft.
And here, the pipeline works like this.
Basically, you use a keyboard and a mouse action space, so you can align this action space with the human actions.
And OpenAI hires a bunch of Minecraft players and actually collect data in -house.
So they record the episodes done by those gamers.
And now you have a data set of video and action pairs.
And you train something called an inverse dynamics model, which is to take the observation and then predict the actions that caused the observation to change.
So that's the inverse dynamics model.
And that becomes a labeler that you can apply to in the 70k hours of in the wild YouTube videos.
And you will get these pseudo -actions that are not always correct, but also way better than random.
And then you're trying imitation learning on top of this augmented data set.
And in this way, OpenAI is able to greatly expand the data because the original data collected from the humans are high quality, but they're extremely expensive.
While in the wild, YouTube videos are very cheap, but you don't have the actions.
So they kind of solved and got the best of both worlds.
But still, it's really expensive to hire these humans.
Now, what's beyond the videos, right?
I'm a firm believer that multi -model models will be the future.
And I see text as a very lousy kind of 1D projection of our physical world.
So it's essential to include the other sensory modalities to provide a full embodied experience.
And in the context of embodied agents, I think the input will be a mixture of text, images, videos, and even audio in the future.
And the output will be actions.
So here's a very early example of a multi -model language model for robot learning.
So let's imagine a household robot.
We can ask the robot to bring us a cup of tea from the kitchen.
But if we want to be more specific, I want this particular cup that is my favorite cup.
So show me this image.
And we also provide video demo of how we want to mop the floor and ask the robot to imitate the similar motion in context.
And when the robot sees an unfamiliar object like a sweeper, we can explain it by providing an image and showing this is a sweeper.
Now go ahead and do something with the tool.
And finally, to ensure safety, we can say, take a picture of that room and just do not enter that room.
To achieve this, back last year, we proposed a model called VIMA, which stands for Visual Motor Attention.
And in this work, we introduce a concept called multi -model prompting, where the prompt can be a mixture of text, image, and videos.
And this provides a very expressive API that just unifies a bunch of different robot tasks that otherwise would require a very different pipeline or specialized models to solve in prior literature.
And VIMA simply tokenizes everything, converting image and text into sequences of tokens, and trying to transform on top to output the robot arm actions autoregressively, one step at a time during inference time.
So just to look at some of the examples here, like this prompt rearrange objects to match the scene.
It is a classical task called visual goal reaching that has a big body of prior works on it.
And that's how our robot does it, given this prompt.
And we also give it novel concepts in context, like this is a blicket, this is a wook, now put a wook into a blicket.
And both words are nonsensical.
So it's not in the training data, but VIMA is able to generalize zero shot and follow the motion to manipulate this object.
So the bot understands what we want and then follow this trajectory.
And finally, we can give it more complex prompt, like these are the safety constraints, sweep the box into this, but without exceeding that line.
And we would do this using the interleaving image and text tokens.
And recently Google Brain Robotics follow up after VIMA with RT1 and RT2, robot transformer one and two.
And RT2 is using a similar recipe as I described, where they first kind of pre -train on internet -scale data and then fine -tune with some human -collected demonstrations on the Google robots.
And RobotCat from DeepMind is another interesting work.
They train a single unified policy that works not just on a single robot, but actually across different embodiments, different robot forms, and even generalize to a new hardware.
So I think this is like a higher form of multimodal agent, where the physical form factor, the morphology of the agent itself is another modality.
So that concludes our looking forward section.
And lastly, I want to kind of put all the links together of the works I describe.
So this is mindojo .org.
We have open -source everything.
Well, for all the projects where big fans are open source, we open source as much as we can, including the model code, checkpoints, simulator code, and training data.
And this is voyager .mindojo .org.
This is Eureka.
And this is VIMA.
And one more thing, if you just want an excuse to play Minecraft at work, then Mindojo is perfect for you because you are collecting human demonstration to train generalization.
And if there's one thing that you take away from this talk, it should be this slide.
And lastly, I just want to remind all of us, despite all the progress I've shown, what we can do is still very far from human ingenuity as embodied agents.
These are the videos from our data set of people doing like decorating a winter wonderland or building the functioning CPU circuit within Minecraft.
And we're very far from that as AI research.
So here's a call to the community.
If human can do these mind -blowing tasks, then why not our AI?
Let's find out together.
