大家好，感谢大家今天参加我们的CS35讲座。
今天我们很荣幸地邀请到了来自NVIDIA的吉姆·班，我们将讨论开放式世界中的通用代理。
他是NVIDIA的高级人工智能研究科学家，他的使命是构建具有游戏、机器人和软件自动化应用的通用能力人工智能代理。
他的研究涵盖了基础模型、多模态人工智能、强化学习和开放式学习。
他在这里斯坦福获得了计算机科学博士学位，受刘培培教授的指导。
此前，他在OpenAI、Google AI以及魁北克人工智能研究所进行过研究实习。
是的，我们热烈欢迎吉姆。
是的，谢谢你们邀请我。
我想先讲一个关于两只小猫的故事。
这个故事在我的职业生涯中给了我很多启发。
所以我想先分享这个故事。
你知道，1963年，有两位来自MIT的科学家，赫尔德和海因。
他们做了一个巧妙的实验，在这个装置中放了两只新生的小猫，这些小猫还没有见过视觉世界。
所以这有点像一个旋转木马，两只小猫被一个刚性机械杆连接起来。
所以它们的动作完全是镜像的。
右侧有一只活跃的小猫，这是唯一能自由移动并将运动传递到被动小猫的那只，它被限制在篮子里，不能真正控制自己的动作。
然后几天后，海尔德和海因把小猫从这个旋转木马上拿下来，然后对它们进行视觉测试。
他们发现只有主动的小猫能够发展出健康的视觉模型循环，例如正确地对接近的物体或视觉悬崖作出反应，但被动的小猫没有健康的视觉系统。
所以我觉得这个实验很有意思，因为它显示了拥有这种具体的主动经验对于真正奠定智能系统的重要性。
让我们把这个实验放在今天的人工智能背景下。
实际上，我们有一个非常强大的被动小猫，那就是ChaiGBT。
它被动地观察和排练着互联网上的文本，而且它没有任何具体的体现。
因此，它的知识有点抽象和不牢固。
这在一定程度上有助于ChaiGBT产生与我们的常识和物理经验完全不相容的幻觉。
我相信未来属于积极的小猫，这意味着通用型代理。
它们是不断反馈循环中的决策者，并在这个完全沉浸式的世界中具体体现出来。
它们与被动小猫并不是相互排斥的。
事实上，我将积极体现部分视为在许多其他人的被动预训练之上的一层。
然而，我们是否已经实现了通用型代理呢？
回到2016年，我记得，那像是2016年春天。
我坐在哥伦比亚大学的一堂本科课上，但我并没有注意听讲。
我在看一场棋盘游戏比赛，我的笔记本电脑上。
这个截图是亚瑟·Go对阵丽莎·道的时刻，亚瑟·Go在五场比赛中赢得了三场，成为第一个击败人类冠军的AI。
我记得那一天的肾上腺素飙升。
我见证了历史的发展。
天哪，我们终于迎来了通用人工智能。
每个人都兴奋不已。
我认为那是AI代理进入主流的时刻。
但是当兴奋消退时，我觉得尽管Arthur Go如此强大，如此伟大，它只能做一件事，而且只能做一件事。
之后，在2019年，还有更令人印象深刻的成就，比如OpenAI 5在Dota比赛中击败了人类冠军，DeepMind的Arthur Stark击败了星际争霸。
但所有这些与Arthur Go有一个共同的主题，那就是打败对手。
代理人需要做到这一点。
在Dota或Go上训练的模型无法推广到任何其他任务。
它甚至不能玩其他游戏，比如超级马里奥或我的世界。
而且这个世界是固定的，几乎没有开放式的创造力和探索空间。
因此，我认为新闻记者代理应该具备以下基本属性。
首先，它应该能够追求非常复杂、概念丰富且开放的世界目标。
基本上，你用自然语言解释你想要什么，代理就应该在动态世界中为你执行动作。
其次，代理应该具有大量的预训练知识，而不仅仅是了解一些极其特定于任务的概念。
第三，大规模多任务。
记者代理，顾名思义，需要做的不仅仅是一两件事情。
在最理想的情况下，它应该是无限多任务的，尽可能表达出人类语言所能规定的多样性。
那么，需要什么呢？
相应地，我们需要三个主要因素。
首先是环境。
环境需要足够开放，因为代理的能力上限受到环境复杂性的限制。
我认为地球实际上是一个完美的例子，因为它是如此开放，我们生活的这个世界，它允许一种叫做自然进化的算法在这个星球上产生所有不同形式和行为的生命。
那么我们是否可以拥有一个模拟器，它本质上是一个低保真的地球，但我们仍然可以在实验室集群上运行它？
其次，我们需要为代理提供大量的预训练数据，因为从头开始在一个开放性环境中进行探索是不可行的。
而且数据将至少发挥两个目的。
一个作为如何做事的参考手册，第二个是关于哪些值得追求的有趣事物的指导。
而且GPT至少只到GPT-4。
它只从网络上的纯文本中学习，但我们能否为代理提供更丰富的数据，比如视频演示或多媒体、维基文档和其他媒体形式？
最后，一旦我们有了环境和数据库，我们就准备好为代理训练基础模型了。
它应该足够灵活，以追求开放式任务而不做出任何特定任务的假设，也足够可伸缩，以压缩我刚描述的所有多模态数据。
在这里，我认为语言至少会发挥两个关键作用。
一个是作为一个简单直观的界面，将任务、人类意图传达给代理，第二是作为一个桥梁，将所有多模态概念和信号联系起来。
这样的思路使我们陷入了《我的世界》，这是有史以来最畅销的视频游戏。
对于那些不熟悉的人，Minecraft是一个程序生成的3D像素世界，在游戏中，你基本上可以做任何你想做的事情。
关于这个游戏的特别之处在于，与AlphaGo、星际争霸或Dota不同，Minecraft没有定义要最大化的特定目标，也没有特定的对手要击败，甚至没有固定的故事情节。
这使得它非常适合作为一个真正开放的人工智能游乐场。
在这里，我们看到人们在Minecraft中做着非常令人印象深刻的事情。
就像这是一个YouTube视频，一个玩家在游戏中一块一块地用手建造了整个霍格沃茨城堡。
这是另一个例子，有人只是在地下挖了一个大洞，然后建造了一个漂亮的地下神庙，并在附近有一条河流。
这一切都是用手工制作的。
还有一个例子，有人在游戏中构建了一个功能完整的CPU电路，因为在Minecraft中有一种叫做红石的东西，你可以用它来构建电路，比如逻辑门。
实际上，这个游戏是图灵完备的。
你可以在游戏中模拟一台计算机。
想想这有多疯狂。
在这里，我想强调一个数字，那就是1.4亿活跃玩家。
为了让这个数字更具有说服力，这比英国的人口还要多两倍。
这就是每天玩Minecraft的人数。
碰巧的是，玩家通常比博士更快乐。
所以他们喜欢直播和分享他们正在做的事情。
这每天在线产生了大量的数据。
这是一个我们可以利用来训练泛化模型的学习材料宝库。
记住，数据是基础模型的关键。
因此，我们推出了 MindDojo，一个新的开放框架，帮助社区开发通用能力代理，使用 Minecraft 作为一种原始汤。
MindDojo包括三个主要部分，一个开放的环境，一个国际知识库，以及一个由模拟器和大量数据开发的强大代理。
让我们来详细看看第一个。
这里是一个使用 MindDojo API 可以做的有趣事情的样本画廊。
我们提供了一个包含超过 3,000 项任务的大型基准测试套件。
据我们所知，这是迄今为止最大的开源代理基准测试。
我们实现了一个非常灵活的 API，可以释放游戏的全部潜力。
例如，MindDojo 支持多模态观察和完整的动作空间，如移动、攻击或库存管理。
然后，它可以在每一个细节上定制。
就像你可以调整地形、天气、方块放置、怪物生成，以及游戏中的任何你想要自定义的东西。
鉴于这个模拟器，我们引入了大约 1,500 个程序化任务，这些任务的成功条件是由 Python 代码定义的。
你也可以明确地使用这个 API 编写稀疏或最佳奖励函数。
一些例子包括收获不同的资源、解锁科技树，或是与各种怪物战斗并获得奖励。
所有这些任务都有模板化的语言提示。
接下来，我们还介绍了 1,500 个创意任务，这些任务是自由形式的，开放式的。
这与我刚提到的程序化任务形成对比。
比如说，我们想让代理人建造一座房子，但是什么样的建筑才算是一座房子呢？
这是一个模糊的定义。
就像图像生成一样，你不知道它是否正确地生成了一只猫。
因此，使用简单的 Python 程序来给这些任务制定奖励函数是非常困难的。
最好的方法是使用在互联网技能知识上训练的基础模型，这样模型本身就能理解抽象概念，比如房子的概念。
最后，还有一个非常特殊的任务，被称为 play -suit，那就是打败 Minecraft 的最终 boss，即末影龙。
所以 Minecraft 并不强制你完成这个任务。
正如我们所说，它没有固定的故事情节，但对于任何初学者来说，这仍然被认为是一个非常重要的里程碑。
我想强调的是，这是一个非常困难的任务，需要非常复杂的准备、探索，以及战斗技能。
对于普通人来说，解决这个任务将需要很多小时甚至几天的时间，在单次游戏中可能会有超过 100 万次的行动步骤。
这将是迄今为止在政策学习中创建的最长的基准任务。
所以我承认我个人是一个平均水平以下的人。
我从未能够击败末影龙。
我的朋友嘲笑我，但我说，总有一天我的人工智能会为我可怜的技能报仇的。
这是这个项目的动力之一。
现在让我们继续介绍第二个部分，即 Minecraft Gojo 的互联网技能知识库部分。
我们在这里提供了三个数据集，YouTube、Wiki 和 Reddit。
综合起来，它们是我们所知道的最大的开放式代理行为数据库。
第一是YouTube。
而且我们已经说过Minecraft是YouTube上最受欢迎的游戏之一，玩家们喜欢叙述他们正在做的事情。
所以我们收集了超过700,000个视频，包含20亿字的相应文字记录。
这些文字记录将帮助智能体了解人类的策略和创造力，而无需我们手动标记事物。
其次，Minecraft玩家群体非常热衷，他们编制了一个庞大的Minecraft专用维基百科，基本上解释了你在游戏的每个版本中需要知道的一切。
真的很疯狂。
我们爬取了7,000个维基页面，其中包含交织的多模态数据，如图片、表格和图表。
这里有一些截图。
这是所有关于进攻模式和攻击模式的图片库。
而且维基上还有成千上万的制作配方。
我们都进行了爬取。
还有一些复杂的图表、表格和嵌入式图形。
现在我们有了类似GPD4V的东西。
它或许能够理解许多这些图表。
最后，Minecraft的Subreddit是整个Reddit中最活跃的论坛之一。
玩家展示他们的创作，并提出寻求帮助的问题。
所以我们从 Minecraft Reddit 上抓取了超过 300,000 条帖子。
这里有一些人们如何将 Reddit 用作 Minecraft 的堆栈溢出的示例。
我们可以看到一些顶级回答实际上非常好。
比如有人问，为什么我的小麦农场不长？
答案说，你需要用更多的火把照亮房间。
你的光线不足。
现在，考虑到庞大的任务、小麦和互联网数据，我们有了构建新闻记者代理所必需的组件。
所以在第一份《Mind Dozer》论文中，我们介绍了一个基础模型，称为 Mind Clip。
这个想法非常简单。
我可以用三张幻灯片解释。
基本上，对于我们的 YouTube 数据库，我们有时间对齐的视频和文字记录。
这些实际上是我们数据集中的真实教程视频。
你在第三个片段看到，当我在这只猪面前举起我的斧头时，你知道只会发生一件事。
实际上，有人说过这个，一个 Minecraft 的大 YouTuber。
然后，根据这些数据，我们像 OpenAI Clip 一样训练 Mind Clip。
对于那些不熟悉的人来说，OpenAI Clip是一个对比模型，它学习图像与其标题之间的关联。
这里的想法非常类似。
但这次是一个视频文本对比模型。
我们将文本与每个运行约8到16秒的视频片段相关联。
直观地说，Mind Clip学习视频与描述视频活动的文本之间的关联。
Mind Clip输出一个介于0到1之间的分数，其中1表示文本与视频之间存在完美的相关性，而0表示文本与活动无关。
所以你看，这实际上是一个语言提示基础奖励模型，它了解像Minecraft中的森林、动物行为和建筑等细微差别。
那么我们如何在实践中使用Mind Clip呢？
以下是我们的代理与模拟器进行交互的示例。
在这里，任务是获得羊毛。
当代理在模拟器中探索时，它会生成一个视频片段作为移动窗口，可以对其进行编码并与此处的文本提示的编码一起输入到Mind Clip中。
Mind Clip计算相关性。
关联度越高，视频中代理的行为与语言越一致，这是您希望其执行的任务。
这成为了任何强化学习算法的奖励函数。
所以这看起来非常熟悉，对吧？
因为本质上就是来自于人类反馈的强化学习，或者说是在《我的世界》中的 RLHF。
而 RLHF 是使 ChatGPT 可能的基石算法。
我相信它也将在 Jonas 代理中发挥关键作用。
我会快速概述一些定量结果。
我保证这里不会有太多的数字表格。
对于这八个案例，我们展示了 200 次测试集合中的成功率百分比。
在绿色圈圈中是我们 Mind Clip 方法的两个变体。
而在橙色圈圈中是基线。
所以我会突出一个基线，我们为每个任务使用 MindDojo API 手动构建了一个密集的奖励函数。
这是一个 Python API。
您可以将这一列视为一种神谕，性能的上限，因为我们投入了大量人力资源来设计这些奖励函数，只是为了完成任务。
我们可以看到Mind Clip能够匹配许多这些奖励的质量，虽然不是全部，但是很多是手工设计的奖励。
强调一下，Mind Clip是开放词汇的。
因此，我们使用单一模型来处理所有这些任务，而不是为每个任务使用一个模型。
我们只是用不同的任务来提示奖励模型。
这是唯一的变化。
Foundation Model的一个主要特点是强大的开箱即用泛化能力。
那么我们的代理能否适应视觉外观的巨大变化呢？
因此，我们进行了这个实验，在训练过程中，我们只在默认地形、正午、晴天的情况下训练我们的代理，但是我们在各种地形、天气和昼夜循环下进行了零射击测试。
而且你可以在MindDojo中自定义一切。
在我们的论文中，我们的数据显示，面对这些分布偏移的情况时，Mind Club明显击败了现成的视觉编码器。
这并不奇怪，对吧？
因为Mind Clip是在YouTube上数十万个Minecraft视频片段上训练的，这些视频覆盖了所有的场景。
我认为这只是使用互联网规模的数据的一个重要优势的证明，因为您可以获得开箱即用的鲁棒性。
这里有一些我们学到的代理在各种任务上的演示。
所以您可能注意到这些任务相对较短，大约在100到500个时间步之间。
这是因为Mind Clip无法计划非常长的时间跨度。
这是训练流程中的固有限制，因为我们只能使用8到16秒的视频。
因此，它受限于短暂的动作。
但我们的希望是构建一个能够自主探索并做出新发现的代理，全靠它自己，它会一直持续下去。
而在2022年，对我们来说，这个目标似乎是无法实现的。
MindDojo是在2022年6月。
而今年发生了一些事情，那就是GB4，一个在编码和长期规划方面表现出色的语言模型，所以我们不能坐以待毙。
我们建造了Voyager，第一个由大型语言模型驱动的终身学习代理。
当我们释放Voyager在Minecraft中时，我们发现它就是在不停地前进。
顺便说一下，这些视频片段都来自Voyager的单一一集。
不是来自不同的集数，而是同一集。
我们看到，航海者能够继续探索各种地形，挖掘各种材料，与怪物战斗，制作数百种配方，并解锁一个不断扩大的多样化技能树。
那么我们如何做到这一点呢？
如果我们想要充分利用GB4的全部功能，一个核心问题就是如何将事物流式化，将这个3D世界转换为文本表示。
在这里，我们需要一个魔法盒子。
幸运的是，再次感谢疯狂的Minecraft社区为我们构建了一个，并且它已经存在了很多年。
它被称为MindFlayer，这是一个高级JavaScript API，专门用于与任何Minecraft版本配合使用，并且正在积极维护。
MindFlayer的美妙之处在于它可以访问周围代理的游戏状态，例如附近的方块、动物和敌人。
因此，我们实际上拥有了一个地面真实感知模块作为文本输入。
同时，MindFlayer还支持我们可以组合技能的动作API。
现在我们可以将一切转换为文本，准备在GB4之上构建一个代理了。
所以从高层次来说，有三个组件。
一个是编码模块，编写JavaScript代码来控制游戏情节，它是生成可执行动作的主要模块。
其次，我们有一个代码库，用于存储正确编写的代码，并在将来查找，如果代理需要回忆技能。
以这种方式，我们不会重复努力，每当在将来面临类似情况时，代理就知道该怎么做。
第三，我们有一个课程设置，根据代理的当前能力和情况提出下一步要做什么。
当你把这些组件连接在一起时，你就得到了一个驱动代理无限进行并实现类似终身学习的循环。
所以让我们聚焦于中心模块。
我们用文档和示例提示GD4如何使用MindFlayer API的子集。
然后GD4编写代码来执行当前分配的任务。
由于JavaScript运行代码解释器，GD4能够动态定义函数并进行交互式运行。
但是，GD4编写的代码并不总是正确的，对吧？
就像人类工程师一样，你不能一开始就把所有东西都做对。
因此，我们开发了一个迭代提示机制来优化程序。
这里有三种类型的反馈。 
环境反馈，比如你采取一个动作后得到了什么新材料或者附近的一些敌人。
以及来自JavaScript解释器的执行错误，如果你写了一些有bug的代码，比如未定义变量，例如，如果它产生了一些幻觉。
还有另一个GD4，通过代理状态和世界状态的自我反思提供批评。
这也有助于有效地完善程序。
所以我想展示一些快速的例子，说明批评者是如何对任务完成进度提供反馈的。
比如说在第一个例子中，任务是制造一个望远镜，GD4查看代理的库存并决定它有足够的铜，但作为材料的amherst不够。
第二个任务是杀死三只绵羊来收集食物。
每只绵羊掉落一单位的羊毛，但库存中只有两单位。
所以GD4推理并说，好的，你还有一只绵羊要去。
现在，移动到第二部分，一旦Voyager正确实现了一个技能，我们就将其保存到我们的持久存储中。
你可以把技能库想象成一个完全由语言模型通过与3D世界的互动编写的代码仓库。
而且智能体可以记录新技能，并在将来面对类似情况时从库中检索技能。
因此，它不必再次经历我们刚刚看到的整个程序细化过程，这是相当低效的，但你只需要做一次，然后将其保存到磁盘。
这样，航海者在探索和在游戏中进行实验时递归地提升自己的能力。
让我们深入了解一下技能库的实现方式。
这就是如何插入新技能的方法。
首先，我们使用 GBD 3 .5 将程序总结成简单的英语。
总结非常容易，而 GBD 4 则比较昂贵。
因此，我们选择更便宜的层次。
然后，我们将这个摘要嵌入为键，将程序保存为值（一堆代码）。
我们发现这样做可以提高检索效果，因为摘要更有语义，而代码更离散并被插入。
现在，对于检索过程。
当航海者面临新任务，比如制作铁镐时，我们再次使用 GBD 3 .5 生成解决任务的提示。
那就像是一个自然语言的段落。
然后我们嵌入它，并将其用作向量数据库中的查询，并从库中检索技能。
所以你可以把它想象成是强化学习文献中一种上下文回放缓冲区。
现在，让我们转向第三部分，我们有另一个提出了一个任务的 GBD 4，根据其当前的能力。
在这里，我们给 GBD 4 一个非常高层次的无监督目标。
那就是尽可能获得尽可能多的独特项目。
这是我们的高层指令。
然后 GBD 4 接受这个指令，并实施一个逐渐更难的挑战和更多新颖挑战的课程。
所以这有点像是好奇心探索，在这里不是先前文献中的新颖性搜索，而是纯粹地在上下文中实现。
如果你正在听 Zoom，下一个例子很有趣。
让我们一起来看看这个例子。
只是为了向你展示 Voyager 如何工作，我刚才展示的整个复杂数据流。
所以代理发现自己饥饿了。
它只有 20 分之一的饥饿值。
所以它知道，GBD 4 知道它需要尽快找到食物。
然后它感知到附近有四个实体，一只猫，一个村民，一头猪和一些小麦种子。
现在GBD 4开始自我反思。
比如说，我要杀死猫和村民来得到一些肉吗？
那听起来很可怕。
小麦种子呢？
我可以用种子种植农场，但那需要很长时间才能产生食物。
所以抱歉小猪，你被选中了。
于是GBD 4查看库存，也就是代理状态。
库存中有一块铁。
Voyager从库中召回一个技能，即制作铁剑，然后使用该技能开始追求，开始学习新技能。
那就是猎杀猪。
一旦猎杀猪例程成功，GBD 4将其保存到技能库中。
大致就是这样。
是的。
将所有这些结合在一起，我们就有了这个迭代的提示机制、技能库和自动课程设置。
所有这些结合起来就是 Voyager 的无梯度架构，我们不训练任何新模型，也不微调任何参数，这使得 Voyager 可以在 GBD 4 的基础上进行自引导，即使我们将底层语言模型视为一个黑盒子。
看来我的例子奏效了，他们开始倾听了。
是的，这些就是旅行者一路上完成的任务。
我们并没有对这些任务进行预先编程。
都是旅行者号的主意。
探测器永远充满好奇心，也永远在独自追求新的冒险。
因此，为了快速显示一些量化结果，我们在这里绘制了一条学习曲线，其中 X 轴是提示迭代的次数，Y 轴是旅行者在探索环境时发现的独特物品的数量。
这两条曲线分别是 React 和 Reflexion 的基线。
这是 AutoGPT，就像一个流行的软件 repo。
基本上，您可以将其视为 React 和任务规划器的结合体，后者可将目标分解为多个子目标。
这就是 Voyager。
我们能够获得比以前的方法多三倍的新项目，而且解锁整个类型树的速度也明显更快。<EOS
如果你拿走技能库，你会发现Voyager真的受到了影响。
性能受到了影响，因为每次它都需要重复学习，从零开始，并且开始犯更多的错误，这真的会降低探索的质量。
这里，这两个是Minecraft地图的鸟瞰图。
这些圆圈是先前的方法在相同的提示迭代预算下能够探索的内容。
我们发现它们往往会陷入局部区域，无法进一步探索。
但是Voyager能够导航距离，至少是以前的工作的两倍。
所以它能够访问更多的地方，因为为了满足尽可能获取尽可能多的独特物品这一高级指令，你必须旅行。
如果你呆在一个地方，你会很快耗尽有趣的事情可做。
而Voyager旅行得很多，这就是我们想出这个名字的原因。
最后，一个限制是Voyager目前不支持视觉感知，因为我们当时使用的GvD4只能处理文本。
但是没有什么能阻止Voyager在将来采用多模态语言模型。
所以在这里，我们有一个小的概念验证演示，我们要求一个人基本上充当图像描述者。
而人类将告诉Voyager，在建造这些房屋时，有哪些缺少的要素？
比如说你放置了一个门的位置不正确，比如说屋顶也没有正确完成。
所以人类充当Voyager堆栈的评论模块。
我们看到，在得到一些帮助的情况下，Voyager能够建造一座农舍和一个异次元传送门，但是在纯文本领域中，它很难独自理解3D空间坐标。
现在，在完成Voyager之后，我们正在考虑在哪些其他地方可以应用在具体环境中编码的这个想法，观察反馈，并迭代地完善程序。
所以我们意识到物理模拟本身也只是Python代码。
那么为什么不将Voyager的一些原则应用在另一个领域呢？
如果你在这个物理模拟器API的领域中应用Voyager呢？
这就是尤里卡，我的团队就在三天前宣布的，刚刚出炉。
它是用于超人类水平的机器人灵活性。
结果证明，GPT-4加上强化学习比我更擅长旋转笔。
我很久以前就放弃了这个任务，从小时候开始。
这对我来说太难了。
所以尤里卡的想法非常简单直观。
GPT-4生成了一堆可能的奖励函数候选项，用Python实现。
然后，你只需对每个候选项在GPU加速器模拟器中进行完整的强化学习训练循环。
你会得到一个性能指标，然后你会选择最佳的候选项，并将其反馈给GPT-4，它会对候选项的下一个提案进行抽样，不断改进奖励函数的整个种群。
这就是整个思路。
这有点像是一种上下文中的进化搜索。
所以这里是初始奖励生成，尤里卡会根据NVIDIA的ISOC SIM的环境代码和任务描述进行初始奖励函数实现的抽样。
我们发现模拟器代码本身实际上是一个非常好的参考手册，因为它告诉尤里卡可以使用哪些变量，比如手的位置，就像这里，指尖的位置，手指的测试，旋转，角速度等等。
所以你从模拟器代码中知道了所有这些变量，也知道它们是如何相互作用的。
因此，这充当了一个非常好的上下文指令。
因此，尤里卡不需要引用任何人类回报奖励函数。
然后，一旦你有了生成的奖励，你将其插入任何强化学习算法中，然后只需训练至完成。
所以这一步通常非常昂贵且非常缓慢，因为强化学习本身就很慢。
我们之所以能够扩大尤里卡的规模，是因为NVIDIA ISOC链，它在单个GPU上运行一千个模拟环境副本。
所以基本上你可以将其视为将现实加速一千倍。然后在训练后，你将获得每个奖励组件的性能指标。
正如我们从Voyager看到的，GPT-4非常擅长自我反思。
所以这里有一个软件试用提醒你激活许可证。
是的。
Voyager对此进行反思，然后对代码提出变异。
因此，我们发现的这些变异可以非常多样，从只是更改奖励函数权重中的超参数，一直到在奖励函数中添加全新的组件。
在我们的实验中，尤里卡证明是一个超级的奖励工程师，实际上胜过了NVIDIA的ISOC-CM团队的一些专家人工工程师实现的一些功能。
所以这里有一些更多的演示，展示尤里卡如何编写非常复杂的奖励，导致这些极其灵巧的行为。
我们实际上可以训练机器手以不同的方向沿不同的3D轴旋转裤子，而不仅仅是一个方向。
我认为尤里卡与航海者相比的一个主要贡献是弥合了高级推理与低级模型控制之间的差距。
因此，尤里卡引入了一个我称之为混合梯度架构的新范例。
所以回想一下，航海者是一个无梯度架构。
我们不触碰任何东西，也不训练任何东西，但尤里卡是一个混合梯度，其中一个黑盒推理语言模型指导一个可学习的白盒神经网络。
所以你可以把它想象成两个循环，对吧？
外循环是无梯度的。
它由GD4驱动，有点像选择奖励功能。
而内循环是基于梯度的。
你要像完整的强化学习过程一样训练，以通过训练特殊的神经网络控制器达到极高的灵巧度。
而且你必须拥有两个循环，才能成功实现这种灵巧度的交付。
而且我认为这将是未来训练机器人代理的一种非常有用的范例。
所以这些天当我上Twitter或X时，我看到AI每周都在征服新的领域，聊天、图像生成和音乐，它们都很容易实现。
但Mindojo、Voyager和Eureka，它们只是触及到开放性记者代理表面的一小部分。
展望未来，我想分享两个我个人认为非常有前途的研究方向，我自己也在这方面工作。
第一个是对MindClip的延续，基本上是如何开发能够从互联网规模的视频中学习的方法。
第二个是多模态基础模型。
现在GD4v即将到来，但这只是一个时代的开端。
我认为将所有的模态都融入一个单一的基础模型是很重要的。
首先，关于视频。
我们都知道视频是丰富的，对吧？
就像YouTube上有很多数据，远远超过我们有限的GPU可以处理的范围。
它们对于训练模型非常有用，这些模型不仅具有动态感知和直观物理特性，还能捕捉到人类创造力和人类行为的复杂性。
这一切都很好，只是当您使用视频对身体民族进行预训练时，会出现巨大的分布偏移。
因为你是被动的观察者，所以你也不会得到行动标签，也不会得到任何基础知识。
因此，我认为这里展示了为什么从视频中学习是困难的，即使对自然智能来说也是如此。
所以，小猫看到拳击手在摇头，它认为摇头是最好的格斗方式。
对。
这就是为什么从视频中学习很难。
你不知道为什么...
这太好了。
我们再来玩一次。
你不知道泰森为什么要这么做，对吗？
猫不知道。
然后，它就会把这件事和错误的政策联系起来。
但可以肯定的是，这无助于战斗，但绝对会增强猫的信心。
为什么从视频中学习很难？
现在，我想就如何利用如此多的视频进行归纳指出一些最新研究。
有几种方法。<EOS
第一个是最简单的，只需从视频中学习一种视觉特征提取器。
这是斯坦福大学切尔西小组的 R3M。
这个模型仍然是图像级表示，只是它使用了视频级损失函数来训练更具体的时间对比学习。
之后，您可以将其用作任何代理的图像骨干，但您仍需要使用代理的特定领域数据进行微调。
第二条路径是从视频中学习奖励函数。
而 MindClip 就是这一类别中的一个模型。
它使用了转录器和视频之间的对比目标。
在这里，VIP 这项工作是在图像空间中为目标条件任务学习基于相似性的奖励的另一种方法。
因此，VIP这项工作是由《尤里卡》的第一作者领导的。
Eureka是他和我的实习项目。
第三个想法非常有趣。
我们能否直接从视频中进行模仿学习，但比我们刚才看到的那只猫更好？
所以我们刚才说视频中没有动作。
对。
我们需要找到一些伪标记动作的方法。<EOS
这是去年OpenAI关于解决Minecraft中长程任务的VPT的视频培训。
而这里，流程是这样的。
基本上，你使用键盘和鼠标的动作空间，这样你可以将这个动作空间与人类的动作对齐。
然后OpenAI雇佣了一群Minecraft玩家，实际上在内部收集数据。
所以他们记录了这些玩家完成的任务。
现在你有了一个视频和动作配对的数据集。
然后你训练一个被称为逆动力学模型的东西，它的作用是接受观察结果，然后预测导致观察结果变化的动作。
所以这就是逆动力学模型。
然后这变成了一个标签生成器，你可以将其应用于在野外YouTube视频中的70,000小时。
然后你会得到这些伪动作，它们并不总是正确，但比随机好得多。
然后你试图在这个增强的数据集上进行模仿学习。
通过这种方式，OpenAI能够大大扩展数据，因为从人类那里收集到的原始数据是高质量的，但非常昂贵。
而在野外，YouTube视频很便宜，但你没有这些动作。
所以他们有点解决了并得到了两全其美的结果。
但是，雇佣这些人的成本真的很高。
现在，视频之外的是什么呢？
我坚信多模型模型将是未来。
我认为文本是我们物理世界的一种非常糟糕的一维投影。
因此，包含其他感官模式以提供完整的体验至关重要。
在具体的体验代理的背景下，我认为输入将是未来文本、图像、视频，甚至音频的混合。
而输出将是行动。
所以这是一个用于机器人学习的多模型语言模型的一个很早的例子。
那么，让我们想象一个家庭机器人。
我们可以要求机器人从厨房给我们拿一杯茶。
但如果我们想更具体，我想要这个特别的杯子，那就是我的最爱杯子。
那么给我看这个图像。
而且我们还提供了关于如何拖地的视频演示，并要求机器人在上下文中模仿类似的动作。
当机器人看到一个陌生的对象，比如一个扫把时，我们可以通过提供图像并展示这是一个扫把来解释它。
现在，继续使用这个工具做些什么吧。
最后，为了确保安全，我们可以说，拍摄那个房间的照片，然后就不要进入那个房间。
为了实现这一点，去年我们提出了一个名为VIMA的模型，它代表着视觉动作注意力。
在这项工作中，我们引入了一个名为多模型提示的概念，其中提示可以是文本、图像和视频的混合。
这提供了一个非常表达性的API，它只是统一了一堆不同的机器人任务，否则这些任务在以前的文献中需要非常不同的流程或专门的模型来解决。
VIMA只是简单地对一切进行标记，将图像和文本转换为令牌序列，并尝试在其上进行变换，以自动逐步输出机器人手臂动作，在推理时间的每一步。
所以只是看一下这里的一些例子，比如这个提示重新排列对象以匹配场景。
这是一个经典的任务，被称为视觉目标达成，有很多先前的研究作品。
这就是我们的机器人是如何做的，给定了这个提示。
我们还在上下文中提供了一些新概念，比如这是一个blicket，这是一个wook，现在把一个wook放到一个blicket里。
而且这两个词都是无意义的。
所以它不在训练数据中，但VIMA能够泛化零样本并跟随运动来操作这个对象。
所以机器人理解我们想要的，然后跟随这个轨迹。
最后，我们可以给它更复杂的提示，比如这些是安全约束条件，将盒子扫入这里，但不要超过那条线。
我们将使用交错的图像和文本标记来实现这一点。
最近，谷歌大脑机器人跟进了VIMA，推出了RT1和RT2，机器人变形器一和二。
RT2使用了我描述的类似方法，首先在互联网规模数据上进行预训练，然后再在谷歌机器人上进行一些人类收集的示范数据上进行微调。
DeepMind的RobotCat是另一个有趣的工作。
他们训练了一个统一的策略，不仅适用于单个机器人，而且还适用于不同的机器人形态，甚至可以泛化到新的硬件上。
所以我认为这就像是一种更高级的多模态代理，物理形态因子，代理本身的形态是另一种形式。
这就结束了我们的展望部分。
最后，我想把我描述的所有作品都联系起来。 
所以这就是 mindojo .org。
我们已经开源了一切。
嗯，对于所有那些热衷于开源的项目，我们尽可能地开源，包括模型代码、检查点、模拟器代码和训练数据。
这是 voyager .mindojo .org。
这是尤里卡。
这是 VIMA。
还有一件事，如果你只是想找个借口在工作时玩《Minecraft》，那么 Mindojo 对你来说是完美的，因为你正在收集人类示范以训练泛化能力。
如果有一件事你从这次演讲中记住，那就是这张幻灯片。
最后，我只想提醒我们所有人，尽管我展示了所有的进展，我们能做的仍然远远不及人类智慧所体现的代理。
这些是我们数据集中的视频，展示了人们进行如装饰冬季仙境或在《Minecraft》中构建功能性 CPU 电路等任务。
在AI研究中，我们离那还很远。
所以这里向社区发出一个呼吁。
如果人类能完成这些令人惊叹的任务，那么为什么我们的AI不能呢？
让我们一起找出答案。
