大家知道这是什么吗？
有个斯坦福，斯坦福的地点。
你知道哪一个吗？
嗯，首先，这是什么？
发生了什么事？
电信领域的第一个开始。
没错。
然后斯坦福有什么关联吗？
我相信这是 Maparti。
是的。
谁开始了 SAIL，如果我理解正确的话。
是吗？
因为他开始了 SAIL 吗？
是的，我想是他。
但无论如何，有趣的是，看看他们在他们的宣传册上写了什么，或者说他们在他们的目标中写了什么，是有趣的，对吧？
所以字体有点小。
好的，所以这项研究是基于这样一个假设进行的，即学习的每个方面或智力的任何其他特征原则上都可以被如此精确地描述，以至于可以制造出一台机器来模拟它，对吧？
太棒了。
所以单一的机器，你想模拟所有的，就像人类智能一样。
精心挑选的一群科学家，我们认为实际上在第二组红色下划线之前的段落中，如果一群精心挑选的科学家在一个夏天一起工作，我们认为可以在其中一两个问题上取得重大进展。
好的。
我觉得他们当时其实并不知道AI寒冬。
他们那时不知道。
第三件有趣的事情是，主要障碍并不是机器容量的不足，而是我们无法充分利用我们所拥有的优势来编写程序。
所以，虽然目标是崇高的，但令人惊讶的是，在座的一些最聪明的人中，你会有多么错误，对吧？
所以Selfridge和神经元Prokopogee是最初的混乱，我认为他基本上什么都懂，除了黑盒优化中的路径问题。
然后是明斯基，当然还有香农，所罗门诺夫，我想这是所罗门诺夫，最小描述长度。
在很多方面，你可以说这是当今自监督学习的基础。
但真的很有趣的是看到第一次，我是说，至少我不知道我们是否能够表征或写下所有智能的规则。
所以你可以想象，他们采取的方法都是这些基于规则的系统，对吧？
而且在机器容量上，他们可能错得不能再错了。
今天的transformer是数据中心，对吧？
我猜，他们可能需要一个真的、真的很长的夏天来解决这个问题。
所以是1955年，大约60年了。
不，我快要70岁了，对吧？
基本上我们再次讨论同样的问题，只是有些东西有效，有些东西无法解决。
而这次谈话是关于使这个更大的企业运作的其中一个组成部分。
我们离达特茅斯大会最初的目标越来越近了，是的。
再说一遍，这就是巨大的差距。
我是说，最终在这个领域发生的事情是，他们希望有一个单一的系统，能够解释大部分的事情，能够模仿我们的认知能力，这肯定包括图像处理或图像理解，还有语言处理，对吧？
我的意思是，该领域受挫了，也就是说，有一个单一的模型或方法来完成所有这些事情的目标，被成千上万个不同的研究项目摧毁了。
所以我的意思是，这里没有整合，但是这是另一个，这是另一个，这是另一个，这会更难一些。
你能告诉这是什么吗，这是2009年，而不是一个单一的系统，这是一个复杂的机器翻译系统。
所以当我开始我的博士学位时，机器翻译系统实际上比这更复杂一些。
成千上万的管道系统，你首先必须提取，你首先必须做单词对齐，实际上看起来像注意力，你想象一下它是硬注意力，然后基于此我们提取了所有与其他短语对齐的更大短语，然后你必须弄清楚它们如何，然后你必须教，那里有一些机器学习，你必须教模型如何评分它们彼此连接。
那么你能，有人知道神经网络在哪里吗？
所以这是2009年的一个机器翻译系统，CSM是一个连续状态语言模型。
那是用于重新评分的用途，对吧？
那时候世界是如此离散，以至于你不得不称这些模型为连续状态语言模型。
我是说，这主要是受到神经概率语言模型的启发，哦，它没出现。
抱歉，在那儿，我认为是2003年的本杰鲁的神经概率语言模型。
所以，即使在2013年我发表了一篇关于神经网络语言模型的论文，这些模型仍然被放入，前馈神经网络语言模型仍然在重新评分。
现在想想，这真是令人难以置信。
所以就合并方面来说，所有这些复杂的系统，现在都被仅仅是彼此交流的神经元所取代，你只需从数据中自动地学习规则。
所以这很有趣，很有意思。
自那时以来，你知道，这就是2013年的EMLP会议的样子。
你看到这些不同的，像垂直化分析，不同领域，如形态学，对话和篇章。
我甚至不知道现在人们是否还和模型交谈。
我不知道是否还有研究课题了。
然后就像机器翻译一样，所以像这样的意见，挖掘，激励和分析。
现在的模型会让你生气或心烦。
所以您可以看到，就在2013年，这个领域，甚至研究都分成了这些更小的领域，每个人都有自己特定的，他们带来了自己特定的领域信息，他们必须专注于某个领域才能解决一些任务。
我们在某种程度上解决了一些任务，比如机器翻译，因为，我是说，可能还有很多政府资助，我们取得了很多进步，正在开发部署到实际环境中的翻译系统。
谷歌翻译就是一个很好的例子。
对。
自那时以来，你知道，就像你有这个，你知道，我们首先开始了，我们都同意我们需要分布式单词表示。
你看到了这个，我可能还记得这个有趣的嵌入代数，aim减去man加上woman等于queen来自word to vec。
而且，实际上有一个大型的模型产业，这些模型仅仅是单词表示，而这些单词表示实际上在下游任务中是有用的。
然后，又像是，你知道，这个过程的另一步，这个过程的另一步，现在我们开始说，好吧，这些表示是在那里的，但是只有在上下文中学习才有帮助。
对。
所以国王应该根据上下文进行更改。
波斯国王或皇帝国王都没有穿衣服。
对。
因此，我们看到了诸如序列到序列学习之类的方法，我们开始制定，我们开始创造这些解决自然语言处理中任何任务的一般性公式。
对。
所以序列到序列的公式，如果你能，你可以你可以你可以你可以用语言来制定许多任务，序列到序列，问题回答，机器翻译，对话。
然后当然，我们然后我们发展了注意力。
对。
这是一种非常有效的基于内容的方法来总结信息。
如果你通常使用编码器或解码器架构，每个人可能都对编码器或解码器架构感到熟悉。
对。
所以你进入解码器架构，解码器一侧的位置会根据其内容总结源句的所有信息。
对。
这是一种非常有效的基于内容的方法来总结信息。
开始发生的事情是，我们开始形成这些一般性的范例。
序列到序列学习可以解决大多数语言问题，因为大多数语言问题都涉及到学习可变长度的表示。
目标是学习可变长度序列的表示。
如果你成功地做到了这一点，你就有可能解决那个问题。
然后通过一种出色的方式，一种基于内容的方式来注意，实际上总结一些邻域的信息。
而且，直到那时的主要工作马是这些是当前模型或LSTM。
对。
基本上方法通常是相同的。
你有一个你有一个句子，你把句子压缩成一组向量，一组表示，通常是每个位置一个。
对。
LSTM的工作方式是它沿着句子走，吃掉一个词，然后总结这个，将整个历史总结为一个固定的瓶颈。
然后这个瓶颈会被传递，会根据下一个词进行更新。
所以现在，如果你成功地学到了表示，我们就可以解决这些任务，翻译、总结对话。
所以这是一个重要的进展。
在2020年，我猜是在2015年NeurIPS发表了序列到序列的学习论文？
然后我们看到了2015年、2016年的注意力论文。
机器翻译界是第一个做出回应的，他们说，嘿，你知道，机器翻译是一个经典的序列到序列学习问题。
就像，我们为什么不首先开始重新评分，然后我们还能不能用序列到序列模型重新思考机器翻译？
对。
这些都是很棒的模型。
我不知道你们有没有做过这些关于LSTMs的练习并且会计数。
比如说，如果你训练一个编码器解码器模型，如果你想要将A模拟到结尾，B模拟到结尾。
所以你输入NA并且要求解码器预测NBs。
实际上只有一个单元的LSTM，如果你知道LSTM的结构，它有一个基本上保持的单元。
所以这是一个状态的概念。
而只有一个单元就能够实际上做简单的计数。
它计算你消耗了多少个A，然后它递减。
然后当你消耗了与A相同数量的B时，有什么东西会亮起来并且说，我完成了。
我已经识别出这种语言。
所以你可以将其视为从A到结束，从B到结束。
而且在这里，你有一个抱歉，这不太清楚，但你在这里有一个语法。
而且你可以看到这些是不同的单元格。
这里大约有八个单元格，每个单元格实际上都会递增其计数器。
一旦它读取特定的符号，它实际上可以跟踪你在这个层次结构中有多深，你在这个语法中有多深。
而且谷歌，当然，是序列到序列模型的巅峰成就，这点我当时确实是对的。
我很幸运能够看到人们正在做的工作。
谷歌神经机器翻译系统是他们采用的一个地方，他们采用了LSCM的地方。
我的意思是，他们添加了许多先进的东西。
有很多系统改进，谷歌拥有的大量数据，他们制作了当时你可能会称之为的东西。
当时的最先进的神经机器翻译系统序列到序列模型。
所以现在这个大而复杂的系统，看起来更加复杂，现在变成了一个同质的、就像一个单一的同质神经网络。
因此，当时我们最大的挫折是，我的意思是，这些 LSTM 是主要的劳动力。
 而我们遇到的最大挫折是，我的意思是，我们不仅在生产，不仅在积极地生产输出顺序，我们还在从左到右依次解码输出。
但我们也在按顺序读取输入。
因此，为了生成第 10 个单词的表示，你必须吃掉第一个单词、第二个单词和第三个单词。
所以这真的很慢，而且 LSTM 的另一个大问题是，你有一个瓶颈，基本上包含了你过去的所有信息。
因此，您现在必须将您可能进行的远距离交互和本地交互都打包到您需要传输的单个固定向量中。
而顺序性抑制了并行性，这意味着你甚至无法读取，就像编码器甚至无法并行地读取句子一样。
当然，解码是自回归的，所以你甚至无法并行写入。
而卷积法在很大程度上开始成为一种解决方案。
我的意思是，他们的设想非常成功。<EOS
他们还想出了如何对它们进行优化，如何在GPU上使它们运行得非常快，因为它们基本上只是矩阵乘法，而矩阵乘法在很大程度上是可并行化的。
所以卷积就是解决这个不能并行读取的问题的一个解决方案，对吧？
因为你可以并行地，每个词基本上通过查看其邻居生成其表示。
它的本地邻居。
还有一些非常重要的论文，比如ByteNet用于机器翻译，卷积序列到序列模型与Transformer同时期，实际上可能早了几个月。
他们在编码器和解码器中都使用了卷积，以在机器翻译上获得比Google神经机器翻译系统更好的分数。
当然，可能最成功的是WaveNet，这是一个当时处于最前沿的文本到语音系统。
而且，卷积仍然存在这样一个问题，即，我猜它们是可并行化的，但问题是你仍然不能直接捕捉单词之间的长距离交互，对吧？
所以如果你基本上是一个感受野，如果它是一个三乘三，如果它是一个一乘三，那么它基本上是与因子成线性增长，每次它扩展三倍。
所以你仍然需要线性数量的层来捕捉这些长距离关系。
但是另一方面，注意力是这个真正有效的机制，我们知道它实际上可以让我们进入一个，它实际上可以使用基于内容的寻址来捕获一个词与每个其他词之间的所有交互作用，对吧？
因为卷积基本上是将权重与参数匹配，注意力实际上能够使用内容与内容。
所以根据我与我的邻居有多相似，根据我与我的邻居有多相似，我将吸收那些信息。
而且这实际上，这种模式实际上无处不在，甚至在计算机视觉中也是如此。
所以也许实际上我可以去那里。
所以这是一个愿景。
有这样一种方法。
这里的人们知道非局部均值吗？
所以在计算机视觉中，有一种称为非局部均值的方法。
基本上，这最初是为图像去噪而开发的。
所以如果你想去噪声一个图像片段，基本上，你看看你所有的邻居，然后你看看哪个片段和你非常相似。
根据相似性，你实际上会吸收那些信息。
所以，这在图像中基本上是有效的，因为图像是非常自相似的。
这开始听起来像是，嘿，基于内容，我想吸收那些信息。
而且，还有类似的，以前有一些方法，比如Ephros的纹理合成，如果你想做绘画或者生成图像，那么你会看看一个和你字典中的这个矩形相似的片段，或者是你有的数据库中的一个片段。
然后根据最接近的，你实际上把它带过来，对吧？
所以你会把那个片段带过来，然后把它粘贴在那里。
所以这些看起来像是注意力的方法已经普遍存在了。
这是一个非常自然的表述。
而且巴丹诺的论文表明这对语言也非常有效。
所以问题就是，好的，为什么我们不能学习表示，而不是作为这个源目标，为什么我们不能通过句子自身的关注来学习表示呢？
所以现在你基本上使用的是，而不是参与一个源句子，参与一个目标句子，它能不能只关注于自身？
而最初的目标实际上是当我们想要实际进行并行解码时。
因此，注意力机制在构造上是可并行化的，因为每个令牌基本上可以从其邻居那里并行地构建其表示。
对。
并且它直接捕捉令牌到令牌的交互，因为当然，我们会遇到长度的复杂性，但我们可以讨论，并且我们将讨论并解决一些这些问题以克服它们。
但你可以直接捕捉这些交互，而不是有这种线性增长和接受域，因为如果你有一个非常非常大的接受域，卷积在计算上变得非常昂贵。
它还具有这些显式的门控和乘法交互，这是我们经常在门控像素CNN或Gell -E中看到的，这些显式的门控乘法交互通常有助于训练，并且已经导致了更好，更好，更好的准确度。
正如我之前提到的，我们实际上想要做这件事情的最初动机是我们说，好的，所以这些弹性都是，你有很好的翻译系统，但问题是实际上读和写都是按顺序进行的，我们能不能同时并行进行？
所以我们想要读取，我们想要并行读取德语句子，然后翻译，然后也通过并行进行写作。
而不是实际上自回归地解码它，你能否在高度上解码它而不是按时间解码它？
就像你先吐出一个词或者你吐出所有的词然后逐步定义它们。
对。
这事实证明是非常非常具有挑战性的，直到今天也还没有成功解决。
因为最大的挑战实际上是，每当你解码时，当你预测一个词时，你会在概率分布上施加影响，然后这个概率分布就会固定下来，进而缩小你之后要预测的范围。
而且，允许你基本上确定这些模式的顺序非常难学习。
所以强加一个左右顺序比实际上没有一个顺序并且在解码时必须学习它要容易得多。 
所以最初的方法没有奏效，但我们仍然能够通过阅读并行数据来找到救赎。
所以我们说，好吧，让我们把这个问题带回到编码器-解码器模型。
而不像当时，有几种表达方式。
对。
所以我们有了最初的格雷夫斯注意力公式，然后我们有了加性注意力公式。
我们选择了点积注意力公式，主要是因为它允许我们这样做，因为它允许我们将注意力实际上作为矩阵乘法来执行。
通常情况下，物理学实际上是神经网络中的一个很大的约束条件，如果你能使你的架构适应现代加速器，你就有更好的成功机会。
点积注意力可以被表达为矩阵乘法。
而且，已经有了针对在GPU上非常有效地执行矩阵乘法的内核。
所以公式是，好吧，现在我们有了类似于点积注意力的东西，我们有一个缩放因子，简单地因为如果点积实际上变得太大，并且你可以在表示中的均值和方差的某些假设下解决它，它实际上并没有被更新。
是的，所以我们的表述基本上是，你有你的查询，你最终要做的是，如果你有一个位置，你就把它强制投射到查询中。
 然后，同一标记、同一标记的表示也会被投射到键和值中。
首先，查询决定了你实际上要从所有这些键中提取多少内容。
因此，首先要对查询和每个键进行点乘。
然后在此基础上，根据使用软最大值对其进行归一化后的得分，组合或提取所有这些位置上的内容。
因此，从某种意义上讲，您可以将自我关注视为一种基于内容的汇集机制。
而缩放因子基本上可以避免你，就像它使我们避免了这些对数实际爆炸和训练变得不稳定。
而在解码器方面，你只需添加一个注意力掩码，就能轻松实现因果关系。
而这给我们带来的好处就是，好了，现在我们已经解决了翻转的问题，我们稍后会讨论这个问题。
但现在我们有了一个可并行化的机制。<EOS
它直接提供了内容，它直接提供了标记交互，我们相信这将帮助您更好地学习这些单词之间的关系。
自注意力的复杂度比卷积更快，对吧？
因为卷积在数量上是二次的，它们在通道数和隐藏维度的数量上都是二次的，但是自注意力在长度上是二次的。
所以如果您的长度不比隐藏维度多得多，您实际上已经节省了浮点运算。
现在，这还不是完整的图片，因为并不是所有的浮点运算都是相等的，我们稍后会谈论这个。
当您把一切都放在一起时，基本上我们将基础部分进行了调整，这实际上与 ResNet 架构非常相似。
所以如果我们看看 ResNets，对吧，那么在 ResNets 中您有收缩，您有通过卷积进行空间混合，然后您再次扩展，对吧？
如果您只是，如果您只是调整，如果您只是将它向下移动一步，那就非常相似了。
你有关注，然后你有扩张，收缩，但是残差连接的位置有所不同，但是它是非常相似的，是一个非常相似的基本构建块，有残差连接，还有这些收缩和扩张。
然后是transformer，那些是多头注意力与扩张收缩，这是在前馈层中。
然后关于注意力的一个挑战，我们的循环，LSTM可以计数，它们可以影响，它们可以计数，它们可以学习有趣的时间模式，但是注意力是排列不变的。
所以我们实际上不得不添加位置，我们不得不添加位置信息，以便我们可以学习顺序。
所以我们在输入时添加位置信息，这通过残差连接传输到其他层。
在原始论文中，我们有后层归一化，但后来我们意识到，随着模型变得更深，后层归一化无法有效训练。
所以我们不得不使用预层归一化的公式，这也在原始ResNet论文中观察到。
所以这个模型基本上是，好的，你有你的输入，然后你有空间混合，通过注意力进行空间混合，前馈层，这种情况反复发生。
解码器端的不同之处在于，现在每一层都有编码器-解码器注意力和编码器-解码器注意力。
如果有任何问题，是的。
是的，你对输入和层后归一化背后的直觉是什么？
哦，所以最终，如果你使用层后归一化，实际上，丽兹，我有那张幻灯片吗？
让我查一下。
可能我没有。
但如果你使用层后归一化，那么你基本上是压缩了残差和可加性部分。
所以当你，所以你从较低层的激活不断地通过层归一化。
但在层前归一化中，只有残差路径有一个层归一化，这意味着从模型底部一直到顶部的激活是自由的。
它们是不受影响的，可以通过。
是的。
好，所以现在，我是说，直到这一点，我们还没有讨论为什么我们，你知道，我们还没有讨论多头注意力，这最终变得非常重要。
所以注意力的一个问题是，想象一下，如果你想，我是说，往往语言是关于理解谁对谁做了什么。
那么在这种情况下，天主教徒，所有者的手，所以晚了，谁舔了什么，就像天主教徒，所有者，对吗？
所以现在如果你实际上想要结合这两个插槽的信息，这些位置，这些向量，那么你能做的最好的就是使用注意力是0.5，0.5的单层，并且半概率，半概率。
然后它们被混合在一起，对吧？
但现在想象一下，想象一下卷积的强度。
它实际上可以有，实际上应该有，好吧，好吧，我认为重点仍然会传达出来。
所以现在卷积可以做的是因为它具有，它基本上应用了本质上的卷积，在这种情况下，它是一个五乘一。
它真正做的就是在每个位置应用不同的线性变换，对吧？
所以它可以接受任何，因为这些线性变换是不同的，它可以，第一个线性变换可以学习，我将从这里获取一点信息，我将从这里获取一点信息，然后我将它们放在一起，对吧？
而注意力，你实际上可以做到这一点的最佳方式是通过平均化，我们将展示所有这些事情。
但是有不同的线性变换可以让你在这里获取嵌入的一部分，在这里获取嵌入的一部分，进行混合，然后可能在不干扰彼此的情况下把它们放在一起。
而多头注意力，有点类似于基本上是一个多磁带、多头的图灵机，具有不同的读写头，基本上允许你开始恢复这种性质。
现在，你所做的是，你基本上，你恢复了选择输入不同部分的能力。
所以你把隐藏维度分成独立的部分，然后每个部分现在都能进行注意力。
现在你可以在这个地方有概率1，而在这个其他子空间中有概率1，而不是有0.5、0.5的概率。
现在你不必像，你不必获得这些平均效果，你实际上可以有选择性，对吧？
而且出于计算原因，我们不是实际上有八个注意力层，或者六个注意力头，每个维度为d，而是有八个注意力头，每个维度为d乘以八，对吧？
所以，因为，这样我们就不会增加任何浮点运算次数，对于相同数量的浮点运算而言。
但这只是故事的一半，因为注意力头本身确实非常昂贵，后来必须进行改进，对吗？
而且最重要的部分，可能是最重要的结果是，使用transformer，我们能够胜过以前的集成模型。
而且这是非常令人兴奋的，单一模型实际上能够胜过以前的集成模型。
而且不仅如此，这是一个机器翻译 WMT 2014 英语、德语和英语-法语机器翻译任务，我们不仅能够在更少的浮点数运算中完成，而且这些，就像，很明显这是一个非常通用的模型，因为我们立即将其应用于解析，我们能够用一个小型模型获得出色的结果。
所以，在某种意义上，我们是，这非常令人兴奋，因为这意味着，好吧，现在我们正在尝试进行的机器学习的这种整合，我们可能拥有比以前更通用的模型，现在我们可以将其应用于不同的问题，对吗？
最终，为什么？
因为拥有一个能够结合语音、图像和语言表示的单一模型会很有帮助。
而如果你有一个在所有任务中都表现良好的通用基质，那么潜在地我们可以达到单一的多模型。
有时可解释性就像茶叶一样，就像阅读茶叶，所以你要小心，但很好的是，注意力本身可以给你一些可解释性。
而且我们能够看到一些这些注意力头部或注意机制实际上能够学会远距离的关系。
有些实际上学会了在transformer早期就变得相当早的，我们看到这种通常不变的模式，其中一些注意头部基本上就像卷积一样，它们只是，它们只是吸取本地信息。
当然，现在有了一些机械解释性方面的更高级的工作，涉及到grokking和在entropic中发生的事情，这是它们现在学会了如何解释这些感知头部。
所以这很有趣，但我们能够看到一些这些头部实际上执行非常明显和清晰的动作的轶事证据。
好的，如果还有任何问题，那么我会暂停一下。
你购买了这方面的研究吗，就像是感应头在进行上下文学习的？
是的，很难说因为，所以我还没有看过最近的工作，但他们已经解决了这个叠加的问题，是吗？
所以现在解决了这个问题，他们能够，大致上，这是否意味着他们现在能够为每一个感应头分配区分特征并解释它？
从我的理解来看，或者说在上下文学习的部分，是这样的，是他们必须展示它，还是说他们说上下文学习是因为感应头？
哦，这是一封信。
是的，不太清楚因为，是的，我认为可能有许多，许多种类的上下文学习已经在很多不同的任务中得到证明，而我实际上并没有很好地关注这个。
我不知道，我不知道感应头通常是什么？
它们有什么样的性质？
你知道它们有什么样的机制吗？
好的，是的，那么，由于我们俩都不太了解这个，我在这里无法深入讨论，但我不确定他们是否已经能够解释大多数上下文学习的内容，因为归纳头部，根据我的理解。
他们可能已经做到了，是的。
有人了解归纳头部吗？
好的，那么，多年来，有一些，有许多论文，但有一些重要的改变。
有一些变化是被保留下来的，并且新的transformer通常具有这些改进，对吧？
我们将从底部到顶部列出其中一些，并看看哪些实际上被保留了下来，对吧？
所以我们从第一个开始，自我注意力最大的问题之一是，自我注意力本身是置换不变的，对吧？
您需要，您需要，您需要，您需要为其提供位置信息，以便它能够学习某种时间结构。
在原始的transformer中，我们使用了这些正弦函数，我们希望它实际上能够学习相对位置编码，因为您可以将另一个位置的位置编码分解为另一个位置的位置嵌入的线性函数。
我们曾经，并且一些，并且另一个，并且另一个因素，这是的，它取决于两者之间的相对距离，但是这并没有发生。
在原始论文中学习位置编码也是如此。
因此，我们无法完全使用正弦函数获得这些模型相对距离。
然后有一些，您知道，一些重要的，这是一个非常偏见的样本，但我认为它通常涵盖了大量的这些，它涵盖了大量的，大量的论文。
大致有三种类别，对吧？
因此，所有这些现在都明确地学习相对嵌入。
所以，在相对位置transformer中，我们为每对相对位置都有一个嵌入，并且基本上使用那个，然后我们对那个嵌入进行点乘，与产生的查询相比，产生一个根据相对距离调制的逻辑。
我们发现这个非常，我们发现这个非常适用于翻译，但我也会展示音乐。
另外，另一种可能是简化。
这是Alibi论文，这是非参数的，这些不是学习到的，而是对于每对位置而言，你实际上只有一个单一的偏置，对吧？
所以，你只需要给logit添加一个单一的偏置，你可以学习它，或者你可以使用启发式方法，Alibi就是这样做的。
相对位置编码的另一个优点是，它们可能允许你对新的、更长的序列长度进行外推，而这是绝对位置编码所无法做到的。
我很好奇大家对此有什么看法，但我相信最新的分区相对位置编码，我相信它被称为行形式，他们基本上只是，你知道，每对维度之间稍微旋转嵌入。
所以，旋转的角度取决于你的实际绝对距离，但最终发生的是，当你执行注意力操作时，你最终会得到相对的效果，你会根据相对距离调节logit。
现在这种方法的显著之处是什么，什么结合了两种世界的优点，对吧？
它实际上，它是绝对位置编码，相对位置编码在其中有一些挑战，因为你必须为每一对维护一个额外的逻辑或嵌入。
所以这占用了很多内存，结果增加了你的内存。
这里，这些实际上是绝对位置编码，但它们给了你，它们最终给了你所需要的相对调制在注意力操作中。
我相信共识是这是最成功的，这是最成功的位置编码。
是这样吗？
或者还有其他人，是这样的，有其他人吗，是共识吗？
好的。
所以，看起来，我会说，这些相对旋转来自于，或者说改革者中的方法，基本上是一个真正的新的改进，现在将会留在transformer中。
它拥有所有你想要的伟大特性。
它是一个绝对位置编码，可以给你相对效果，这正是我们最初想要的。
而且，而且，而且其中一个，而且，而且，强调，强调我们需要相对的，就像，强调两件事。
一种，那就是建模，比如有趣的时间关系，这在音乐中非常重要，需要一个良好的位置表示。
我们实际上在音乐transformer中找到了显著的改进。
这可能可以播放吗？
好的。
所以，所以，所以这里是一个，这是一个启动序列。
这实际上是安娜·黄的工作，顺便说一句。
所以这是在音乐中的上下文学习，因为你实际上看到了这个提示，然后要求模型完成它。
好的。
所以现在这是普通的transformer，你已经可以看到。
所以你可以看到这些，我们尝试了学习和正弦函数两种，你可以看到它一开始都很活泼和开心，但后来变得有点沮丧和困惑。
对吧？
所以它不能够捕捉到这些，因为音乐具有这些有趣的主题，嗯，有不同级别的主题，因为你知道，局部有一些重复，但整个乐曲也有重复。
所以现在在这里，这是使用相对transformer。
这是第一种方法，我们在这里使用了相对嵌入，而且我们必须，我们必须，我们必须开发一种计算高效的方法，通过使用一些矩阵健身来确实将logits放在正确的位置，这样你就可以阅读论文。
很有趣。
所以这里是相同的主序列，让我们看看完成。
安娜，这篇论文的第一作者，也是一位音乐家，告诉我这实际上捕捉到了很多音乐结构。
我觉得比之前的听起来更好，但也许取决于人们的口味，可能一些前卫爵士音乐的粉丝会喜欢第二个，会喜欢第一个，但是这里的重点是，不工作和工作之间的差异是相当明显的。
我认为人们可以尝试一下，使用新的旋转位置编码。
好。
好。
所以，所以，所以，所以现在我们有一个比我们最初用来建模相对距离的更好的机制。
而且，而且，而且，而且，还有，还有，还有基于旋转位置编码的进展，通过调整基频，你可以，你可以，当你，当你遇到更长的序列时，你可以调整基频，然后模型就不会，模型就不会退化。
所以这有好的性质。
可能最重要的是，有，有几个，几个对注意力机制本身的重要贡献，这，这是，这是这里的主要工作马。
它是，你可以将其看作是，它要么是，它是，它是，有感应头正在学习如何复制订单。
或者可能它真正做的只是路由信息，使得巨大的前馈层实际上可以学习重要的特征。
对。
但是有广泛的两类问题。
注意机制存在两类问题。
今天提到的一个显而易见的是长上下文本身。
对。
正如我们记得的那样，复杂度是与序列长度的平方成正比。
当你的序列变得非常非常长的时候，一旦你的序列变得非常非常长的时候，不仅仅是，我的意思是，有一个问题会出现，它会变得非常，它会变得在计算上代价高昂，而且logits也会变得不可行。
好的。
因此，一般来说有几类论文。
其中一类是限制注意力窗口。
我们曾经对图像进行过这样的处理，对于图像，我们对图像进行了局部的一维和二维注意力。
而第一个，我们实际上只是对图像进行了光栅化处理，并且我们进行了局部的一维注意力，这与Mistral论文中的滑动窗口注意力非常相似。
然后在二维情况下，我们有了空间二维注意力。
是的。
然后有这些稀疏版本，你实际上，你有这些特定的模式，经过许多层，我是说，你可以把它想象成如果你有这些稀疏矩阵，你必须将它们相乘多少次，直到你得到一个非常密集的矩阵？
是的。
所以大致上，这种情况可以得到连接性。
那是给我的吗？
不是。
好的。
你可以很快地在远距离像素、远距离像素或音乐旋律中的远距离音符或单词之间建立连接。
然后还有第二个，这个方面的工作还不够，存在一些挑战，但是这些非结构化的稀疏注意方法。
它们通常是在更高层次上，它们实际上试图假设我走到你面前告诉你，嘿，这些是一堆令牌，它们具有非常高的兴趣相似性。
就像它们很可能会关注彼此。
我能多快地近似它，而不必实际执行整个计算？
对。
所以你有两种方法，在路由注意力方面，你使用向量量化，在LSH或者，在这篇论文中，他们使用了LSH。
在路由transformer中，大多数层实际上都是本地的。
最后的层，通常是那些倾向于建模这些长距离关系的层，实际上是使用这种基于内容的非结构化稀疏注意。
结果通常更好。
而且有趣的是，也许我们可以在非常长的序列上构建模型，其中大多数层都是相当局部的，只有很少的层实际上在执行这些远距离的注意力。
现在，其中一个更大的挑战，实际上，即使最终你会无视掉很多如果你没有引入注意力会做的浮点运算，问题总是最终变成了内存移动，总是最终变成了内存移动。
而且在这里还有更多的创新，比如内存带宽的提高，也许其中一些方法在今天变得比我们写这些论文时更可行。
但这是一种有趣的方法，基本上是试图近似原始的注意力矩阵。
抱歉，这有点傻，但澄清一下，这种非结构化的稀疏思维看起来会与仅在某种程度上稀疏的卷积有很大不同，因为你正在失去任意比较元素的很多远距离或无关的上下文吗？
对。
所以我想说这与卷积很相似。
如果你做得完美，那么你关注的内容将具有非常低的关注率。
所以你基本上是在尽力猜测彼此之间可能发生的情况。
因此，您可以将此用作基于结构稀疏性的内容。
可能还有一些更有趣的工作要做。
也许不仅仅是一次处理一个标记，从而导致大量的内存移动，而是决定哪些块希望自我关注哪些块，然后您一次只移动一对块。
对。
所以我认为这里有一些有趣的方向。
当然，事实上，最终投资的那些最简单的方法也是存在的。
因为结构稀疏性很容易，您可以在现代加速器上轻松优化它。
所以再次强调，物理学，你应该把物理学当作你的朋友。
因此，通常本地注意力或滑入注意力，我们仍然经常看到它经常出现并表现良好。
这些其他非常狂野但更具表现力的非结构稀疏注意方法通常并未取得太大成功。
当然，还有线性注意力变体，我认为今天没有任何一种最先进的体系结构中包含它们。
有其他方法，嘿，不是实际上做 n 平方，而是做 n 平方 d，其中您学习了新的 k 个嵌入，您做了 nkd，然后您做了ndk。
所以你基本上将其分解了，对吧？
就像模拟矩阵分解一样。
另一种有趣的方法是我们通常使用检索作为工具。
那么，为什么不假装您的记忆，您的记忆本身就是文档，并在那里使用检索作为工具呢。
因此，记忆转换器基本上在局部和长期记忆之间进行了混合，然后从非常非常长的记忆中检索。
他们发现您无需从头开始训练模型。
您所需要做的就是在一些少量数据上采用这种方法，然后您就能学到一个很好的检索机制。
我认为这很有趣。
它在某种程度上，仍然是基于内容的，基于内容的决策，决定我应该关注什么。
但我喜欢的是它确实使检索成为一种工具，您可以将其用于自己的记忆或者您可以将其用于文档。
这是一个很好的总体观点。
好的，现在是第二部分，基本上，你会遇到一个问题，即不是所有的浮点运算都是相同的。
对。
所以，如果你看内存层次结构，很多存储在 GPU 高速内存中的激活值，今天在 H100 上大约是 80GB，但你知道，H100 是 80GB，而 A100 是 40GB。
对。
所以，高带宽内存是有限的。
你首先必须从高带宽内存到 SRAM，然后再到计算单元，最后返回。
对。
所以每一次，我是说，这可能，你知道的，无论何时，如果感兴趣，你可以看看 roofline 分析，roofline 分析实际上可以为任何设备提供一个很好的特征图。
你知道，在哪里你需要，在哪里你的工作负载或操作需要，以便你可以有效地利用计算机。
你希望成为计算密集型，因为最终，如果你不计算表示，你不计算，你就不会得到任何输出。
但是，如果你花很多时间在移动东西上，相对花费较少的时间在计算上，那么你实际上，你在浪费努力。
对。
所以标准的注意力机制之一，对吧。
其中一个问题是，好吧，想象一下，你的查询、键和值都在你的内存中，但是然后你需要的标准方法就是，你移动它，你从HBM中移动它，你进行计算，你计算注意力，你计算logits，你把logits移回HBM，然后你可以得到softmax，对吧，将softmax移回HBM。
然后你基本上加载概率和值，最终计算输出，对吧。
所以算术强度，或者算术强度或操作强度，即你在注意力上提供的flops数量，即使它的flops少于一对一卷积，它的强度更低，因为它通常有更多的内存移动。
而一对一卷积的内存移动较少，你只是移动权重，移动激活，进行计算，然后将它们带回来，对吧。
对于卷积也是一样的。
卷积具有很高的算术强度，但并不是你只想要最高的算术强度或操作强度的操作，因为你仍然希望有用的参数，对吧。
这是一个权衡。
所以有一堆改进是会保留下来的，我的意思是，它们几乎肯定会保留下来，试图应对这个问题，无论是在训练时间上，因为你的logits可能会变得非常大，还是在推理时间上，或者你的KV，在进行推理时，你只有一个查询，但是你的KV哈希，对吧，你必须维护你的键和值，这可能会增长相当多，所以你必须移动它。
所以第一步是，嘿，让我们只减少激活内存。
所以多查询方法，基本上是，你减少了多个查询，但是，你将读头的数量减少到只有一个，所以你只有一个键和一个值，这确实降低了你的表达能力。
所以分组查询，现在是一个简单的平衡，基本上是说，嘿，我们不要把所有这些临时激活内存的极端情况考虑进去，让我们把它们分组到不同的查询中去。
所以一堆查询将关注相同的键和值。
然后发生的另一个事情是，这里要注意的另一点是，所有这些都是相对的，因为在这些非常、非常的工作中，但是我应该说，不担心你的注意力的第三种方法，就是让模型变得非常大。
然后你只需进行大约三倍的计算和你的注意力计算，就像一小部分那样，所以你不用担心。
对吗？
所以通常，即使是这些较大的模型，即使是分组的查询注意力，它比多查询具有更多的激活内存，但是对于这些大型模型，它仍然不是一个更大的比例，它并不是一个更大的比例，它仍然是你在三倍中所做的较小部分，所以你在验证，对吗？
所以我猜三件事情像是被忽略了，使它变得很大。
其次是，我猜，你相信长期的上下文，你可以做一些我们谈论过的方法，但是你也有这些系统优化，这些相当酷。
所以softmax有一个有趣的属性，你可以以在线方式计算它，你可以增量计算它。
所以如果你有一堆logits，你有点流式传输它们，如果你有一个部分softmax，一个新的logit进来了，你可以以在线方式更新它。
对吗？
那是什么意思？
那意味着现在，你从来不需要将logits或键写入HVM。
所以你节省了很多，对吗？
如果有非常长的序列，你最终会写很多。
所以你在那方面节省。
而这两种方法最终都归结为一种情况，第一篇论文是关于引入了TPUs的，它引入了这个属性或利用了这个属性，以便能够以在线方式进行softmax操作。
而第二篇论文，即今天的闪光注意力，有许多进展，他们实际上进行了一些系统级的优化，现在你实际上可以在GPU上处理非常非常长的序列，通过基本上不将logits移回HVM，使用这个在线属性，并且还编写了使用S-RAM和一切的正确列，使用GPU。
有任何问题吗？
现在几点了？
所以我们基本上是20分钟，我结束的时间。
所以我刚刚涵盖了这两个，你知道，有很多，有很多，我想，还有其他重要的改进。
你知道，我倾向于在预处理和后处理与后处理层规范之间，有一些，有一些对前馈层本身的变化，就像你可以，你可以，你可以盯着前馈层看很久，一切都变成了注意力。
但在前馈的情况下确实如此，如果你看一下，你可以把它们看作是类似于注意力的东西。
而且有一篇论文，有点像，你把它变成了一点像，把它变成了记忆。
最初是由Facebook提出的，我实际上忘记了是什么，但它并没有，就像前馈层仍然存在。
我是说，我们通常没有看到很多关于它们的改进。
确实有一些，我，确实有一些关于更高阶注意力的努力，注意力，如果你把它看作是第三阶交互，你有查询、键和值，但是，但是现在，但是你可以想象实际上有四阶交互，你实际上正在计算一对事物相对于所有事物的逻辑，对吗？
所以这些不是更高阶的交互，现在你可以有复杂的，就像这样的几何形状，你实际上包含在你的，包含在你的注意力计算中。
也许这对生物学或某些，生物学可能很重要，但它并没有被深入研究。
实际上起作用并且可能不会保留的是一些关于密码解码的方法，不完全是最初的，不是我们所期望的非自回归愿望，而是这些更加猜测性的解码，在那里的启发法则相当简单。
是的，如果你想的话，你可以不使用一个繁重的模型生成，而是使用一个真正轻量级的模型来生成，从而捕捉到多样性，然后用一个繁重的模型进行评分。
所以你，然后你重新排列列表，这样做效果非常好。
而且大多数，最先进的，大多数的生产部署可能都使用了推测解码。
好的。
现在换个话题，我想，你知道，我们开始，我们开始，我们开始这个，我们开始这个，我们开始，我们开始通过，你知道，编码达特茅斯会议，他们想要建立一台单一的机器。
现在的问题是，随着大型语言模型如今占据了大部分互联网，我们是否已经达到了那里？
我们看到了一些了不起的东西，我们终于看到了自监督学习以前所未有的规模上的工作。
现在，通过精心策划和庞大的文本量，用非常大的模型仔细地消化，你能够，然后他们能够执行可能仍在等待确认的任务，像是执行了一系列任务，或者他们至少能够执行各种各样的任务，只需在提示中指定它们。
现在，现在，现在就好像你有了，你知道的，你有了一台新电脑，对于那些对代理人的未来非常兴奋的人来说，现在他们可以用同一台电脑编程成千上万个代理人。
哦，也许你现在有了，现在他们有了，现在他们有了可以用同一台电脑编程的多个代理人，然后协调解决问题。
所以我们离单一模型更近了，虽然还不能完全指定智能的所有规则，但至少能从数据中学习所有规则。
我们非常接近了，比以前更接近了。
现在，这还不包括所有重要的事情，所有重要的专业化之后必须发生的事情，就像我们的，检查或调整模型以使其更易操控。
是的。
但是，就目前而言，transformer表现出的缩放定律比任何其他现有模型都要好。
是的。
有一个有趣的问题，你知道，我们能建立一个更好的模型吗？
从斯坦福、克里斯·雷实验室，有一些努力。
有一些RNN的复兴，但我认为唯一的，唯一的，唯一的事情我会说的是，注意力操作本身，实际上是移动信息或根据内容路由信息的这个操作，非常，非常有用。
而且，这种空间混合采样、下采样架构的普遍性在提供、计算机视觉和语言领域都保持了下来，这也许并不奇怪，现在有了Transformer。
因此，有一些不变量可能会保持，但我认为也许在那方面还有很大的改进空间。
我的意思不仅仅是在架构上，而且在数据本身上，可能在数据上有2倍的改进。
但我不会说未来没有能够获得更好的扩展规律的架构。
它们可能会，但Transformer的一些特性，如自注意力和它的一般结构，很可能在未来的架构中会看到。
而且，要真正考虑现代的大规模Transformer，很难，如果有人真的很想研究大规模现代Transformer，你得研究所有的reducers，InfiniBand，Rocky以及它们是否存在拥塞等问题，它们有非常大的集群。
这很有趣。
所以，计算机，transformer现在，在某种意义上，是一个数据中心，因为它没有分割开来，这些大型的transformer分布在成千上万的GPU上。
所以，如果你，现在你实际上必须真正关注几个部分，基础设施和模型本身。
但真正有趣的是，我想，你知道的，我刚刚在想表现出 emergent phenomena 的最小模型。
嗯，我们当然知道 GPT-4，可能是，我不知道你是否被允许说，一些大的，比如，万亿参数。
是的，我想你是被允许说的，是的，那是一个万亿参数规模的模型，这是每个人都说的，规模模型。
然后你有 Brocking，这是一个两层transformer，表现出了一种奇怪的 emergent behavior，当你只是不断地在一些数据上进行训练时，突然就展示出了一个宇宙飞船，对吧？
所以我们很幸运。
真的会有很多，有力量，到处都是奇怪的地方。
小模型和大模型都很奇怪。
也许我们可以通过研究这些小模型来了解大模型，人们希望如此。
但很有趣的是，非常大的模型和非常小的模型中仍然存在着未解释的现象。
但是大型transformer已经不再只是一个整体实验室了。
只是，我是说，可能还是可以的，但是，你必须，有太多的东西，你必须在你的堆栈中保持，才能真正优化整个，这个模型。
当然，一些非常令人兴奋的方向是使用工具的LLM。
是的，现在，现在语言模型或transformer实际上开始使用外部实体了，所以它们正在与世界其他部分连接。
我想这是一个很好的，这是一个很好的销售点，因为通过与，就像，如果你想要获得下一个能力的一大笔资金，它们将从哪里来呢？
而且，很可能通过大量的使用，你将更多地了解如何指导这些模型以及如何在没有，然后在真空中训练它们。
现在，你肯定仍然可以做非常非常重要的工作。
甚至用一个更小的模型，甚至不建立产品，不建立产品，因为有很多重要的未解决的问题。
也许你甚至不应该去研究transformer，因为现在它就像是燃烧人一样，每个人都参加同一个派对。
但是，但是，我认为你将能够在这些，一旦有了这个人，人机协作之后建立新的能力。
当然，你知道，教授模型，或者模型能够表达他们不知道的东西。
在推理时学习新技能的方法，对于代理者也很重要，有一些有趣的工作在Minecraft上显示了这方面的一些证据。
这些扩散模型中一些很棒的特性是，你花费的计算资源越多，图像的质量可能就越好。
但是对于语言来说，我们并没有完全拥有这一点。
那是什么意思呢？
就是说，今天，最好的模型，那些可以进行推理和规划的最熟练的模型也是最大的模型。
可以分离出来。
我们是否可以有一些较小的模型，进行一些自适应思考，并能够匹配潜在较大模型在推理和规划方面的能力呢？
也许答案将通过连接到外部计划者和规划者，或者通过更好地表示数据来获得，实际上您可以更好地推理它。
而且，这再次是一个更系统的部分，但令人着迷的是您实际上可以降低您的位数，您可以使用的位数很少，仍然可以得到一些有用的东西。
我们已经从原始的Transformer使用32位精度进行训练，然后转向B-float 16，现在有迹象表明int8和fp8也会起作用。
我认为这是有用的工作，再次回到相同的论点，即如果您实际上使用较少的位来表示一个数字，那么您实际上从HBM传输的位数就更少。
所以实际上您可以更快。
您可以更有效地利用您的矩阵乘法器。
就是这样。
所以涉及了许多话题，但希望涵盖了一些有趣的内容。
谢谢。
您能谈谈您目前正在做什么吗？
是的。
是的。
所以我是与我的Transformer共同作者Niki一起创办的初创公司的联合创始人。
我们正在努力构建模型，最终将自动化工作流程。
我们从数据开始。
因此，公司中发生的事情非常令人困惑。
公司基本上只是一团黑暗的知识，对吧？
很少有人既具有技术特权又理解得了能够提出问题的人，通常是分析师，但你越是不理解它们，你的公司就越无效。
那么，你如何最终帮助任何人在某种意义上成为有效的分析师？
对。
帮助他们提出正确的问题，帮助他们最终弄清楚为什么，这就需要一些相当复杂的反事实推理。
从数据开始，因为它非常重要，公司基本上是被淹没在其中的，然后从那里开始扩展，然后尝试自动化其他工作流程，最终定价。
但我们相信，我们正在看到一些早期迹象，我们的立场是，我相信这将需要一种全栈方法。
因此不仅仅是构建模型，因为你随后可以控制你得到的反馈。
所以，如果模型中存在空白，你希望得到反馈，这样你就可以在明天改进。
这就是我们正在做的事情。
请在我们完成后与我们交谈。
听到你最后对工具相当大胆的看法，就像你如何尝试支持第三方工具一样，我感到惊讶。
我们一开始讨论的是，你的动机是让transformer帮助我们摆脱流程，但我觉得规则又是重新使用流程。
所以我很惊讶你能谈论到这一点，以及你认为这将会发展成什么样子。
是的。
是的。
所以直到我们达到这样一个程度，就像你知道的，我们永远在下面，就像transformer永远在下面一样。
不，我认为那个工具只是让你...
所以这有点像你如何与一个能思考的机器交互，对吧？
是的，你必须构建某种接口，如果你构建了一个有用的功能，你希望机器能够接受你的功能并且用它做一般有用的事情。
对。
而且我认为使用工具只是利用人们已经构建的东西和软件的一种方式。
某些工具可能会被模型吸收。
对。
有些不会。
这样我们仍然有能力，就像，是的，这样我们仍然有能力，就像...
还有一些transformer本不应该做的事情。
抱歉。
我的意思是，你不想花费十亿次浮点运算来计算两个数字。
你不想花费更多的浮点运算来执行只需要十亿分之一的浮点运算量的操作。
对。
所以有些事情模型不应该做。
它应该使用外部工具。
还有一些模型应该进行的某种思考方式。
所以即使从能力的角度来看，有一个重要的问题是这个神经网络应该具备哪些能力。
对。
而且还能够利用其他人已经完成的工作，其他人构建的软件。
它更多地讨论了为什么最初的解码并行和集成优化的方法。
是的。
那么为什么这样做会有效呢？
是的。
有时候，如果你确切地知道为什么事情有效，也许你可以让它变得有效。
但实际上，你可以做一些傻事，比如随机排序，这意味着如果有人走到你面前并且你可以，我的意思是，你可以打破两种模式。
就像你可以说升序或降序。
那么我怎么说呢？
所以通常当你解码时，想象一下当你给出提示时，你有许多可能的完成方式。
每次做出选择时，你都会缩小那个空间。
每次另一个选择时，你都会缩小那个空间。
所以你学会了在某种意义上缩小所有可能路径集的空间，以一种模型不必决定你必须按什么顺序做的方式。
当你在进行这种不太或非顺序侵略性生成时，你必须做两者。
对。
同时学习两者是困难的。
我是说，但最终我认为如果对于一个特定的，我认为这可能是真的。
如果有一个神谕走到我面前并告诉我，这就是所有这些句子应该生成的顺序。
首先你应该生成这三个词。
然后你应该生成其他两个。
然后是其他两个。
如果有人走到我面前，把这个 Oracle 对所有人类语言的排序给了你，我觉得我们会有更大的机会，我们实际上可以得到这种不那么无序激进的固定工作的一代。
所以一个基本上就是排序本身。
我觉得这在一定程度上是必须的，因为排序可以帮助你锁定模式。
它决定了你接下来要生成的内容。
所以归根结底，我认为这归结为什么是正确的非无序激进的排序。
这可能是你仍然一次生成一个词，或者不那么无序激进，或者你一次生成几个词，然后基于此生成其他几个词。
所以你可以一次生成所有词的条件应该是彼此独立的。
对。
就像你到目前为止生成的应该已经完全解释了它们一样。
然后你之后生成的又应该是条件独立的。
那么你如何学习这些条件独立性呢？
是的。
如果有人走到我面前，把它们给了我，我想我们可能会学到它们。
是的。
克里斯蒂安。
越来越多地涉及到一般的元素。
最近我觉得我看了你们俩谈论语言元素，人们正在学习这些东西和所有关键元素。
我认为他更多的想法是仅仅扩大这些模型并不能帮助它们真正学习到实际世界是如何运作的。
难道我们对真相和现实世界的分析没有一个好主意吗？
是的。
你同意有人这么说吗？
所以，是的，我觉得很有意思。
你不能仅仅通过语言学习一个词模型，对吧？
所以，我的意思是，其中一些模型并不是每年都在被学习。
你正在进行RLHS，你得到了一些反馈，这意味着你正在应用一些，它们正在根据一些偏好进行修改。
所以它不仅仅是一个纯粹的语言模型，但这很有趣。
所以你看到一些工作，其中机器人现在可能开始蓬勃发展，因为他们能够使用这些大型模型作为规划者。
所以我认为令人惊讶的是，在这其中，世界有多少信息他们能够承载。
如果我理解正确的话，Seikan基本上是作为一个语言模型现在作为一个规划者使用的，对吗？
是的。
这就是标准认知和甚至解决机器人问题的经典方法。
所以，我是说，是的，扬可能仍然是对的，但它的用处在需要世界知识的地方是显而易见的，对吧？
所以我认为你可以用你现有的知识做很多事情。
我是说，可能吧，是的，我是说，我们还没有完全挖掘出这些模型的所有用处和我将要说的内容。
而且可能同时是正确的，但仍然有很多可以获得的东西。
所以我对这个问题很类似，你也在谈论新兴的事物，对吧？
我只是好奇知道你更多地对概括性和出现性的想法是什么。
我知道有一篇关于科学的BeatMind的论文，是的，我想，是的，他们真的不能超出他们接受训练的范围来概括。
特别是因为现在这些大型模型只是被训练在一切上，是否真的还有什么东西是超出分布范围的，你真的可以用来作为基准吗？
所以我曾经说过，如果我所有的测试都在我的训练中，我就会赚到十亿美元。
是的，所以我对此没有问题。
但我仍然这样认为。
好的，如果我说错了，请纠正我，但一般的论点是，这些模型已经学到了如此庞大的分布和现象集，通常当你询问它们时，它们往往会巧妙地融合或从它们学到的信息中获取信息，对吗？
可能是的，然后它们有这些算法任务，模型无法泛化，对吗？
所以我会专注于前者。
我认为这是一个非常有用的特性。
可能是这样，所以我认为也许感觉是我们实际上并不完全了解我们可以了解多少，甚至在文本中表示了多少。
其次，我们如果能够从不同的信息中融合信息，那么我们能走多远，比如能够用诸如乔叟的韵律和词语来写关于斯坦福大学的这堂课是不可能的，因为没有人这样做过，对吧？
但我认为你能做到，对吧？
现在，那是不是已经融合了你已经有的信息？
如果是的话，那就是一种你能够做到的令人难以置信的技能，对吧？
是的，我还没读过。
这是最近的事情，但我相信那项工作。
我认为你可以在这些模型中展示出来，但我认为有一些令人惊讶的新事物，你可以通过将已经学到的信息进行混合来实现。
是的，主要是因为有这么多的信息。
所以另一个问题。
之前我有两个问题。
我想我之前有一个顺序在脑海中，然后又想起来了。
抱歉，但给我一秒钟。
我在想你是否对连接不同的代理、transformer或其他东西有所了解。
神经元是一个很棒的transformer，本质上就像是大脑中神经元的一个很棒的连接方式。
而且它非常棒。
所以你找出了连接它们的最佳方式。
所以它不是指神经元，你所说的所有神经元都在大脑中起作用。
不，是transformer中的神经元，对吧？
就像transformer是连接不同部分的方式。
然后当你将它们连接在一起时，它就会起作用。
我在想你是否对能够最佳地一起执行的建立系统有一些见解？
是的。
这不会像这样。
我喜欢开这个玩笑，但实际上最优秀的代理是神经元，因为它们可以彼此通信。
它们可以非常、非常好地更新自己。
但其他代理在做什么呢。
试图理解试图使一堆系统一起工作的基本问题是什么是什么制约。
这就是你在问的。
目标分解是什么？
是的。
一个是第二个大的，协调是第三个，验证是第三个。
就像如果你解决了根据你对这些代理的规模的估计的目标的成功分解，如果你能找到他们做了什么，如果你能够协调，那么我认为你可以取得很大的进展。
对。
所以我没有回答你的问题。
我不知道在这三个领域你们取得了多少进展。
但是有人在这里有什么想法吗？
它有一个并行轨道。
而且你有一些甚至不是并行轨道的东西。
而且你想验证每个轨道实际上是否在工作。
这一切就像一个处理器。
有很多角色信息，你想确保你知道你要使用哪个角色，并验证一切，然后把它做得更大。
你需要看看这几乎就像是计算机架构。
就像是微观跟踪一切并随时间做出决定。
而且这都有点像，你会问自己你喜欢上一次的情况吗？
但它仍然像写作一样。
没有人知道如何完成任务。
是的，对。
但是，就像，问题似乎可能在某种程度上是必要的。
上网查查。
这是一个非常有趣的问题。
那么，我可以问一个问题吗？
所以，人脑非常模块化。
那么，这种模块化就像是一种紧急现象。
你需要一些特殊的观点来理解这一点。
是的，这里的模块化，你的意思是，它是指这个，这个区域有这个责任吗？
甚至构成不同，结构不同吗？
你的意思是什么？
因为你可以两者都有，你可以争辩说没有，责任在模型中分散了。  
当只是专家的成本时，它试图朝相反的方向走，这是我可能应该提到的。
这是另一个非常令人兴奋的方向，肯定已经发生在人们身上，并将会持续下去。
我完全错过了。
那种试图获得专业化的方式，对吧？
所以，也许那是某种模块化，对吧？
学习模块化。
执行任务的其余责任可能是分布式的。
但是如果现在你将这些子系统本身具有不同的组成部分，那么你就会回到，我知道这是谷歌的路径项目的一个目标，你希望这些非常模块化的系统彼此通信。
我认为这只是需要花费很长时间来创造性地思考。
事实上，有时候我认为刚性的建筑结构在这个意义上是如此出色。
我觉得如果你能够通过伟大的下降来学习，那是非常有用的。
也许实际上可以使这些模块化系统起作用。
我们已经通过专家拥有了一些，我想象了我们之前讨论过的一些问题。
这有意义吗？
抱歉，回到现在，无论怎样，七个问题之前，你提到同时解码的问题是解码通常假设输出可能是独立的，但它们不是吗？
从这个意义上说，如果你有一个潜在空间，如果你把潜在空间作为你的先验，那么你的后验输出应该是条件独立的。
所以，很好的观点。
那么你从哪里得到潜在空间呢？
嗯，从编码器或者开始的任何地方。
对，但可能有很多种方法来翻译某件事，对吧？
是的，有很多。
所以如果只有一种模式，那么是的，是来自你，对吧？
但如果有多种方法，嗯，实际上有两件事。
潜在空间实际上承载了多少？
这是一个重要的问题，对吧？
它实际上承载了多少？
因为它不仅仅是一个潜在向量，而且你每次都在传输，你一遍又一遍地进行注意力。
但我们采取了这种方法，我们精确地做到了这一点。
我们使用向量量化自回归地生成了一个新词汇中的标记。
所以条件依赖性是在一个潜在的空间中建模的，使用向量量化进行了离散化处理。
然后基于此，我们以条件独立的方式生成了所有内容。
这确实起了作用。
但是，这在翻译中确实起了作用。
问题在于，那里有一些奇怪的问题，潜在的潜在向量序列只在你可以直接在原始数据上学习时才是有效的。
你必须做一些像蒸馏这样的事情，因为蒸馏本身可能会丢弃一些模式。
所以通常情况下，熵越低的数据，我们就必须在其上进行训练。
第二个问题是对于实际系统，你必须使整个过程真的非常快。
这是一个很好的研究练习，但最终它没有正确的实际细节。
因为实际上解码实际上使用我们现在拥有的东西时效果不好。
齐心协力，我认为。
是的，确实如此。
是的。
但你是对的。
我认为如果你能够生成足够好的延迟，那么是的，你是对的。
我们可以假设那会使一切都是有条件独立的。
我们设法做到了一点，但并不是很好。
我想这是最后一个问题。
哦，哇。
那太私人了。
非常私人。
我在那里有朋友，他们都非常棒。
他们正在做一些可怕的事情。
我认为我们会惊讶地发现有很多事情要做。
如果是这样，首先是动力，对吧？
有一个全新的，有一个全新的一桶，或者说是一种中子转变的能力，你将在人际互动中获得。
你可以制造一个产品，人们使用它，他们给你反馈，模型变得更加智能。
这种封闭的循环系统确实可以带来，确实可以推动模型的进展。
然后带来价值，对吧？
这是第一点。
第二，我认为有些深度学习非常受益于各种各样的想法和人们的产出，人们，追求重要方向的人们。
我对建立公司产品以及用这些模型构建新型产品的公司也是同样的看法。
所以我想说我们有很多可以做一些令人难以置信的事情的领域。
那是第二个部分。
第三，你知道，是的，或许那就是，那就是我可能错了的更个人化的方向。
谢谢。
