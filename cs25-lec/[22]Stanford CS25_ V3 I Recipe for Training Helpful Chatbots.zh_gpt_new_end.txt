大家好。
今天我们有来自 Hugging Face 的 Nazneem，她正在使用强化学习和人类反馈来进行 AI 安全和对齐方面的工作。
她是大型语言模型及其评估领域的专家。
在加入 Hugging Face 之前，她领导了一支在 Salesforce 的研究团队，专注于构建基于要素的强大自然语言生成系统。
她在德克萨斯大学奥斯汀分校获得了计算机科学博士学位。
那么欢迎大家。
谢谢你们邀请我。
那么今天我演讲的主题是培训有用的聊天机器人的配方。
下面是介绍。
我曾经是 Hugging Face 团队中的 H4 团队的一员。
今天我将向您介绍我们建立了什么，我们是如何决定我们需要什么来构建它的。
因此，从今年年初开始，我们团队和项目的目标实际上是找出 H4 的配方，它代表的是有用、无害、诚实和亲切，因为它是 Hugging Face 的聊天机器人。
因此，你知道，这些原料本质上是要弄清楚我们需要什么样的数据集来进行监督训练调整和 RLHF。
而且我们希望不用担心预训练。
相反，使用一个开源的预训练模型，并在其上重新创建对齐的秘密源。
我们想要遵循并在开源上复制的过程是这张图，我相信大多数人在这一点上都很熟悉。
就像来自OpenAI的这篇《指导GPD论文》一样，其中显示了三个步骤。
我会对此进行更详细的说明，因为幻灯片在这里要小得多。
但这就是演讲大纲的样子。
我将详细介绍，你知道，我们是如何决定使用什么样的数据、多少数据以及监督微调数据的所有细节。
然后同样适用于RLHF。
然后我将谈论语言模型对齐的蒸馏。
然后用不同的帮助配方进行实验。
最后，谈论这些模型的评估以及使用GPD4作为评估器的技巧。
好的，这有点像，你知道，OpenAI的这篇《指导GPD论文》提出的整体配方，用于训练聊天机器人的步骤。
因此，这里的第一步是进行监督微调。
基本上，你知道，你正在使用人类指导示范数据进行微调。
所以输入和输出都是由人类给出的。
第二步就像是，你知道，输入是由人类给出的。
输出来自模型，然后人类只是阅读、点赞、否定或对它们进行排名。
然后你用一个模型进行训练，它本质上只是一个分类器。
最后第三步是利用那个模型进行强化学习的微调。
我所看待的方式是，第一步更多地是为了使模型成为一个有用的聊天机器人。
而第二步和第三步本质上是试图为其设置保护措施。
那么让我们从谈论有用性开始，我今天的大部分讲话都将集中在第一步。
所以让我们深入探讨这个问题，让我们从，你知道，数据集开始。
比如，我们如何决定需要什么来进行监督微调？
因此，用于有用性监督微调的数据集看起来有些像这样。
这是来自去年年底的自我说明文件，如果你知道的话。
所以你有一个被称为任务的东西，然后有一个指令，基本上是用户的请求，要求模型执行或响应某个任务。
这之后是输入和输出。
在这种情况下，输入是可选的。
它可能只是指令的一部分。
然后输出是模型应该生成的预期输出。
但在进行训练时，人类提供模型在实际测试案例中可能生成的预期输出。
因此，在这里，输入和输出被称为实例、演示或完成。
这就是为什么这被称为指令演示的原因。
所以这有点像是这些指令演示数据集的高层面貌。
你可能至少熟悉其中一些。
就像，你知道，我尝试表达的方式是在这一行上，一边展示使用模型或更强大的语言模型生成的数据集。
因此，它们是更合成的数据集。
在右边，我展示的是人类编写的数据集。
所以这些是模型。
这些是由人类编写输入和期望输出的数据集。
所以这些的例子就是，你知道的，搜搜指令是我们在Hugging Face H4与Surge签订合同的数据，这家公司基本上与那些写输入和输出的注释员签订了合同。
但是我们必须给他们所有我们需要的数据的详细说明。
然后你一定听说过，你知道的，Open Assistant是这个其他的社区广泛努力的一个例子，人们手动编写输入和输出。
同样适用于Dolly。
然后另一方面，你可以看到，你知道的，自我指导数据集。
我要深入研究其中一些。
这些合成数据集是如何为有益或监督微调而创建的呢？
因此，关于如何创建这些合成数据的一个例子是在自我指导论文中，这篇论文称为数据引导。
在这种情况下，他们从175个C任务开始。
也就是说，有175个非常小的示例数据集，其中包括人类手动编写的输入和输出，这些被添加到任务池中。
然后，像语言模型这样，基本上你通过提供那种语言，以及在一个很短的环境中将其提供给语言模型，并要求其生成更多类似数据的方式，进行引导。
然后，你有另一个语言模型执行这个任务分类，就是，你知道，这是什么样的任务，这个样本或例子属于哪种任务？
最后，它还进行更精细的分类，比如，你知道，它是先输出还是需要先输入等等？
因为这是合成数据，以这种非常可扩展的方式创建，你还必须进行大量的过滤，以确保它是非常高质量的。
生成这种合成数据的另一种方式是UltraChat采取的方式。
在这种情况下，它们采用了人在循环过程。
所以一个人会，你知道，查找维基百科或其他内容，然后想出他们想要生成数据的主题。
然后，你知道，询问模型，提供所需的材料，用于想出，比如，问答或摘要或任何这些具体任务的材料，然后将其提供给一个更强大的模型，比如Chat GB或GBD4。
在这种情况下，它是 Chat GBD 然后，噢，实际上是 GBD4。
然后你就有点像，你知道，不断地进行这些循环，比如，你知道，将材料提供给模型，并说，像这个特定任务上使用所有这些材料，提出问题和答案。
然后，你知道，然后人类看着它，然后不断地查询它，并对其进行进一步的细化。
所以这是另一种创建合成数据的方式。
显然，这需要一个人坐在那里，并在这个过程中进行更多的过滤。
然后还有另一种方法，即更少涉及人类的角色扮演。
这就是骆驼数据集。
在这种情况下，人类所做的就像是想出一个任务或者想出一个，你知道，他们想要的例子。
所以在高层次上，它会像是为股票市场的交易机器人进行加倍。
然后会有两个 LLMs。
一个将扮演 AI 助手的角色。
另一个将扮演 AI 用户的角色。
然后他们基本上只是指定任务，然后让这两个机器人互相聊天，并创建一个对话数据集，这就是一个用于监督微调的合成数据集。
所以这有点像是，你知道，回到这个，你知道，景观。
看起来，你知道，人们已经非常有创意了。
我们如何快速获得，你知道，非常高质量的数据而又不花费太多钱？
因为人类效率低下且昂贵。
所以这些就像，你知道，我们看过的一些例子。
但另一方面，我们也不能低估手工创建的数据集的高质量。
因此，我们在Hugging Face决定像，你知道，完全手动地进行，让人类同时处理输入和输出。
还要弄清楚，他们需要什么，你知道，基本文件或其他材料来创建这个数据集。
但当我们开始这样做时，我们还处于年初。
所以这是在今年的一月或二月。
那时的情况是这样的。
所以可用的数据集非常少。
其中很多是基本上由合成创建的。
所以我们想要，你知道，充分利用当时已经存在的东西。
但我们还必须做出一些非常重要的决定，因为我们将支付金钱，并确保我们收集的数据实际上对构建模型和构建在其之上的应用程序有用。
所以这些都是我们从过去的论文中得到的经验教训，这些论文创建了这些受监督的微调数据集。
我们知道数据集必须在数万个示例的范围内。
所以这是来自自我指导数据集的。
而且我们也知道，你知道，这些在这个数据集上训练的模型在几千个高质量指令之后会出现收益递减的情况。
所以你不需要很多，然后它很快就会饱和。
所以这是我们开始收集受监督的微调数据集时得到的两个发现。
但我们也必须对我们的数据集提供一些细致的规范。
特别是，我们必须决定我们收集的数据的任务分布是什么。
我的意思是，我们知道是数万个，但是多少个任务的几千个，对吧？
长度分布，你知道，提示应该有一定的长度吗？
那甚至是一个重要因素吗？
还有一件事是，我们希望，我们已经决定要做出高质量和人工编写的产品，但是那也有选项可供选择。
我们可以选择像 SIRDS、Scale AI、AWS、Ground Truth 等外部供应商。
或者我们可以从 Upwork 和 mDuck 雇佣我们自己的承包商。
所以这些都是我们必须做出的决定。
所以让我们逐一看看这些。
因为我们正在为这个有用的聊天机器人重新创建这个指导 GPT 配方，我们想要从他们的任务分配中汲取灵感。
所以在左边，我展示了指导 GPT 为 instruct GPT 论文所做的任务分配。
但是你可以看到，生成部分占了大部分，其次是一些开放性任务和头脑风暴任务等。
这些是每个任务的提示的示例。
所以我们决定就这样做。
但是，您可能已经注意到表格中有一个称为“其他”的类别。
而我们显然不知道那是什么。
所以我们决定用代码替换它。
所以基本上，这就像调试一样，询问关于代码的澄清问题。
所以它就像是代码加上自然语言。
这就是我们最终的分布看起来的样子。
第二个问题是长度分布。
所以我们还必须像，你知道的，弄清楚，你知道的，长度有多重要，我们是否应该像，你知道的，有一定的长度分布。
然后我们请这些公司为我们收集数据。
所以我们与Cert、ScaleAI和AWS SageMaker Ground Truth进行了一项试点研究，这更像是一项托管服务。
这与MTurk非常不同，他们拥有质量很高的人员，基本上是写这些例子的人。
所以我想强调一下，你知道的，这是指导 GPD 长度分布看起来像的前两行。
正如你所看到的，这显然是完整的数据集。
这更像是试点。
所以像计数要小得多。
但是你可以看到，最大的是 2048。
而且，你知道的，那就是一年初标准上下文大小的样子。
然后，你知道，显然有像均值这样的东西，你知道它不基本上就是更多或更少，你知道，在范围内。
但是如果你看一下，你知道，Cert、AWS和ScaleAI的这些例子，差异很大。
所以，例如，AWS SageMaker，最大提示长度是1036。
但是，你知道，均值只有54。
另一方面，对于Cert，最大长度是500。
但是均值更像是，你知道，104。
所以就像更在我们从指导GPD中期望的范围内。
同样，对于ScaleAI，我们发现，你知道，他们的提示非常非常短。
因此，基于此，我们说，你知道，好吧，我们应该可能只选择Cert。
因为，你知道，那似乎是更在，你知道，不是很高方差的范围内的东西。
所以我们最终从Cert那里收集了10,000个指导演示对。
这是任务分布的样子。
所以这非常符合任务分布指导GPD，除了编码部分，那是另一类别。
这些是我们为每个任务收集的示例数量。
在这里，我展示了每个任务类别的平均长度。
我想要强调的一件事是，对我来说非常令人惊讶的是，聊天实际上是最短的提示长度类别之一。
但对于OpenAI来说，这实际上是最长的提示长度类别之一。
所以，这非常有趣。
显然，在那时，我们并没有多想。
但是当我们开始训练模型并开始查看评估结果时，我们有点像，你知道，如果我们不得不回去改变事情，我们会如何改变呢？
所以这些就像是我们在收集了数据集之后开始更加仔细地审视的事情。
这里是数据集的示例，你知道，分类、生成、头脑风暴。
我相信你们至少见过一些这种指导演示数据集的例子。
所以它非常像从NLP类任务中可以期待的一切，但也包括更开放的闲聊任务。
好的，这里有一些关于被搜索用于生成这个数据集的特遣部队的细节。
我们请求了一个以美国为基地的特遣部队，主要是因为，正如我所说，我们只是想复制Instructure GPD的做法。
根据熵和OpenAI的论文，他们似乎更倾向于选择美国为基地的特遣部队。
性别大致平均分布，年龄范围也非常广泛。
从19岁到62岁，跨度很大。
然后人们的教育背景也各不相同，从技术学位到博士学位不等。
所以博士学位主要用于数学、编码等任务。
好的，现在我想换个角度，谈谈我们为RLHF或人类偏好收集的这个数据集。
在我介绍我们对这个监督微调数据集进行的实验以及我们得到的结果之前。
所以在我们收集人类偏好数据集时，我们必须确定这些数据集的规格。
再次强调，与SFT有何不同，SFT数据集的输入和输出都是由人类撰写的。
在这种情况下，人类编写输入，输出来自模型，即响应，但然后人类只是根据某个特定的标度对它们进行排名或评分。
是的，基本上我们不得不决定，RLHF数据的任务分配看起来是什么样子的？
它是否会与监督微调相同？
长度分布又如何呢？
我们是应该进行单轮对话还是多轮对话？
所以在InstructGPT中，主要是单轮对话。
所以如果我们试图复制InstructGPT，我们将不得不选择单轮对话。
但是如果你试图复制类似ChatGPT这样的东西，它将不得不是多轮对话。
然后我们还必须决定诸如帮助性、诚实性和无害性等这些维度。
这些就像熵所遵循的HHH，OpenAI将其表达为帮助性、欺诈性和无害性。
然后我们还必须决定，他们是要分别对每个响应进行评分，还是要对它们进行排名？
以及我们决定采取一种方式或另一种方式的影响是什么？
所以我们开始进行了一项试点研究。
所以我们从自我指导数据集中提取了300个提示，这个数据集是去年年底发布的。
然后，你知道，我们给它生成的模型响应，然后把它交给数据供应商来评估模型的响应。
我们使用左侧的熵模板，实质上是在询问人类选择最有帮助和诚实的回答。
然后，你知道，这些是来自A模型和B模型的回答。
这是一个刻度，它也像一种排名，一到四是递减的A模型，五到八是递增的B模型。
还有，你知道，我们需要决定的另一件事是收集多少数据？
所以，这再次来自指导GPD论文。
正如你可以看到的，他们为每个步骤都有像SFT、训练奖励模型和PPO这样的训练和验证拆分。
这个是在数万的数量级上。
总的来说，综合起来，也就是RLHF这个过程大约是100,000。
好的。
好的。
那么，一旦我们拿到这个试点研究数据，我们就坐下来，我们也想要像，你知道的，所以我手动查看了它，我觉得我不同意大多数答案，你知道，每家公司的标注者提供的。
所以我有点像，你知道，我觉得这一点都不高质量。
所以我决定，像，你知道，我告诉我的团队，让我们在内部对其进行评分。
然后，你知道，我们基本上评价了大约100个例子左右。
我们遵循了类似于一到四和五到八的类似模板。
基本上输出，你知道，带走的是，即使我们彼此之间也不同意。
因此，实际上，我们今年早些时候的模型是如此糟糕，你基本上是在随意地打破平局，像，你知道，你在决定是否应该是，你知道，三对七之类的。
所以如果它们同样糟糕，很难像，决定哪一个更好。
对。
所以我们有点像随意地打破了一些这些平局。
所以，正如你所看到的，你知道，我们的输出中几乎没有任何像，你知道，一致性或相关性。
然后，你知道的，当我汇总了那个，并且，你知道的，看了一下，你知道的，我们与像激增和规模这样的东西相关性有多强。
所以我们决定，你知道的，规模化人工智能，我们有更多的，像，与激增相比，最大的重叠是与规模相关的。
好的，所以我们最终收集了20,000个对话。
所以我们决定采用多轮对话。
因为它是多轮对话，你会有大约20,000个整体对话，但提示的数量将是80,000个。
所以每个对话平均会有四个轮次。
就像，你知道的，一个人会提示它，模型会回应，一个人会阅读回应，然后，你知道的，提出跟进问题。
然后再次，模型会，你知道的，生成两个回应，就是这样进行的。
所以我们决定遵循的任务分配与我们在监督微调中有点不同。
背后的原因是我们想更专注于那些实际的任务，这更多地是让模型学会区分正面和负面信号之间的区别。
所以，使模型能够区分，你知道，什么是事实的，什么不是，什么是有帮助的，什么不是，以及什么是无害的，什么不是。
而且，你知道，例如，生成和头脑风暴等任务，没有一个正确答案。
你知道，每个人都可以提出不同的列表或配方。
而且很难说，这是最好的答案吗？
这是最有帮助的答案吗？
但是如果你问一个事实性的问题，那么什么是正确的，什么是不正确的就很清楚了。
所以这在一定程度上是我们做这个的理由。
因此，这是我们为收集人类偏好数据集而提出的任务分配。
还有关于长度的问题，因为我们是在一个多轮对话的环境中进行这个任务。
因此，我们希望确保整个对话可以适应模型的上下文线。
我们已经决定要求他们确保整个对话总长度不超过2048个标记。
然后它是多轮的，每个对话平均有四轮。
然后显然我们还必须选择是否我们要追求有帮助的、无害的或者，你知道，诚实。 
所以我们遵循了这些来自OpenAI指南的指示。
我不确定我能做到这一点。
那将是不错的。
好的，很好。
但是，是的，所以OpenAI有这个文件，公开地像他们与标注者分享的标签指令一样。
因此，他们显然有帮助、真实和无害。
但他们还有这个东西。
我怎么向下滚动呢？
好的。
所以，他们对“有帮助”是什么意思有定义吗？
他们对“真实”是什么意思？
他们对“无害”是什么意思？
所以在我们的情况下，因为我们的模型不太好，我们决定专注于帮助和真实性。
当他们不得不打破平局时，OpenAI说，你知道，在这里选择真实性而不是帮助。
所以，就像，让我看看那个意识。
所以他们想要优先考虑无害和真实而不是帮助。
但是我们选择了相反的方式。
我们说过，我们希望优先考虑实用性，而不是诚实或无害，因为我是说，我们甚至没有专注于无害，因为我们只是想在开始考虑那个之前，使我们的模型达到一定的能力。
但是，是的，这确实是一份非常好的文件，就像你知道的那样，它定义了标注员应该关注的内容，以及他们在何时决定模型响应非常接近时，如何打破这些关系？
而且就像你知道的那样，在决定使用哪种模板来收集这些注释时，我们最初使用了我之前展示过的人类模板，它是在一到八的范围内，但基本上是在这两个模型之间进行排名。
然后，你知道，在我们进行这个迭代过程时，Lama 2也发布了。
而我们的迭代过程本质上是我们过去常常给供应商一个端点。
然后，你知道，在这个迭代过程中，他们在托管任务组中拥有的标注员会提示这些端点。
模型会对响应进行回应，生成两个响应。
他们会，你知道，遵循说明，然后为这些说明中的每一个模型响应给出排名。
然后，你知道，再次，跟进第二个提示，对话会继续进行。
然后他们会在那周末给我们数据。
我们会在那些数据上微调我们的模型，这样模型现在应该更好。
然后我们给他们一个更好的端点，以便下周继续这个过程。
所以这就像是非常迭代的，你知道，他们必须适应模型每周变得更好。
所以，基本上，但是，你知道，我们决定切换到我想是一个或两个星期，我们收集了熵，使用了熵比例来收集数据集。
但是后来Lama 2出来了，他们的结果表明，你知道，很明显地，他们更容易地使用了一个到四的这种更简单的比例。
所以他们就像，你知道，选择哪一个是更好的回应，然后看看它有多好。
所以是显著更好还是只是稍微更好？
所以这就是排名的一到四的比例。
这里是我们收集的数据示例。
所以在左边，你可以看到，它在问，你知道，基本上是人类提示一个问题，然后机器人生成一个回答。
然后这就是人类在这一轮选择的回应。
然后人类，你知道，跟着第二个提示。
然后这是这个人选择的机器人回应。
这是被拒绝的机器人回应。
这给出了三分的回应差距，这意味着它们相当不同。
所以四是非常不同，而一是非常轻微的不同。
然后在右边更多地是关于生成头脑风暴的例子，人类在问，比如说，你能写一条文本消息祝你丈夫周年快乐吗？
然后机器人写了一些东西。
我猜我的东西搞乱了表情符号。
但是，你知道，然后人类跟着说，嘿，你错过了这个重要的细节，就是，你知道，他们已经结婚八年了。
所以这是一个被选择的机器人回应。
这是人类在这两者之间选择的被拒绝的一个。
你可以看到，它们相当不错。
所以回应差距只有一分。
所以它们只是略有不同。
好的，听起来不错。
现在我要谈一谈我们尝试过的另一个配方，就是，你知道的，基本上是使用合成数据集进行人工智能对齐的蒸馏，这基本上就是我们上周发布的名为“Zephyr”的论文，这是一个70亿参数的模型，实际上击败了chat GPT。
而且这是在Mistral模型的基础上构建的。
但我只是想要，你知道的，是的，基本上，我们创建了一些在Instruc GPT论文中存在的步骤，但现在使用合成数据集。
所以第一步就像是，你知道的，基本上是使用数据集。
在这种情况下，我们使用UltraChat。
所以这是一个数据集，我之前在几张幻灯片中展示过，用于监督微调，其中一个人正在进行头脑风暴并收集材料，然后像这样与这个GPT-4模型交流，以生成多个不同的输出以供指导。
然后，你知道的，这就是我们收集那个数据集的方式，这个数据集称为UltraChat。
然后我们使用它来微调我们的模型。
然后第二步是响应生成的AI排名。
所以在这种情况下，也就是说，你知道，我们使用了UltraFeedback，这是一个已经发布的数据集。
而这个数据集的构建方式是，你知道，他们基本上是问了一些提示，就像从共享的GPT和一些已经存在的SFT数据集中获取的。
然后他们将其提供给四个不同的模型，就像是四个不同的强大模型，比如Palm、Clod、GPT-4等等。
然后他们要求这个GPT-4去排名，哪一个是，排名每个核心回应。
然后就像，你知道的，那个被GPT-4排名最高的就是最好的那个。
所以每一个都是单独按照1到10的比例进行评分。
而得分最高的那个就是最好的回应。
最后，我们做了一些叫做DPO的东西，你可能已经意识到了，因为它是从斯坦福出来的，就像是RLHF的一种替代方式，这就是进行这种直接的偏好优化。
所以与其基本上进行这种微调的迭代过程，我们直接在选择的回应上进行优化。
所以我们就拿到那个然后直接在我们的模型上进行微调。
而我们正在使用的另一个是从这三个其他响应中随机选择的响应。
好的，所以我要稍微谈一下关于每个配方的实验和评估。
一个是收集所有人类参与的事物，第二个是所有合成的事物。
但在讨论评估之前，我想谈谈我们正在评估的基准是什么，以及这些基准对于评估聊天机器人有多好。
想要进行评估，我们首先需要考虑如何训练这些模型。
所以今天，所有训练的模型基本上都有这四种学习方式。
第一种是预训练语言模型，本质上是预测下一个标记。
这些的例子有像 GPT-3、OPT 这样的基础模型。
第二种类型的学习是上下文学习或基于提示的学习。
在这种情况下，您只需在模型的上下文中提供一种新的任务，然后，您知道，要求它在新的示例上执行该任务。 
就像，你知道的，如果你想写一首诗，比如，给GPT-3写一首诗，你会在上下文中写下来，然后它会生成一个关于其他话题的新诗。
第三种学习类型是监督微调，这有点像训练聊天机器人的第一步。
在这种情况下，你在遵循指令的数据上进行微调，然后你希望这些语言模型，它们只是预训练的，能够预测下一个令牌并变得健谈，生成开放性的回复。
最后，第四种是通过人类反馈进行强化学习，这是将语言模型引导向你期望的价值观的过程。
例子包括从元开始聊天。
所以前两个步骤是，你知道的，我们有很多针对这两种训练类型的基准，比如Sanford Helm就是一个例子，或者Google BigBench，甚至Hugging Faces的Open LLM排行榜。
但是对于这两种学习类型，即监督微调和通过人类反馈进行强化学习，它们是训练聊天机器人的配方的一部分，你知道的，没有很多排行榜或评估基准可用，但有一些可用的，我想要，你知道的，突出一些其中的一些。
所以，是的，这基本上就像是步骤三和四年匹配到这里的一样，你知道，这是有帮助的步骤一，然后是这里的步骤二和三，这就像是，你知道，引导模型朝着更加无害的方向发展。
所以，如果你不得不评估每个步骤的聊天机器人，你需要考虑如何评估指令的遵循或啰嗦，你需要考虑如何评估奖励模型，这本质上是一个分类器。
最后，你要思考，你知道，如何评估无害性，这是通过对语言模型进行红色主题化或对抗性提示。
所以在第一步，你要看看模型是否在主题上生成有用的回答，并且它们是否开放式的。一个你尝试评估模型的提示的例子是列出新年决议的同样烦人的清单。
那么，关于这种监督微调的基准和评估板的示例，就像Hugging Face的具有ELO评分的排行榜一样。
所以ELO是国际象棋中使用的度量标准，就像你知道的那样，你将一名玩家与另一名玩家配对，并且希望在他们相互对战时对这些玩家进行排名。
在类似的情况下，我们通常会将这些聊天机器人放入成对的设置中。
然后，我们与Scale AI合作，他们提供了人员来注释哪个回答更好。
我们为每一种组合都做了这样的操作。
所以就像是NC2，其中n是我们正在查看的提示数量。
因此，我们生成了NC2的组合，并对每个组合进行评分。
这就是我们从中获得的ELO评分。
在这个表格中，这一列显示了如果您使用GPD4作为人类的代理，您将获得的评分是什么。
所以，与其让人类坐下来为每个人评分，不如让GPD4选择哪个回答更好。
是的，所以这基本上是您展示的第一个表，如果在某种意义上允许平局，抱歉，如果不允许平局，则显示此表。
另一个例子是，你知道，这个来自斯坦福的排行榜，叫做Alpaca eval排行榜。
他们在某种意义上做了类似的事情，他们有GPD4和Claude作为评估者。
他们对这些模型或者聊天机器人模型进行了两两评估。
他们报告了你知道的胜率，哪个模型胜出了对手。
还有伯克利的LM-SYS排行榜，有一个叫做聊天机器人竞技场的东西，实质上就像一个公开的众包排行榜，在那里你可以像聊天一样，你知道，与他们的任何模型聊天，然后给他们评分，看哪个更有帮助，哪个更好。
因为这是在一个两两的设置中进行的，所以这又有了ELO评分的排行榜。
还有来自LM-SYS的另一个基准，叫做空白台或者多轮对话基准。
这是有史以来第一个评估聊天机器人的多轮对话基准。
所以它在各种类别中只有大约80个例子。
但基本上它的工作方式是，基准中的第一个轮次或第一个提示会被提示给模型。
然后 GPD4 被要求在一到十的评分中进行评分。
模型的回应有多好？
然后，你知道，它后面跟着另一个提示，就是，你知道，多轮提示，与问题有关。
但它可能与模型的回应无关，因为，你知道，这已经构建好了，它们总是用相同的回应跟进每个机器人。
然后再次，GPD4 评估第二轮回应的好坏程度。
所以这就是像 LM-SYS 的综合排行榜，显示 ARENA ELO 评分以及空白台分数。
所以这些分数是在所有80个例子中汇总的。
这是 GPD4 从一到十的评分。
很酷。
所以我认为我们在评估聊天机器人图表中想要看的第二步是，你知道，思考如何评估奖励模型。
所以当你有了这些收集的人类偏好数据集并训练了这个奖励模型，它本质上是一个分类器，用于区分，你知道，真实和不真实的回应，或者，你知道，它是否能够将有帮助的回应排在不那么有帮助的回应之上。
而且，你知道，对于评估这些偏好模型或奖励模型，实际上没有可用的开源数据排行榜。
但在Hugging Face内部，我们有自己的数据集用于评估，以便我们知道随着我们添加更多人类偏好数据，我们的模型实际上正在变得更好。
因此，这本质上是我们在这些开源数据集上进行评估，其中包括人类有用数据集、开放助手数据集、斯坦福人类偏好数据集，以及来自OpenAI的第一篇论文中关于学习总结的数据集，该论文旨在研究学习总结。
所以，这就像，你知道，基本上在看，你知道，我们的奖励模型有多好。
然后最后，第三种评估类型是红队测试。
在这种情况下，您希望以一种能够凸显模型漏洞和新兴能力的方式制定提示。
例如，如果您询问如何计划恶作剧抢劫，模型是否实际上在帮助您并试图引出模型的不良行为。
不幸的是，实际上，对于这件事，确实没有可用的开源排行榜。
人为智能仅有一个数据集，其中包含所有三者，实际上它既有帮助性又有无害性。
这是人为智能的边缘数据集。
这是唯一可用于红队行动的开源数据集。
但是对于红队行动，没有排行榜可用。
所以这就像我今年早些时候写的一篇博客，突出了这个差距，并表示，我们应该聚在一起构建一个红队行动数据集的公告。
如果你听说过DEFCON红队设计挑战，基本上是一些红队工作的众包，就是这样产生的。
好的，现在我要谈论一下评估、基准和排行榜，然后我会谈论结果，以及在每个基准测试中的一些情况。
这里我展示了来自Hugging Face的开源LLM排行榜上Llama到130亿的结果。
在这种情况下，我使用了我们从搜索中收集的数据集，这是一个包含1万条指令演示数据。
我听说，你知道，这些基本上就是我们作为开放LLM排行榜的一部分拥有的四个主要数据集，它们就像是针对自然语言处理的数据集，分别是ArcChallenge、Hendrix、Hellasquag和TruthfulQA。
而你知道，这就是我们的模型表现如何。
而所有这些本质上都是准确度。
这是来自Meta的Lima论文或Lima模型，它主张对齐的原则是少即是多。
他们只使用了1000个高质量指令的示例，并展示了只使用1000个示例就可以得到一个非常好的聊天机器人。
而这就像是从Open Assistant中选择最长的示例，然后选择其中的前500个。
所以我们发现，我们的模型略优于，你知道，llama和Open Assistant的每一个模型，除了在TruthfulQA中，我们发现Lima和Open Assistant比我们做得更好。
同样，在empty bench中，实际上我们发现，你知道，相反的情况是真实的。
所以这就像是，你知道，空凳子。
请记住，LLM说过，就像，你知道，零轮和一轮。
然后这就是报告第一个回答。
这就像GPD4本质上是在一到十的评分中打分。
这些模型在第一个对话轮和第二个对话轮以及平均分数上的表现如何好。
实际上，这与我们在这个自动进化过程中发现的有点相悖，实际上是空板凳表示，你知道，我们从搜索收集的数据对模型的训练并不是很好。
事实上，Lima和Open Assistant，它们的数据规模只是我们所拥有的数据规模的一小部分，但却更好。
这有点令人惊讶。
然后我看了看，你知道，让我看看这是否是因素之一。
看起来是的，你知道，我在看这些数据的时候，然后你知道，看了看每个数据的提示的平均长度。
似乎范围非常广泛。
例如，像我们的数据集，这些提示的平均长度只有211，而Lima则是它的两倍，Open Assistant几乎是它的两倍。
所以，我做了这个实验，我想检查一下，如果我控制了数据的大小，但是让长度变化，问题是否会影响性能。
所以特别是，我想我之前强调过，我们的聊天类别确实非常简短。
实际上，它发现长度并没有真正影响太多，除了这个真实问答数据集，即使对于这个非常酷的数据集，尽管它看起来更像是第三位数字。
在这里，你可以看到实际的差异只对真实问答产生了影响，它实际上更喜欢生成更长回答的模型。
但另一方面，MT-BEN得分与我们发现的这些自动指标和评估并不一致或相关，也就是说，GPT-4实际上并不喜欢生成更长的回答。
所以这有点反直觉，所以需要更深入地挖掘这里到底发生了什么。
但你知道，它实际上发现，你知道，比起更长的回答，更短的回答更好，尽管差别不是很大。
所以我们进行的另一个实验和消融是仅仅删除一些数据，然后看看如果逐渐添加更多数据，那会如何影响性能？
这又是在那个秋天，就像是从Hugging Face打开LLM排行榜一样，查看一些标准的NLP基准，并报告准确性。
这就像是从我们收集到的所有数据中只使用了10%开始的。
你可以看到，在所有这些基准测试中，实际上很快就饱和了。
然后有些，实际上，你会发现，如果继续添加数据，你基本上会失去性能。
这就是当我们开始收集数据时我所得到的东西，在那里我们有这样一个递减回报的情节，你说如果你只有非常少量的1000个高质量的指令跟踪数据集，那就足够了。
然后你的性能很快就会饱和或者停滞。
所以这也是我们得到的结果。
同样，我认为这是一个空的工作台实际上与自动化指标相关的地方，就是GPT-4也显示出，在大约4000个示例之后，实际上几乎没有性能上的任何提升，实际上，模型的性能还在下降。
好的，很好。
所以这就是在失利方面的所有结果，像这些是由人精心策划的非常高质量的数据集。
那么从合成数据集提炼出的结果如何呢，特别是我们使用UltraChat进行监督微调和Ultra Feedback进行DPO。
这就是结果。
所以这基本上只是上周发布的工作，我们还没有发布代码和数据集，这周我们会发布。
所以你强调Zephyr是我们构建的模型，我们使用Mistral作为基础模型，然后使用UltraChat进行微调，然后在ultra feedback上进行DPO。
正如你所看到的，它实际上在这个alpaca eval排行榜上击败了ChatGPT。
而且，它是在所有开源中最好的，它击败了大多数的130亿参数模型。
它在alpaca eval排行榜上也是相当有竞争力的。
所以这个模型既有SFT又有DPO。
所以我们进行了评估，看看像SFT和DPO这样的两步过程有多好或多有用，因为首先你在指导演示上微调，然后在人类偏好上微调。
所以，这里的第一行显示的是，如果你直接在超级反馈上进行DPO，而没有进行监督微调，你会发现实际上效果非常糟糕，根本不起作用。
而第二行则表示，但如果你只是进行了监督微调，而没有进行DPO，这实际上是第一步。
这实际上效果还不错，基本上可以让你达到整体性能的80%到90%。
最后，这是在人类偏好数据上进行监督微调。
所以你采取这个角色，对人类偏好数据进行另一轮监督微调。
所以你记得你有被选择和被拒绝的。
所以你提供所有的对话历史，然后期望的完成是被选择的对话回复。
在这种情况下，你并不是真的在进行那种歧视性的事情。
你仍然在进行SFT过程，但你只是，你知道，以一种聪明的方式使用数据集。
这样它就遵循了监督微调的模板。
然后我们也发现，那并没有什么帮助。
所以最好的配方显然是DPO加上SFT。
所以，你知道，在超级聊天上先进行SFT，然后在超级反馈上进行DPO，这两个数据集都是合成的。
然后，你知道，这只是比只做SFT稍微好一点。
好的，我正在进入我的演讲的最后一部分，基本上是在看，你知道，所以我们已经看到了很多这些评估和基准以及排行榜，其中许多开始采用这些强大的模型，如cloud two和GPT-4，并将其用作在评估中代替人类的代理。
那么在这样做时与此相关的怪癖是什么，以及我们在以非常大的规模进行此操作时应该考虑的其他事项。
所以当我们这样做时，当我们使用GPT-4作为评估器时，我们发现它实际上具有位置偏见。
特别是，在成对偏好收集设置中，它倾向于生成评级为一。
那么，你知道，这里的图表显示了模型响应在整个数据集上的平均评分。
而另一方面，人类评分差不多是均匀的。
因此，您期望，您知道，这个分布似乎比这个分布好得多，这个分布向右倾斜。
然后我们做的是，我们提示 GPT-4 说，嘿，你有这种左倾的倾向，你总是生成这种评分为一的评价，你知道，要注意这种偏见，然后告诉它去消除这种偏见，它实际上会把偏见转向相反的方向。
然后它开始变得更加自我意识，因为它知道，你知道，它有这种偏见，现在它开始生成更多的五和六的评分。
一种摆脱这种偏见的方法是，我们确保每个响应在右边和左边位置是同等可能的，这样就可以稀释它对每个位置的偏见。
然后，你知道，我们发现实际上促使 GPT-4 生成分数，就像要求它分别为每个响应打分一样，空凳子也是如此。
然后，我们发现，与成对设置的排名相比，实际上发现这样可以稍微缓解问题，但并不能完全消除问题。
我们还发现了训练和评估之间的兴奋剂使用证据。
特别是，我们发现 GPT-4 更偏爱在 GPT-4 数据上训练的模型。
所以这些模型都是在使用 GPT-4 引导数据进行训练的。
而且，你知道，它更喜欢这种方式，而不是像更加实际、更高质量的人类，但他们可能非常简洁而直接。
所以当我们使用 GPT-4 作为评估器时，这是我们应该注意的一件事。
另一件事是，你知道，它也与这些其他论文的发现相吻合，即 GPT-4 更喜欢具有更高多样性的模型。
因此，这就是响应中唯一标记数量以及较长响应的独特标记数量。
所以如果你有这种列表的列表响应，就像 ChatGPT 一样，GPT-4 更倾向于将其评分高于不生成此类响应的模型。
我们还发现 GPT-4 在低熵任务（如数学编码和推理）上与人类的相关性较差。
所以记得我给你看的排行榜，我们比较了 GPT-4 的 ELO 评分与人类的情况吗？
然后我们更深入地研究了它在每个不同任务分布和类别上的表现如何？
这就是它的样子。
看起来，你知道，它说在一些更加事实性的内容上与人类的相关性较低，比如，你知道，期望一个正确答案的种类。
而在那些更高熵的任务上，例如头脑风暴和创造性生成，它们实际上与人类高度相关，这有点违反直觉，因为你可以用很多不同的方式想出，比如，你知道，一道菜谱或者一份清单。
但这就是GPT-4和人类的评级更相关的地方。
好的，最后一点就是要点。
所以这里有很多内容，但让我们试着把它分解开来。
基本上，你知道，我们讨论了如何为数据筛选、监督微调和RLHF提出步骤？
这涉及到几个关键因素，比如需要收集多少数据？
提示的长度是多少以及这些长度的分布，任务的分布，以及人类的作用是什么？
比如，你知道，你是否需要合成数据？
是否需要完全手工筛选还是介于中间的某种情况？
而且我们看到，有许多工具可以有效地微调开源LLMs。
根据SFT的结果，我们发现真实的QA是这些自动评估指标的主要区分基准。
然后我们发现，空的基准分数实际上与这些自动指标不相关。
所以更多地是，你知道，只有在一些这些模型上，我们发现它们是相关的。
对于蒸馏结果，这是来自Zephyr 7D的，我们在合成数据上进行微调，我们发现在AI生成的数据上的DSFT和DPO或DPO在AI反馈数据上的蒸馏实际上击败了CHAD GPT，即使模型只有70亿参数。
然后我们发现，你知道，在特定情况下评估RLHF模型的基准差距，我们没有用于评估奖励模型的基准。
而且，我们也没有用于评估红队行动和模型漏洞的开源基准。
然后最后，我们想，你知道，更深入地探讨使用GPT-4或一些强大的LLM作为评估者的一些怪癖。
其中一些就像，你知道，他们更喜欢在GPT-4的数据上训练的模型。
它有左位置偏差。
然后，与编码或推理任务相比，它与人类在创造性任务上的相关性较高。
我的工作曾被《纽约时报》的封面文章报道过，谈论了对齐的秘密成分，这对于 CHAD GPT 来说是对齐。
我也是联合国咨询委员会的一员，上周宣布的。
所以真的很荣幸能成为其中一员。
这里有一些博客文章，你知道，基本上，是的，我们今年没有发布很多东西，但我们写了一大堆博客文章，突出我们发布和工作的内容。
而且，你知道，其中一些是我刚刚讨论的演讲的一部分。
这是 Edge 4 团队的一部分。
我很感激能成为其中的一员。
谢谢你的倾听。
当你在一些模型中交替时，你是否选择非常高的温度？
还是保持温度值相对稳定？
这也可能是一个很好的产品。
是的，所以我们选择了，你知道，基本上我们尝试了不同的温度，但后来我们发现只是使用不同的抽样策略效果更好。
所以，你知道，使用不同的 P 值，然后 K 值以及一些组合，而不仅仅依赖于温度。 
有一个问题，就是阻塞能够编写人类能够编写更多的空间模型。
那么，你能不能就在规模上做到这一点提出你的想法？
还有，关于上一次，我认为是在尝试涵盖的不同主题和技术，是什么呢？是的，所以我认为在规模上进行红队行动，实际上有一篇最近发布的论文，叫做GPD模糊器，实际上它会引导并使用这些强大的LLM生成破坏其他LLM。
还有一篇DeepMind的论文，我认为实际上是大约一年半到近两年前，是使用大型语言模型进行红队行动。
那么，你喜欢如何使用另一个强大的语言模型对语言模型进行红队行动和评估呢？
所以我认为这是在规模上前进的方式。
那么第二个问题是什么？
我猜我在问，红队行动LLMs的独特类别是什么，或者是什么使红队行动LLMs与红队行动常规基础设施或软件不同？
是的，我认为一个重要的事情是新能力的出现，这本质上就像，随着你的规模扩大，我们正在看到的一个趋势，就是，你知道，随着我们的规模扩大，这些模型会做一些事情，或者，你知道，会出现一些之前较小的模型所没有的能力，我认为，举例来说，思维链推理，这个例子，你知道，GPT-2或者GPT之前是做不到的。
而且随着我们的规模扩大，另一个例子是这种几次提示，我们首次在GPT-3中看到，就像，你可以给它一个完全新的任务，并且不以任何方式更新其参数，只是将其作为提示的一部分。
然后，你知道，现在它只是启动了任务，然后它可以在许多示例上执行它，对吧？
所以像标注和所有这些东西都是通过使用GPT-3作为标签器等等而出现的，当我们发现了那件事情时。
所以我认为，本质上，你知道，另一个例子就是操纵。
我不认为任何开源模型目前都有这种能力。
但我知道像 Anthropoc 和 Openian 这样的公司，它们专注于欺骗和操纵，因为当你开始与这些模型交谈时，你开始将它们视为伙伴，特别是如果你有类似角色 AI 的东西，你可能会尝试向它倾诉，开始向它们分享你可能不应该分享的信息。
然后他们可以利用它对你造成影响，也许就像，你知道，最近我们看到 GPT-4 实际上操纵了某人，让他以某种方式阅读验证码，并告诉它验证码读的是什么。
所以这是一个非常具体的操纵例子。
所以现在似乎这些模型有能力做到这一点。
我认为开源模型还没有达到那个程度。
但这些就像，你知道，当你在最后做的时候，会出现的东西和漏洞。
我认为你想写一个想法，我不喜欢，就像那些线索变得更加真实时一样，像 GPT 这样的创建负责任的数据集非常重要。
但我知道也有一个问题，就是，如果你是一个开源的，对吗，提供数据集，你只是在告诉坏人如何使用这些模型吗？
所以我猜你对这个想法的看法是，关于开源。
是的，我会说，就像，这不是关于开源一个被设计来引发这种行为的数据集。
更多地是关于我们应该考虑的伤害类型。
所以更多地是关于，你知道，幻觉或抄袭，操纵，试图泄露个人身份信息，人们的信用卡，社会安全号码之类的事情。
更多地是思考这些不同方面，并举出具体例子，说明这些模型如何可以引发这种行为。
但我觉得你想要谈论的是，你知道，如果我们给他们一些具体的方法，比如具体的提示如何越狱，然后他们可以去尝试那样做。
我觉得首先要做的是，你知道，当我们在做这个的时候，我们会评估我们的模型，然后开始考虑自己的防护措施和安全性。
而且如果确实像你说的那样，数据集非常好，以至于我们可以说很多这些强大的模型在这方面失败了，那么显然你不会立即开源它。
但是你实际上要考虑的是最好的方法是先确保模型的安全，确保它不会，你知道的，基本上不会引发那种行为。
然后在分享它的时候，你已经，你知道，已经跨越了那个桥梁，并且说，是的，我的模型已经受到保护了。
所以更像是，是的，我们需要做的事情的一个梯度过程。
有一些关于重新训练数据的论文，你知道，会导致模型崩溃。
我的意思是，显然，语言不是问题，但是如果你正在使用数据进行微调，你有没有看到过潜在的崩溃，或者是如何防止那种情况的？
是的。
所以你在谈论当你正在使用合成数据引导另一个语言模型时，你有没有看到过一些模式崩溃之类的崩溃或者类似的情况？
所以实际上，到目前为止，很明显，这些都是好的。
就像这些实际上转变了，你知道，普通的聊天机器人和普通的语言模型成为了聊天机器人，而这些聊天机器人与与ChatGPT聊天时的体验一样好。
但尽管，你知道，就像我提到的那种怪癖，就像，你知道，当你有了这些模型，然后你现在把它们放在一个基准上，然后突然间你发现它就像是90%一样。
这可能只是因为你使用了作为评估器的模型来生成数据，然后创建了这个模型，然后反过来就像这个兴奋剂的事情，对吧？
所以这是一件很重要的事情需要考虑的。
另一件事是，我要说什么来着？
我忘了。
是的，另一件事是关于许可部分的，这有点不相关于你所问的，但基本上，你知道，有这样一种情况，你不能，就像我们不能开源并且进行商业化。
所以就像，你知道，仍然是有限制的许可，你不能在未来用它来构建和销售应用程序。
但是它仍然像是一个很好的研究工具。
所以我认为如果允许商业使用这些东西，我们可能会看到这种类似的崩溃发生，然后人们会说，哦，但实际上最近我们确实看到了，有这样一家公司叫做 Daxter，他们使用了，他们曾经使用 GPD 来进行摘要，他们用一个叫做 Mistral 的开源模型来替换它。
他们说他们的客户没有抱怨。
而且，你知道，他们节省了大量的资金，而且似乎效果很好。
他们说，你知道，这一点都不差。
但我并不是说 Mistral 是在任何合成数据上进行了训练，但这只是一个例子，说明通过这种 A B 测试，你可以替换这个模型为另一个模型，看看对事情的影响如何变化。
我在 Zoom 上有一个问题。
是的。
对。
嗯，看起来你可能还会检查 GPT 的另一个方面，就是成本。
所以我想知道你的总预算是多少，或者你生产打败它们的模型的总成本是多少。
哦，所以 Zephyr 7B 只是在 16 A 100 上进行了四个小时的训练。
所以大概不到 50 美元吧。
因为我们使用了一个合成数据集，它已经是开源的，是超级聊天和超级反馈。
但是整体成本相关的成本，所有的人和一切。
是的。
我明白了。
好的，所以在所有的人和一切方面，我想没有，我猜，像超级聊天那样，可能会报告一些成本和超级反馈，但它们大多是合成的，几乎没有人为干预。
所以它们可能会，我不知道他们是否报告过，我还没有深入研究。
但我想说，这仍然比我们从搜索和Scale AI购买数据花费的要便宜得多。
我们花费了约50万购买约20,000个人类偏好的提示，20,000个对话和约10,000个指令演示数据。
所以那是相当多的。
我对你用来评估GPT偏向的规模很好奇。
所以我看到幻灭比例是1.7。
是的。
哦，是的，这是熵标度。
就像，记得，一到四是递减的A，五到八是递增的B。
是的。
那是在最后一刻将模型提供给出口吗？
是的，确切地说。
是的。
在这些类型的评估中，对于购买价值而言，对提示的敏感性如何？
就像，你有一个新的库声称它考虑到了这种偏见，你知道，要写入偏见。
是什么阻止你说分布应该是均匀的，分布应该是正常的，然后迭代地看看它的样子和感觉如何？
是的。
是的，我认为这点很好，就是我们没有研究过是哪些任务或提示导致GPT生成这种偏见。
尽管我会说，你知道，这也是由LMSIS观察到的，也是研究的一部分。
但是，是的，LMSIS的论文也有这个内容。
但如果它在很长或者是数学之类的提示上生成这种内容，将会很有趣，因为这些情况很难在太具有响应性时进行评估。
至少作为一个人类，就像，当我看到像这边和那边一堆代码的时候，然后很难说，它们两边都在尝试做同样的事情，但是方法却非常不同。
评估它们是非常困难的。
对。
所以，是的，我们还没有研究过这个。
也许另一件事是，你认为顺序重要吗，就像你给GPT工作者的服装？
是的，我的意思是，基本上要点就是，你知道，有趣的是，人类通常具有最近偏见，也就是说，你记住的是你最后读到的东西。
所以你会尝试更多地选择那个，你就更倾向于这样做。
GPT-4实际上有左偏倚。
所以它首先在某种意义上看到的是第一件事情，我认为有些像LMSIS提出了这个因为它有这种从左到右的训练。
也许这就是它具有这种偏见的原因。
但是，所以我认为我们提升它的方式是，你知道，让每个模型的输出同样有可能出现在左边和右边。 
所以，如果我们在做alpaca和vacuna，那么我们不是只在左边做alpaca，在右边做vacuna，而是随机地交换它们。
所以两者都有可能出现在这两个位置。
你还看到左倾的情况吗？
如果你只是让它在一个一到四的范围内评分，是的。
但如果你说，你知道，嘿，你有这种偏见，并让它意识到它，那么它就会翻转，生成类似这样的东西。
所以是的。
是否还有其他方法，你可以通过重新排列提示来提示模型，然后你可以设计出结果？
通过重新排列提示，你的意思是像...
你能设计出最后的提示来重新排列你输入的顺序吗？
是的，所以我们所做的就是，你知道，我们会随机地打乱左边和右边。
然后，每个模型，所以基本上，就像你有你创建NC2的组合。
假设你想评估三个模型在10个提示上，那么你将有10 C2的组合，N是提示的数量，或者抱歉，是模型的数量，然后你会，你知道，基本上像生成，这将是一个总的数据集。
就像你知道的那样，你会从每个模型中生成10个响应，然后将它们放在这个3C2设置中放在一起。
然后，就像那样，就是每一个的组合。
然后你要确保每次左边的模型出现在右边时同样有可能发生。
所以如果你正在进行像模型一，然后模型二的操作，那么你也要确保像你也做了模型二然后模型一在一个不同的提示上。
还有更多的问题。
谢谢。
哦，好的，当然。
抱歉。
我应该保持放大吗？
谢谢。
那么我要问你关于强化模型证据的一件事。
所以，只是为了告诉你我是否理解正确，从人类反馈中可能学习到的第一件事是构建一个奖励模型，然后这个奖励模型，我输入文本，然后人类给它评分。
然后我有一个奖励模型，它将是下一个令牌，相当于，它到达了结束语句令牌，然后我通过奖励模型和奖励优化这个。
是的。
这就是我们的非常稀疏的方式，对吧？
只有当我们想要最后的结果时，但那就是必须的。
是的，完全正确。
所以你必须，这非常低效，基本上一次又一次地做这个。
这就是为什么你需要 100,000 个示例来做我们的讲座，但只有 10,000 个可能的原因。
这有点有趣。
好的，很棒。
非常感谢。
非常有趣。
