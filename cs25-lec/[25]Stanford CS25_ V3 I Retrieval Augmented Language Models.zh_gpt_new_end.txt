嘿，大家，欢迎来到我们本季度的最后一堂课。
我们非常高兴有Dawa在这里。
他是Contextual BI的首席执行官，这是一家企业LLM公司，同时也是斯坦福大学符号系统的兼职教授。
以前，他是HookingBase的研究负责人。
在此之前，他是Facebook人工智能研究的研究科学家。
他在剑桥大学获得了博士和硕士学位，还在阿姆斯特丹大学获得了逻辑硕士学位，并在本科阶段学习了哲学和认知人工智能。
他的工作重点是机器学习以及自然语言处理，具体来说是开发更好的语言理解和生成模型，以及更好的评估和管理工具。
是的，为Adel鼓掌。
好的，谢谢。
所以我想我应该站在这个角落，这样大家在Zoom上也能看到我。
是的，非常感谢你们邀请我来这里。
所以我问了Steven我应该讲什么。
有几件事我可以谈谈，多模态或者评估，而这似乎是首选的话题，因为其他的已经讲过了。
所以，是的，我非常乐意和大家谈谈关于检索增强的一切。
我认为这真的是我们领域中目前最酷的话题之一。
所以我将简要介绍一下目前发生的事情，以及我认为有趣的问题。
首先，显然，如果你错过了，我们正处在语言模型的时代。
我只是想在这个不是很大的观众中进行一个快速的投票。
我猜在Zoom上有更多人，但是谁发明了语言模型呢？
如果你认为是OpenAI，那我对你感到愤怒，对吧？
实际上，这是一个非常古老的想法。
这个想法就是你取一个序列，然后将标记概率向量化，对吧？
所以它并不是由OpenAI发明的。
它并不是几年前的事。
实际上，它已经有好几十年了。
我提出这个是因为我和某人在谈论，他们说是OpenAI和语言模型。
我就像，你在开玩笑吧？
所以我回顾了一下文献，这实际上是我能找到的最古老的，1991年的第一个神经语言模型。
2003年有一篇来自Bengio的很好的论文，其中实际上已经包含了词嵌入等所有内容。
所以显然这些是LLM，而不是LLMs。
正如事实证明的那样，如果你使它们变得非常大，并用这些庞大的神经网络参数化，那么你就会得到一些非常强大的东西，它真的显示出 emergent properties。
这就是为什么我们对这个东西如此兴奋的原因。
所以如果我们从经典的CS角度来考虑这个问题，有输入输出，对吧？
中间有这种东西，它是生成器。
所以我们取一个序列，输入序列，然后模型的任务是预测下一个令牌。
非常非常简单的模型。
这就是为什么在1991年就很容易提出这个想法了，因为这个想法非常直观，但很长一段时间以来，这个想法的真正问题在于用户界面。
我认为这就是很多人可能百分之百理解ChatGPT的原因，这确实是ChatGPT解决的问题。
所以最初你必须想出这些非常奇怪的提示，以便让你的语言模型做你想让它做的事情。
而人类在这方面很差劲，对吧？
所以我们更擅长告诉周围的人或物体我们想要什么，对吧？
所以如果我们有一只狗，我们会说，坐下，我们不会用一种非常奇怪的方式来提示它坐下，对吧？
和语言模型也是一样的。
如果你想要生成一些像海盗或莎士比亚风格的说唱歌词，那么你就告诉它生成一些像海盗风格的说唱歌词，对吧？
所以这种类型的指令数据实际上在网络数据中非常非常罕见。
所以你需要做的是修复语言模型的用户界面。
而实现这一目标的经典方法基本上就是ChatGPT使用的序列。
所以你以一种特定的方式提示模型，你对模型进行指导微调，然后在其上进行一些对齐，RLHF，以及你在此之上做的任何操作。
所以这是第一件事。
现在你有一个可工作的语言模型和一个可工作的用户界面。
那我们现在就完成了吗？
显然没有。
所以现在语言模型似乎正在风靡一时。
但是如果你跟任何人谈论，尤其是在企业中，例如，他们对准确性有非常严格的要求，他们会告诉你他们实际上还无法将其投入生产。
而原因是因为存在所有这些熟悉的问题，你们中的许多人可能正在解决这些问题，比如幻觉。
所以这些模型经常会以非常高的信心虚构东西，这在某种程度上更加令人不安。
归因，所以我们不真的知道这些模型为什么会说出它们所说的话。
静止，它们会过时。
所以这是一个很大的问题，就像聊天 GPT 不知道某个截止日期之后发生的任何事情一样，他们会不时地更新它，但你希望拥有一个始终完全更新的系统，永远不会过时。
你想要能够修订系统中的信息。
所以，如果你是一个欧洲组织，你必须担心 GDPR，这意味着你需要能够从语言模型中删除信息，或者可能修订事实，但我们不真的知道如何做。
所以，这又是一个很多人感兴趣的研究领域，模型编辑。
但这是我们真正希望能够解决的问题。
然后还有一个很大的问题，那就是你如何定制这些模型？
不同的人有不同的用例，您拥有不同的数据，如果您是一家公司，或者如果您想在自己的数据上使用语言模型，您如何使其在自己的数据上运行？
所以，现在大家都开始使用的一个解决方案是将其与外部存储器配对。
这实际上就是RAG，对吧？
所以我们可以说，整个讲座基本上是关于RAG的，但要理解这里发生了什么，我们有这个生成器就像以前一样，我们有输入和提示就像以前一样，但现在不仅仅是提供这两样东西，我们还提供了这个额外的上下文。
所以我们使用我们检索到的东西对语言模型进行上下文化。
而且，检索器往往非常简单。
它只是一个查询和一个文档编码器，然后您会得到一堆文档，然后将它们作为上下文提供给模型。
所以超级简单的架构。
我认为从这两个独立范式的角度思考这个问题是有用的。
所以如果您曾经参加过考试，我肯定您有过，对吧？
你可以进行闭卷考试，你必须记住所有的内容，所以你必须把所有的知识都塞进你的参数、你的神经元，或者你可以进行开卷考试，在考试时可以访问所有这些书中的信息。
所以，RAG也是一个非常类似的东西，对吧？
你可以将其设置成开卷考试模式，让它访问外部信息，比如维基百科或其他一些信息，或者基本上整个互联网，然后让语言模型在不必将所有信息都记住的情况下完成其工作。
所以，我认为这里还有另一个有用的区别，就是把所有内容都塞进你的参数里，这就是参数化方法，对吧？
所以我们在RAG中所做的就是添加了这个非参数检索组件。
所以如果你想给这个一个名字，你可以称其为半参数化。
好的，那么为什么这样能够解决这些问题呢？
答案基本上是，如果你有这个单独的索引，这个单独的检索器，你可以进行替换，你可以将其替换为新的索引，因此你可以真正地定制它。
因此，你可以根据用户真正想要看到的内容定制你的语言模型系统。
然后显然你可以更新这个索引，所以它不会静止不动，如果一切都出错了，你可以重新修改它。
你还会得到另一件事，对吧？
所以这就是我最初对这种架构感兴趣的原因，因为我一直在思考关于基础和多模式之类的东西，而实际上一个非常好的方法是找到一些其他信息，你可以用它来基于你的生成。
所以你真的希望语言模型只说它在这另一段文字或者甚至是它单独检索到的多模式数据中有证据的事情。
所以如果你这样做，那么你就会减少幻觉，因为你总是可以指向你的来源，它总是建立在你的来源上，而且你会得到归因，因为你不知道模型为什么会说它在说什么，因为它找到了这个东西。
这清楚吗？
好的，接下来的这堂课，我们将会讨论这个基本架构。
所以它看起来像是一件相当简单的事情，对吧？
但实际上你可以问很多很多关于这个系统应该是什么样的问题。
而且这甚至还不涵盖一半你可以问的问题。
那么，我们真正关心的是如何优化整个系统呢？
那么我们有独立的组件，检索器，生成器，还有像这个查询编码器这样的东西，我们如何对查询进行编码呢？
我们如何进行检索呢？
我们是否更新文档编码器呢？
我们如何实际定义文档呢？
它是一个完整的文档，还是一个段落，一块或一个句子或几个词？
所以有很多问题要问。
正如你所看到的，对这些问题也有很多可能的答案。
这就是我们要讨论的内容。
所以有很多体系结构涉及到这些问题。
我认为当我们逐步进行时，你思考培训时发生了什么以及测试时发生了什么是很有用的，对吧？
所以在培训时真的是，好的，我们有这个语言模型，我们有这个检索器，我们应该更新哪一个呢？
我们如何更新它们呢？
我们如何训练整个系统呢？
也许我们根本不训练它吗？
我们是否从头开始预训练它？
我们是否用已经分别训练过的组件初始化它呢？
这些是你设计这样一个系统时必须回答的问题。
然后在测试时，你有整个系统，实际上是以某种方式一起工作的多个模型。
所以在那里你也可以做不同的事情，对吧？
在测试时给它不同的索引或者操纵你抽样的方式之类的。
所以这一切的起点，我认为如果你现在问一个人，像RAG是什么？
他们会想到这个东西。
所以这基本上是冻结的RAG。
这里根本没有训练。
回到这个训练时间、测试时间的问题，这里只有测试时间。
训练时间是与我们没有控制的这种黑盒子模型分开进行的，对吧？
所以有这个文档嵌入模型，就像某个开源排行榜目前的顶部所示。
你使用它来，抱歉，获取一些向量，然后引导这些向量创建这个向量数据库。
然后这个向量数据库只是进行搜索，将搜索的信息提供给语言模型。
并且将其作为上下文传递，对吧？
这仅仅是因为上下文学习才能起作用。
我认为作为一个机器学习者，这感觉非常不优雅。
所以，这节课讨论的是我们能不能做得比这个冻结的东西更好？
那么让我们从这个左侧开始。
好的，如果我们想要仅凭矢量数据库就超越这个冻结的东西本身，那从检索的角度看会是什么样子呢？
检索的一切都是TF-IDF的起点。
大家知道什么是TF-IDF吗？
不知道吗？
好的。
那么TF-IDF基本上是一种稀疏检索方法，其中有一个评分函数，用于查看文档和查询。
所以有D和Q。
然后基本上有两个要紧的术语。
一个是TF，即词项频率，另一个是IDF，即逆文档频率。
所以这种逆文档频率实际上是Karen Spark-Jones的一个非常好的想法，她是一位被高度低估的研究者。
她做了一些令人惊叹的工作。
但基本思想是你想要看一下那些非常特殊的单词，这些单词不会出现在很多不同的文档中。
所以单词之间的重叠实际上并不重要，对吧？
就像在各个地方发生一样。
所以你想要有一些特殊的词。
这就是 TF-IDF 的要点。
它为文档查询重叠给出一个分数。
然后你可以用各种方式来加权。
所以有一些奇怪的不同参数，比如 B 之类的，可以让它比只有 TF-IDF 分数更好。
所以你可以在那里做一些调整。
所以 BM25，实际上，如果你在想，它代表的是最佳匹配 25。
所以我试图发现，25 究竟是从哪里来的？
那是因为之前的 24 次实验都失败了。
所以实际上是第 25 次看起来有效，这就是为什么它叫做 BM25。
但是这是稀疏检索。
它只是计算单词。
所以你有这个庞大的、庞大的向量，其中包含所有这些单词出现的次数。
它是稀疏的，因为大多数单词从不出现。
所以它就像是一个词汇大小维度的向量。
所以大部分都显然是零，如果你想在 CPU 上进行快速搜索，这实际上是一种很好的性质，因为在 CPU 上，稀疏矩阵的乘积计算起来相当容易。
这在名为 DrQA 的系统中使用，这实际上是这种开放域问题回答范式的第一个神经网络实例之一。
所以你有一个问题，比如，华沙的居民有多少人 blah, blah.
所以你基本上想问维基百科这个问题的答案是什么。
然后你有了基于稀疏矩阵的文档检索器，所以在这种情况下，BM25，检索方法，你把它传递给这个，我认为这当时仍然是一个 LSTM，和一个文档阅读器模型，然后那个模型给你答案。
所以我认为这实际上是第一个在检索和生成系统之间有这种分离的情况，用于回答基于开放领域知识的复杂问题。
所以在稀疏矩阵之后，进行了大量关于密集检索的工作。
所以密集检索的优势在于，这只是词嵌入，基本上是向量，对吧？
它们现在是密集的，不再是稀疏的，所以在维度上要小得多。
稠密检索的一个很好的优势是它并不真的关乎具体的词语，对吧？
所以如果它们是同义词，你仍然可以找到相关的文档，而这在稀疏表示中是做不到的。
所以稠密的优势在于你可以获得语义上的相似性。
所以你可以通过词嵌入来做到这一点，但这并不是那么有效，但在人们开始思考这个问题的时候，BERT已经存在了，并且BERT真的很擅长为整个词序列提供一个向量表示。
所以一个句子表示或一个段落表示。
所以有所有这些很酷的系统，比如ORCA和DPR，稠密通道检索器，它们基本上使用检索作为系统中的一种潜变量，并且让这个潜变量能够工作、足够好的方式是在相关信息上对检索器进行预训练。
所以对于ORCA，他们做了一些称为逆关闭的事情。
所以他们做了一种类似于关闭任务的事情，你想要找到与前面的段落有关的段落。
而在DPR中，他们只是对其进行了监督训练。
但实际上，核心思想在于，正如您在这张图中所看到的，如果您添加了大量文档并且计算分数函数的方式要简单得多，那么您可以比M25做得更好。
它只是点积。
点积的好处是，如果您知道自己在做什么，也可以在GPU上非常高效地执行它们。
因此，您真正想要的是最大内积搜索，MIPS。
这是许多这些东西的核心思想之一。
您可以使用ANN（近似最近邻搜索）执行MIPS。
因此，在那个时候，我一些同事进行了一项非常出色的工作，被称为PHASE，它真正成为了所有这些现代向量数据库的基础，对吧？
所有流行的数据库，它们都是对这个PHASE思想的重新实现。
一个是用Rust写的，一个是用Go写的，但基本上都是相同的思想。
就是PHASE。
因此，PHASE真正推动了许多这些东西。
每当有人告诉您有关矢量数据库的信息时，只需想一想PHASE，非常快的点积。
显然，您可以超越点积。
是的。
PHASE是什么？
所以这是一个开源库，Facebook人工智能相似性搜索。
不，这只是基本的现成ANN算法。
是的，所以有各种不同的，我不知道你知道什么是产品量化之类的东西吗？
基本上，你有一堆向量，你可以计算完整的点积，这种方式有点低效，对吧？
所以你可以尝试压缩向量的子空间，然后只看一种类型的质心。
所以你可以量化完整向量的子向量，然后只对质心进行更快的搜索。
这是一个很好的问题。
还有其他问题吗？
好的。
关于这个点积的想法。
所以我们这里有一些人称之为孪生网络的东西，我想是这样的。
你在这里有两个不同的BERT模型或者你的编码器是什么。
然后最后你得到这两个向量，然后你只是做点积，你得到一个单一的分数，但是如果你愿意放弃这种编码器方法，你可以做各种各样更加花哨的事情。
所以从斯坦福的一个同事这里有一个真正好的例子是Colbert。
所以这是晚期交互。
所以，不仅仅是有这个点积，你有一种更复杂的计算分数的方式，其中你在不同词之间聚合了最大相似性分数。
所以我最近才发现这被称为科尔伯特，因为晚间秀科尔伯特。
所以这实际上是奥马尔的笑话，他的名字，但只是让你知道，如果你遇到它。
所以，但我认为如果我们看一下现在技术发展的方向，这些向量数据库的一个好处是它们非常高效，对吧？
所以点积比这个晚期交互的东西更高效，特别是如果你进行近似最近邻搜索。
但是已经有一些很酷的工作。
所以像Splayed这样的东西，它们基本上在某种程度上将稀疏与稠密相结合。
所以一个大问题，正如我所说的，稀疏的一个大问题是你不能真正处理同义词之类的东西，但你可以做的是采用一个密集的模型，比如一个鸟模型，看一下你的序列中的这个词，尝试看看哪些其他词适合在同一个位置。
所以这给你同义词。
现在你可以把所有这些同义词给一个稀疏向量，然后你可以做稀疏点乘。
所以它有一种更高效的搜索方式，而不是放弃从稠密表示中获得的所有酷炫的东西。
这是一件事。
而我非常喜欢的另一个想法叫做Dragon。
我认为，这一侧实际上是最好的广义密集检索器。
所以如果你现在想拿一些现成的东西，然后去Hugging Face之类的地方，那么Dragon或Dragon Plus可能是你想要用于密集检索的东西。
他们训练的方式是通过这种渐进式数据增强策略，通过采样非常困难的负面样本，使模型变得越来越好。
这给你带来了非常好的表示。
关于这个的另一件事，我认为，关于检索的一般情况，我们现在看到的唯一的一点是，如果你看一下Dragon周围的开发者社区，他们现在都在做混合搜索。
所以你实际上可以将来自你的稀疏、BM25或其他显示的东西的搜索结果与你的Dragon组合起来。
然后你会得到这个排名，甚至效果更好。
所以你会得到两全其美的效果，但是随之而来的是如何将结果结合起来的所有这些问题。
关于这部分有任何问题吗？
哦，你能听到我说话吗？
可以。
哦，抱歉。
在之前的幻灯片上，有关于基准测试的研究吗？例如，RAG相对于闭卷问答少出现幻觉的工作量，例如，直接问大语言模型问题，有没有进行过基准测试研究？
是的，有一篇很棒的论文，我可以说是我自己写的，关于检索增强减少幻觉的事实，它是2021年的，我想。
所以是的，你可以找到，如果你简直搜索检索增强减少幻觉，然后你会找到这篇论文。
谢谢。
我们将看到轨迹舞蹈方法以及为什么需要拭子？
是的，通常情况下，你希望对你不想要同义词或最接近邻居的东西进行非常精确的词汇重叠。
所以，如果有一个品牌名称或类似的东西，比如说品牌是苹果，你不想找到关于那些对吧。
这就是你对密集检索器的操作方式。
所以这实际上取决于你想要用它来做什么。
这就是为什么混合方法可能是正确的方式。
这是一个很好的问题。
通过密集方法，它在表达中是有上下文的。
它意识到苹果公司会变得不同。
不，如果它们确实是上下文化的，那么是的，但很多时候它是一个冻结的检索系统。
这就是所有冻结架构的问题之一。
我可能漏掉了一些与您正在进行的因素的重复不同的东西。
您正在使用作为查询或馈送的因素。
不，文档的类型和查询是相同的，对吧？
所以它们要么是稀疏的，要么是密集的。
但是如果它们是稀疏的，向量的组成部分实际上就是其他单词。
所以 - 当你认为它是创建树突的东西时，你只是在惩罚一次。
你是如何第一次得到这个的？
所以它基本上是计数，对吧？
所以基本上它是一个大的文件矩阵，行是文件，列是文件中的单词。
然后你只需计算单词在文档中出现的频率。
这是一种稀疏的方式——我不知道你是否也在指那个。
是的，在这个领域，我们称它们为稀疏嵌入或稀疏检索，因为大多数向量都是零。
因为大多数单词不在该文档中出现。
这有意义吗？
很好。
那么让我们谈谈如何稍微改进一下。
所以回到史蒂芬的问题，关于，好吧，我们有这种检索的东西，但我们如何确保这个检索器在使用时表现良好呢？
所以我们能否将检索器情境化为生成器？
即使是一个我们可能无法访问权重的生成器。
所以它可以是一个 GPD4 模型，我们只是将它发送到某个 API，然后得到一些东西。
所以我审查过的一篇论文叫做 replug。
所以只是为了解释一下这是什么样子。
所以你有这个上下文，你有一个我们用标准检索步骤进行的检索器，这是一个密集检索器。
现在，对，现在你计算可能性。
所以基本上只需对前 K 个文档的得分进行归一化，以得到一个分布。
然后，你会将每个撤退文档分别提供给这个生成器，给你的语言模型。
这样你就可以查看该语言模型对正确答案的困惑度。
所以现在我们有了这两个概率分布或者说是两种可能性，我们可以最小化KL散度，以确保我们实际上可以检索到导致语言模型对正确答案困惑度最低的文档。
所以这是一个非常简单的想法，而且效果非常非常好。
而且这个好处是完全不受上游发生了什么的影响。
所以这对于任何语言模型的编码器解码器都适用。
你所需要的是一个困惑度得分，但对于大多数语言模型，你都可以获得这个得分，不一定是全部都有。
所以这就是一件事。
然后还有另一种非常好的方法。
你改变的是哪个参数的点？
所以在检索器中，你实际上是在更新密集表示，对吧？
所以你的编码器基本上是为了你的密集表示。
这是一个很好的问题。
我们稍后会更深入地讨论这个问题。
所以关于上下文检索、增强语言模型，还有另一篇论文，整篇论文基本上就是关于只做BM25，并且直接将内容提供给语言模型的上下文，事情就能够运作。
这有点像冰冻的残羹，但在某种程度上更为原始，其中检索器是这个非常古老的稀疏算法，但它确实运作得非常非常好。
但后来他们有这个非常棒的章节，展示了你可以在BM25的结果之上添加这个重新排序器，你可以向这个重新排序器回传。
所以现在你仍然完全固定语言模型。
所以这在这里是损失的一部分。
所以你在参数数据上有一种停止梯度的情况。
那只是你的语言模型，但现在你有了这种可以向其中回传的排名函数。
所以这就是你的重新排序器。
基本上，它可以是一个伯克模型或者任何其他在最初从BM25检索到的内容之上运作的东西。
现在你有了这个伯克重新排序器，你可以向其中回传。
所以这也非常非常好运作。
所以我们正在逐步朝着一个更加优化的系统发展，这个系统能够以一种更加有效和上下文化的方式进行检索增强，使其对你想要使用的内容更具有用途性和情境性。
所以，是的，就是为了指出这个重新排名器看起来是什么样子。
所以你实际上只是多了这一步，对吧？
所以我们有我们的检索器，然后我们有一个重新排名器，然后我们有我们的生成器和输出。
你能创建语言模型吗？
不，不一定。
所以对于这个，是的。
但是对于replug，你不需要，对吧？
是的，是的，是的。
所以基本上，是的，你需要得到-不是所有的，一些需要。
但是是的，你可以在此基础上做各种花样，对吧。
所以基本上问题是我们如何让梯度流入其中，对吧？
所以如果你实际上不能访问模型的全部参数，以便你可以贯穿其中进行反向传播，那么你可以在检索中执行一种强化式的损失，然后如果你可以访问的话，你只需传递一种对数似然或其他一些黑盒函数。
好的，接下来你可以做的事情是优化检索器和生成器。
因此，这真的开始进入整个架构的适当上下文化，您希望一切协同工作，对吧？
所以，与其拥有这个冻结的东西，其中一切基本上都不知道其他部分存在，不如说？
就像大脑的两个半球，它们彼此不交流。
一个是您的检索器，另一个是您的语言模型。
它们之间没有连接。
它们就像是把东西扔过篱笆，然后你希望一切顺利。
所以，与其如此，我们让一切更加紧密，一起学习。
因此，通过生成器实现这一点的第一种方式之一是RAG，检索增强生成，我们在2020年在FAIR完成了这项工作。
它与我们已经看到的非常相似。
基本上，我们在这里有一个检索器，它在不同文档上工作。
您得到一些分数函数，交给这个生成器生成答案。
现在，您希望一路回传并更新生成器，对吧？
因此，在我们之前看到的两种架构中，您保持生成器固定，将回传应用于检索器，但在这里，我们更新一切。
好吧，不是完全一切如你所见的那样，但我们还将更新检索器和生成器的部分。
所以在这个RAG模型中，我们实际上有两种不同的方法来做到这一点。
当我们谈论这个时，如果你思考足够长时间，你会觉得，好吧，但是我什么时候实际上需要检索呢？
就像，每次生成一个新的标记时我要检索吗？还是我只检索一次然后生成整个序列，对吧？
或者也许我想要在每个结束标记时检索，对吧？
所以这些是超级克拉姆。
或者也许我想要学习何时进行检索，正如我们将看到的那样，这也是人们所做的。
所以这是两种不同的方法来做到这一点。
在这篇论文中我们所做的基本上是，整篇论文的重点是这个冻结的东西实际上并不太有效。
所以我认为现在人们称之为RAG的东西通常指的是冻结的东西，但如果我们只是做了冻结的东西，整篇论文基本上是不会被任何地方接受的。
整篇论文的重点是你想要对其进行优化。
所以在我们公司中，我们把这个冻结的东西称为“科学怪人”，因为实际上就像你把这些不同的部分拼凑在一起一样，对吧？
你有点像是，是的，这真的像是弗兰肯斯坦。
你只是把它拼凑在一起，然后它就有点像是走路，你知道的，但它实际上并不需要解决问题。
它实际上并没有真正起作用。
它不是真正的东西。
所以我想对这里的每个人来说都是个好消息，因为现在有很多机会可以做得比大多数人现在使用的要好。
所以原始的RAG架构的一个限制是它只支持一个非常小的洞穴。
所以如果你有很多很多的文档，那么问题就是你必须把它们全部放入上下文中，但是你真的怎么能做到这一点呢？
所以你可以做的一件事是首先编码这些内容，这样你就可以得到一个单一的表示或者只有少数几个顶层的表示，然后你把它们连接起来，然后把它们喂给解码器。
所以这就是FID融合和解码器。
正如你所看到的，这可以扩展到更多的段落，这就导致了你关心的分数的相应改进。
所以这真是一个很棒的想法。
所以我们慢慢地转向了更多的仅解码器架构，对吧？
在RAG中，我们有这个BARC模型，它有点是编码器解码器架构，但在这里，你只有这个解码器，在先前检索的内容上进行一些花式的注意力操作。
还有另一种纯解码器语言模型架构是这个，KNNLM，我觉得它在简洁性方面非常优雅。
所以基本上你只是有一个普通的语言模型，但是你用先前检索到的东西插值了普通语言模型的权重。
所以基本上你有某种提示，对吧？
就像奥巴马的出生地是的，你去你的大语料库，找到类似的东西。
你看看跟在类似的东西后面的单词。
你对那个东西进行排名，你采样你的前K个，然后重新规范化。
所以现在你有一堆分数，现在你可以在检索到的非参数化内存分数和参数化语言模型分数之间插值。
所以这在某种意义上是非常晚期的融合。
所以实际上在最后，你把这两者结合起来，它允许你重新加权纯语言模型的概率或概率分布。
所以这个方法非常有效，尤其是在你有一个庞大的检索语料库的情况下。
如果你有数以万亿计的标记在其中，你可以拥有一个规模较小的语言模型，它不需要进行太多的重型计算，因为你可以真正依赖于你正在使用的这个大型源语料库。
这个想法被这篇名为Retro的DeepMind论文所利用，他们展示了你可以从头开始训练一个比原来大25倍的检索增强语言模型。
所以真的是完全从头开始预训练，而在相同的数据上以困惑度方面超过这个25倍更大的语言模型，这相当令人印象深刻。
所以这种架构比参数模型更高效，因为你可以依赖这个外部存储器。
所以如果你的外部存储足够大，你可以获得相当大的收益。
所以当Retro被宣布时，有很多兴奋，但这是DeepMind的一篇论文。
所以真的没有开源，没有真正的验证这是否有效。
最近，NVIDIA推出了一项名为Retro++的工作，其中他们将复古架构与基本上是RAG的东西混合在一起，他们在语言模型的上下文中将排名靠前的一个或前K个结果放在了这个混合体中。
所以这在某种程度上是介于RAG和Retro之间的一种混合。
他们在这里展示了一些非常好的结果，但我认为这在某种程度上指向了这个大缺陷。
那么为什么还没有一个好的开源复古模型呢？
这可能告诉你一些关于它是否真的有效的东西。
在我的职业生涯中，我花了很多时间试图复现DeepMind的论文，但并不总是有效。
所以我认为Retro也是如此。
这就是为什么我们需要在Retro的基础上使用上下文RAG来实际使其工作的原因。
但是这是否只是对于搜索一个权衡书籍的相同真实？
是的，所以。
你想要那个。
所以在那个大语料库上进行检索并不难。
是的，甚至有像分布式阶段包这样的东西。
你可以自己做一切。
所以，是的。
所以在计算方面，要再现这样的东西实际上并不那么困难了，但我已经尝试过好几次，它真的不容易再现。
所以让它起作用的唯一方法是，如果你在 Retro 上下文中执行这个 RAG。
然后，正如你在这里的结果中所看到的，它实际上比纯 GPT 模型更有优势。
所以它起始于一个 GPT，然后他们在称之为 GPT 模型的基础上进行了改进。
总的来说，我认为在从零开始预训练这些系统方面仍然有很多工作要做。
Retro 的一种可能性，但我们并不确切知道如何以正确的方式去做，这也是一个有趣的未解问题。
这确实是一个有趣的开放性问题之一。
关于这个在线有什么问题吗？
好的，那么我们将继续。
所以让我们全面进行情境化。
所以在 Retro 和 RAG 中，我们实际上只更新了查询编码器。
所以更新文档编码器是非常昂贵的。
所以实际上，非冻结的密集检索增强方法的原始论文之一是这篇称为 Realm 的论文。
这真的是一项有远见的工作。
这基本上是第一种完全实现了这一点的版本，其中包括更新所有内容，包括文档编码器。
那么，有人能解释一下为什么更新文档编码器很昂贵吗？
假设我们的语料库中有一万亿个标记。
所以现在我们一路前进。
基本上，我们进行前向传递。
我们在最后得到一个梯度。
现在我们通过检索器向后传播梯度。
我们更新查询编码器。
现在我们必须更新文档编码器。
那么，在我们更新了文档编码器之后，我们需要做什么？
我们需要重新编码整个互联网。
所以基本上每次梯度更新，我们都必须重新编码我们的索引，所以如果这就像是数万亿个标记，那么就像是在每次批量更新之后重新编码互联网。
这样并不是很高效。
嗯，你需要我们看一下我们学到的中间变化，或者其他一些东西，比如如果我们处理所有激活并且听起来非常长，就像对你的实践进行不可预测的变化。
是的，这是一种方法。
所以有很多不同的方法来更新文档编码器。
所以在 Realm 中，他们基本上是为了 T 批次而这样做的。
然后他们停下来，重新对整个互联网进行编码，然后再次进行训练。
所以这有点是异步更新。
他们有这种非常花哨的分片机制，他们会将整个索引的某些部分摘下来，然后在运行时更新它们。
所以你可以做到，只是非常昂贵。
所以很多人一直在思考的一件事情，不完全是 LoRa 的想法，但类似的版本存在，就是，你能不能使它更有效率，这样你就不必异步地进行这个操作了？
所以 Realm 架构的一个缺点是它真的只是一个鸟模型，但然后你用其他鸟模型对一个鸟模型进行检索增强。
它并不是非常具有生成性。
它并不是真正的现代范式中的 Gen AI，但如果你想读一篇关于这个主题的论文，那么这是一篇非常好的论文。
另一篇真的非常值得一读的是这篇名为 Atlas 的论文。
所以 Atlas，这是 fair 团队的一部分，由一群很出色的人共同完成，他们做了像 Rag 和 FID 这样的工作，是一群非常聪明的人。
而这实际上是对这个架构中发生的一切的全面分析。
所以他们首先看的问题是，我们如何训练这个检索器？
所以我们已经看到了这方面的一些版本，但哪一个实际上效果更好？
它们实际上还没有在一个正面比较中被比较过。
所以一件事是，我们有这种 FID 风格的关注蒸馏。
所以这个在这里详细讨论实际上太复杂了，但其他的实际上非常简单。
所以其中一个是我们基本上之前见过的这个损失。
所以我们已经看到过这个，我想是在上下文 Rag one 中。
所以我们在语言模型上有一个停梯度，然后我们更新检索器。
另一个是我们在 replug 中见过的。
所以这基本上就是 replug 损失。
所以我们有文件的 KL 散度，以及当你提供该文件时看到的改进。
它们还有的另一件事基本上是那个的反义词。
所以如果我拿出这个文件，那会如何影响我语言模型的困惑度？
所以这个，我认为实际上相当优雅。
这是因为这个确实涉及到，这个单一文档对于我正确回答这个问题有多有价值？
所以他们比较了所有这些不同版本，你可以看到重新插入样式的损失和这种留一出损失，它们表现得比所有其他方法都要好得多。
所以这个固定的检索器或者没有联合预训练，这些实际上是一种基线的被冻结的Rag模型或者封闭书籍。
而且你可以看到，如果你优化了这些东西，你可以做得更好。
所以我想只是留一个是最好的，我想说。
那么另一个问题是，你究竟怎么样来训练整个系统呢？
就是说你在什么数据或者什么任务上训练这个？
所以他们也尝试了一堆不同的版本。
一个是做前缀LM，如果你熟悉的话。
所以他们基本上是从互联网的某个地方获取一小段，然后他们预测从那一小段中的下一个片段。
所以这真的是像句子到句子。
所以也许以前是跳跃思想，但现在你有了这个检索步骤，你可以预测下一个句子。
然后他们只是做T5风格或去噪音。
所以有大规模的语言建模，如果你熟悉T5的话。
然后他们为这个部分生成过程有个标题。
所以我认为从这个表格中得出的要点基本上是，无论你在这里做什么，他们都在使用T5模型。
所以你在这里做的任何事情，都需要符合你的语言模型的期望。
对于T5来说，那就是T5风格的损失。
然后接下来，他们研究的最后一个问题是回到我们讨论过的，我们如何准确地更新这个检索器？
所以我们是不是需要更新文档编码器，还是我们可能需要做一些重新排序，或者我们可能只需要更新查询？
相当令人惊讶的是，我认为他们发现只是更新查询。
所以就像在原始的红色论文中实际上已经基本上足够好，在许多情况下。
这很好，因为如果你不需要一直更新你的文档，这样效率更高。
我认为这里真正的问题是，你的文档表示有多好？
所以你需要有一个非常非常高质量的嵌入模型才能使其工作。
如果你没有，那么这将不起作用。
但是如果你有了，你就会得到一个非常好的查询端微调的东西。
所以Atlas论文是关于尝试做一些快照式的语言建模任务。
所以这是在上下文中给出了多少例子。
是的，所以这里的主要要点是，如果你比较闭卷等价模型和检索增强模型，你会看到非常大的改进。
这真的是整个部分的唯一要点。
但我认为这确实在某种程度上说明了我们应该考虑的问题。
我还有多少时间？
好的，好的，还有其他问题吗？
训练步骤中的文档是这样说的吗？
是的，所以它们可能是不同的。
因此，在Atlas中，Atlas基本上尝试了所有的方法。
所以他们还试图看看如果我在维基百科上训练这个，但是我换成了一种常见的网络爬虫索引会发生什么。
我认为在Atlas中，以及在复古领域发现只是越多越好。
所以这实际上就像你的索引越大，你就越有可能找到确切的东西，然后做出正确的预测。
关于这个还有其他问题吗？
噢，是的，抱歉，这是关于生成器的一个问题，在 RAG 系统中，我猜是 RAG 系统。
所以最近我看到了一篇关于 Mistral 7B 的论文。
所以它介绍了很多这些新的架构变化，比如滑动窗口注意力，以更小的代价处理更长的序列，以及用于更快推断的组查询注意力。
我想知道你对专门为 RAG 设计生成器的想法，例如，利用 Mistral 7B 目前的情况，因为例如，像滑动窗口注意力，我可以看出它如何适应 RAG 的情况。
是的，也许你对 Mistral 有些特殊之处的看法与我不同。
所以我认为滑动注意力窗口并不是那么有趣。
Mistral 之所以效果那么好，是因为它经过大量数据训练，而且由于有滑动窗口注意力，你可以更有效地进行训练，因为你不需要关注所有内容。
但是回答你的问题，我猜你在问关于生成器架构的问题，如果你知道会有一个检索器的话。
所以我认为这基本上就是 Retro 试图做的，对吧？
Retro 实际上，一些 Retro 论文的作者现在在 Mistral。
所以在这里他们有这个块交叉注意力的想法。
所以基本上你有语言模型，但它在你的复古架构中对你检索的东西进行注意力的方式，它们被整合到一个模型中，不是使用标准的注意力机制，而是使用这种稍微不同的块交叉注意力。
好的，我想我试图解释的滑动窗口注意力点是，它使用一个固定的窗口，所以每当你进行查询关键计算和尝试查询向量和关键向量时，你使用的是固定窗口注意力。
所以我想我的想法实际上是，一、使用动态窗口，因为例如，rag案例，如果在进行注意力时使用固定窗口，有可能你实际上是在留下，你只看了一段固定的信息。
所以如果你能够也许调整Mistral，使其对rag案例更好，例如，将固定窗口大小变成动态窗口。
是的，我认为这是一个有趣的想法。
所以对我来说，Mistral用滑动窗口所做的事情基本上就像是一个ConvNet，对吧？
所以我们有所有这些类似卷积的轻量ConvNets，我们会有词嵌入，然后对其进行卷积，然后拉动，然后你仍然会得到信息。
所以并不是滑动窗口阻止你提前查看，只是这在你的transformer的更高层发生。
是的，是的，好的。
所以我认为这绝对是一个值得思考的方向，是的。
是的，所以我认为说，是否有任何可以引入到这些70亿参数模型中的架构变化并不太疯狂呢？
所以它们可以更好地适应rag case。
是的，所以可能会有，是的。
我觉得一个问题就是你如何对已检索到的内容进行注意力，对吧？
我想这是你在做的 - 是的，嗯，谢谢。
所以只是为了确保我理解，所以是的，在这个复古模型中，你正在检索每个块，当你谈到将检索放入上下文时，你是说你需要在开头做还是在结尾做？
是的，在上下文中，所以这不完全是每一层，是每个标记，对吧？
所以基本上每一步，不是每个块。
所以这没有意义。
所以并不是每一层都进行检索，对吧？
是的，所以每一步，对吧？
所以这有点像rag token是什么。
所以你检索每个标记，然后生成，然后可以再次检索。
或者在retro的情况下，你可以生成一个块，然后再次检索块。
如果你看一下上下文的情况，你在开始时检索一次，然后你提供它。
所以在这种情况下，你是在说在更琐碎的情况下，没有人有足够的内容吗？
是的，所以在上下文的情况下，这里你实际上根本不将其直接提供给模型，对吧？
所以在这里，你让解码器在其上进行注意。
跨页面。
所以我认为跨注意力并不真正起作用。
是的。
其他问题？
是的，我们发现我们定位在接收端进行培训并不是很必要，因为有整体的图像。
所以我想知道在案例的洞察中，哪些案例真的需要在下一次更新中进行培训，或者有没有更新这些参数的任何方式？
是的，所以你确实希望更新检索器，对吧？
但是对于很多情况来说，只有检索器的一部分是必须更新的。
所以这些都是非常特定的数据集，对吧？
自然问答，维基百科向导和Fever。
所以它们确实是非常依赖知识的任务。
所以在这种情况下，如果你已经有一个非常好的系统，比如DPR，它专门为这些任务进行了预训练，那么你只需要更新查询编码器。
所以我认为，如果你超越这一点，转向一般的语言建模任务，比如retro，那么你可能确实想要更新文档编码器，至少以一种可以逃避它的方式。
所以我认为，在这个任务中，通过非常依赖知识，我实际上完成了评论生成器的学习改进，只要我们对文档有一个良好的官方知识和那些好的模型。
是的，但是你需要学会如何查询那个索引，对吧？
所以如果你不这样做，你就得不到很好的性能。
所以那就像是你的闭卷考试表现，对吧？
如果你只有语言模型，你只是像，没有检索器的参数模型本身到底知道什么？
正如你所看到的，那里有相当大的差距。
其他问题？
否则我会解答其他问题。
没有？
你好？
开始吧。
哦，快速问题。
那么，更多层次化的检索呢？
我想会有一些方法，不仅仅是检索单个块，而是一些块的群组或摘要版本。
关于这方面有一些有趣的工作，首先你要尝试找到，这样你可以有多个索引它们可以有点级联，对吧？
所以首先你想找到相关的文档。
所以你有一些文档表示，然后在该文档内，你想找到相关的块。
所以你可以以这种方式做。
你也可以反过来做。
我想我在那张幻灯片上有一些内容，你可以找到块，然后扩展周围的上下文，然后将其提供给语言模型。
所以我认为，是的，你可以在那里做各种有趣的事情。
谢谢。
我想还有一件事，就是，你能比较 RAG 和长上下文 LOM 努力吗？
所以有很多关于只是具有非常长上下文的事情和它能够替代 RAG 的极端的事情，但我喜欢一些想法。
是的，所以每个人都明白这个问题，对吧？
所以有一个趋势，我们希望有非常长的上下文语言模型，这样基本上你可以拿哈利·波特之类的东西，只需将其放入上下文中，然后问一个问题，比如哈利·波特的猫头鹰叫什么之类的，对吧？
然后它可以在整个内容上进行考虑。
那么在回答那个问题时，在整个哈利·波特上进行考虑是非常低效的，对吧？
所以大部分哈利·波特与猫头鹰无关，但如果你使用长上下文窗口，你仍然在某种程度上阅读它。
所以这就是为什么我认为以 RAG 方式进行操作更有效，因为你有这个非参数化的组件来解决这个问题。
而且，如果你真的看一下关于长上下文窗口的文献，解决注意力机制缩放问题的方法是使其非常稀疏。
所以他们基本上在幕后把它变成了一个不同的稀疏形式，但他们将它转化为一个非参数化的检索问题。
所以它们实际上并没有那么不同。
如果你想要扩展长篇上下文，那么你会朝着 RAG 风格的架构迈进。
好的，谢谢。
好的。
那么让我们谈谈其他一些有趣的问题。
所以一个事情，我已经提到了，那就是我们实际上何时检索？
如果我们想要检索每一个标记，那也是非常低效的，因为我可能不必要检索来生成。
我可能可以自己做这件事，用语言模型，这样去检索东西有点浪费。
但是如果我在序列开始时只检索一次，那也可能不太好。
所以我们理想情况下想要做的是能够说，好的，有时我想检索，有时我不想检索，并且我将学会何时要在检索上花费计算预算。
所以一篇很好的论文，他们尝试了这个问题，名为“活跃检索增强的 Flare”，在那里他们基本上让语言模型决定何时应该进行搜索以及搜索什么。
所以我认为这符合你在领域中可以看到的关于代理的一般趋势。
所以我们也可以再谈谈这个问题。 
所以我认为我们已经在这里覆盖了另一个问题，那就是我们如何以规模进行训练？
所以我们可以进行这些异步更新，我们可以进行重新排序，我们可以只在查询侧进行。
有一篇非常好的论文，我认为它与你提出的想法非常接近，你首先使用BM25创建一个批次，基本上在这个批次中，所有检索到的内容都非常相似。
现在你有了这种批内更新。
所以这有点像一个重新排序器，在这个重新排序器中，你对批次中的信息进行编码。
使用另一个模型，现在你可以实时更新这个模型。
所以你不必太担心做完整的文档侧更新。
而且，在这里，真正重要的是你的索引有多大？
如果你有一个很棒的索引，你基本上可以通过查找来解决任何问题。
所以，与其把它塞进你的参数中，不如找到它。
这是一篇名为《Silo》的非常好的论文。
所以我认为在未来一两年内，关于语言模型的一个有趣的事情是，你已经看到了，OpenAI和其他地方都有一堆针对数据的诉讼，究竟数据来自哪里。
所以我认为一个非常优雅的解决方案是，你可以在你知道是安全的数据上训练一个rag系统。
所以你可以在Wikipedia上训练这个东西，但是在测试时，你可以给它一个数据存储，里面可能有一些稍微有风险的信息。
所以这个互联网上所有东西的大量索引，包括一些可能更高风险的东西，你仍然可以把它们放在你的索引中，但是你的语言模型，你的检索增强语言模型，我应该说，你知道那个东西是安全的，因为它是在公共领域的数据上训练的。
所以这就是他们在Silo中所做的，他们表明这个方法非常有效。
所以这是对语言模型部署周围的合规性和法律风险的一个可能的解决方案。
你的一个同事也有一篇很棒的论文，关于联系在中间丢失的问题。
我认为这也是一种很有趣的现象。
这是在冷冻抹布系统上进行的，但语言模型在关注的事物上与人类非常相似。
因此，如果您向它们提供了一堆您要检索的内容，那么它们会关注您列出的第一件事和最后一件事，而忽略中间的内容。
因此，如果它真的尊重秩函数，那么这条曲线就会一直向下，但它又有点向上。
因此，我认为这是一个非常有趣的观察结果，它显示了这些系统的脆弱性。
所以，如果你有一个冷冻抹布系统，它可能会非常非常脆，就像撤退上下文的顺序对你是否得到正确答案非常重要。
在创建读取数据的过程中，你是否会特别关注输出结果是否与正确的数据库相匹配？
是的，所以我刚才描述的，有人问，你实际上是如何做到的，所以我说还有其他方法可以做到这一点，然后问题是，你是如何做到这一点的？
所以，你要做的就是使用强化。
所以，是的，已经有人在做这件事了。<EOS
所以一些较旧的论文在尝试这个方案，但其中一个大问题是，我认为重新连接的解决方案在解决这个问题上更加优雅，因为你实际上是从语言模型中获取信号。
如果你只是进行强化学习，方差会非常高。
所以如果你不想破坏你的索引，这会变得非常棘手。
但是人们已经尝试过了，是的。
所以有一些来自OpenAI的非常好的工作，他们基本上展示了，再次强调，我们在这里更多地考虑的是代理，对吧？
他们展示了与先前的活跃检索相似的东西，这与只能读取一些网络搜索的索引无关，对吧？
显然，在这种情况下，你并不一定能够访问网络搜索。
所以Bing或者无论他们在这里使用什么都不会更新其参数，但我只是想让你想一想，这是你可以做的另一件事，对吧？
如果我们将这个问题真正推广到一般形式，那么你可以将语言模型视为工具使用者。
因此，与其仅仅是检索增强语言模型，我们可以工具增强语言模型，而检索只是语言模型可以访问的许多工具之一。
我们可以在这些工具的输出之上添加重新排序器和其他东西。
所以，我认为一个重要的问题是，你到底如何让系统学习东西呢？
因此，如果我们希望系统真正学会如何正确地采取这些行动，我们就需要强化学习。
因此，是的，在这种自我-评估架构中，他们对这种检索步骤进行了极端处理，然后对其进行了批评，然后基本上只用一个语言模型就进行了一些自然语言推理以及所有这些来回答问题。
所以，另一个缺失的部分，我只是在回顾一些人们研究过的一系列开放性问题，但如果有任何你想了解的事情，请随时打断我。
但是，指令调整，我们在讲座开始时就确定了这对于使事情正常运行非常重要，因此修复用户界面，但指令调整几乎总是只发生在语言模型上，而不是整个系统上。
所以我认为人们现在关注的一个有趣的事情是，使用类似Radit和InstructRetro的工具，我们如何 fine-tune 整个检索增强系统？
一直到检索步骤，我们是否可以生成数据，以便它也正确地遵循指示，而目前在任何这些模型架构中都没有发生。
最后，我认为如果我不谈论人们所谓的高级RAG，那就会过失。
就像开发者社区真的做了一些很棒的工作。
像LaLaIndex和LangChain这样的框架，以及像Chroma和Weaviate这样的所有这些开源矢量数据库，它们都致力于让RAG变得非常容易，但这都是冻结的RAG，对吧？
但即使是在冻结的RAG中，你也可以做一些令人难以置信的事情。
我们已经提到了其中一些，比如child-parent递归检索器，你找到小部分，然后将周围的大部分提供给语言模型，你可以进行混合搜索，其中我们使用相互排名融合。
所以在将最终结果传递给语言模型之前，我们有不同的搜索结果，我们没有在得到最终结果之前将它们合并。
还有zero shot，像一个大型语言模型重新排序器。
所以基本上分数函数并不是来自于您的检索，而是直接来自语言模型。
然后是假设性文件嵌入，我认为这是一个非常棒的想法。
所以您基本上是通过幻觉来修复幻觉。
所以您提出一个问题，然后让语言模型产生一堆可能的答案。
然后您去搜索可能答案的最近邻居，并将它们作为上下文，然后它根据此给出正确答案。
所以这实际上就像是在产生答案的幻觉。
我认为这是一个很好的解决方案。
所以在冻结的RAG社区中也发生了很多事情，我认为这是非常值得关注的。
所以总的来说，看一下这些东西的未来，仍然有很多非常有趣的开放性问题。
所以如果您是一名学生，正在考虑如何解决这些问题中的任何一个，我认为您可以产生相当大的影响。
那么我们究竟如何进行这种架构的预训练，我们甚至需要进行预训练吗？
我认为甚至反向证明了您不一定需要预先训练。
所以也许我们做这件事的方式有些问题。
滑板法律是什么样的？
所以我认为这里有一个非常有趣的问题，如果我有一个巨大的索引和一个非常丰富的编码器，能够包含索引中所有信息，也许我可以移动，所以基本上将所有的记忆解耦到这个索引中。
所以我有一个语言模型它什么都不知道。
它只会说英语。
它只是在上面推理，但它没有知识，因为那总是来自这个检索器。
如果你能做到这样，那么你就会得到非常有趣的扩展折衷。
所以你可以拥有一个微小的语言模型，并使用检索来完成大部分繁重的工作，这很好因为那是一个缓存的计算。
所以你可以，你已经有了嵌入，你只需要做点积。
所以这比在语言模型中进行自注意力要高效得多。
我们可以超越编码器吗？
所以向量数据库，我喜欢建立向量数据库的人，但我不确定我们会保留向量数据库多久，因为我认为重新排序器可能效果一样好，而且M25比向量数据库更有效。
所以我真的不明白为什么我们需要专门的向量数据库。
所以我们正在看到的，但也许这是对硅谷投资策略等的一种批评。
但是很多这些向量数据库公司现在基本上都成为了数据库公司。
所以他们正在添加所有这些稀疏的东西，因为密集的东西还不够。
事实证明，已经有很多相当不错的稀疏数据库存在，比如Postgres之类的。
他们也都在向他们的数据库中添加向量。
所以我认为这一切都将融合到数据库中。
所以我认为有一些有趣的东西可以看一下，以了解数据的性质。
所以真正通过这个指示问题，我们是否可以为合成训练机架系统生成更好的数据？
然后，我认为围绕着我们如何实际衡量机架系统是否好这个问题存在着巨大的开放性问题。
所以现在我们只是看下游的性能，这还算可以，但如果检索出错就很难衡量了。
但是如何衡量您的检索是否正确也非常困难。
所以有一些框架，他们试图取您的检索准确度和语言模型准确度的谐波平均值。
但我认为那些也很糟糕，因为我们实际上没有很好的数据集来衡量它。
所以我认为这也是一个非常酷的问题要解决。
我个人总是对的另一个问题充满兴奋，那就是多模态性。
那么为什么我们要局限在只有文本的机架系统呢？
你可以用图像做同样的事情。
你可以用视觉来增强语言模型。
所以我们在Lens上做了这项工作，我们有一个被增强的语言模型，能够看到你可以简单地给一个计算机视觉管道，就像一个检索管道一样，然后将其传递给一个冻结的语言模型并传递上下文。
那个系统实际上是一个令人惊叹的视觉问答系统。
它接近于DeepMind的flamingo的最新水平，这也很难复制，因为没有开源版本。
所以我们在2021年已经开始了一些关于这个的初步工作，在那里我们有这个跨模态检索，还有FAIR最近的一些工作，他们也在研究这个。
所以我认为真的是的，如果你看一下这个领域的趋势，多模态性与GPD4V之类的东西确实是一个热门话题。
所以一切都在朝着那个方向发展。
这是一个有趣的想法。
总的来说，我认为如果每个人都能从 RAG 1 .0，这个冻结的科学怪人 RAG 转向这个更加优化的版本，RAG 2 .0，那将是很好的。
所以，这确实是关于系统而不是模型。
当你是一个检索器时，不仅仅是你的语言模型，而且它们是有些分开的。
这是关于从系统的角度思考整个事情和你正在尝试解决的问题。
所以我认为在深度学习中，事情总是在进步，如果你端对端地优化系统，那总是会获胜的。
就像在计算机视觉或自然语言处理的早期，我们有像解析器和场景解析器这样的东西。
而现在所有这些都已经不复存在，因为我们端对端地优化了系统。
所以这里也会发生同样的事情。
所以如果我们把它推向极致，就像你的文档中有一个分块器一样，把它分割成片段，你也可以应用到其中。
为什么不呢？
真的有人应该这样做。
所以，是的，我认为权衡成本和质量以及零 shot 领域泛化，这确实是这些内容的重点所在。
语言模型现在非常了不起，但很多时候它们的成本太高，无法部署在公司可以从中赚钱的地方。
所以你想要做的是使其比现在更有效，并找到正确的成本质量权衡。
我能想到的最简单的方法就是通过检索增强来实现。
但显然，我很有偏见。
所以，是的，那实际上就是我全部的内容了。
所以如果你对此感兴趣，我在斯坦福，所以我可以与你一起在这些主题上进行研究项目，或者如果你愿意，你也可以加入 Contextual，因为我们每天都在处理这些内容。
谢谢。
嗯，抱歉，我之前有个问题。
是的，我觉得你之前说的真的，我认为真的非常有帮助，关于 Mistral 7B。
你谈到了将滑动窗口注意力与卷积神经网络进行比较。
我确实看到了这种类比，因为在卷积神经网络中，你有几层不同的卷积层，而顶层的卷积层能够看到底层卷积层中更大的感受野。
使用卷积层，你能够调整滤波器的大小和步幅。
所以你能够看到一个不同的感受野。
我在想，通过调整 Mistral 7B 中的不同的 transformer 层，因为每个 transformer 层将覆盖不同的一组 tokens，你是否能够看到相同的创新。
如果你能够调整 transformer 架构，就像你调整那些卷积层、滤波器大小、感受野那样，也许我们可以在 transformer 领域中做一些优化，就像我们已经在卷积层中做过的那样。
是的，我认为这是个好主意。
有一篇关于轻量级卷积的好论文，我想是 Michael Ali 和 David Gange 还有一群人写的，基本上这篇论文是在完全相同的时间发布的，就像 transformer 一样，而 transformer 稍微更优化于 GPU 计算，但是卷积模型实际上比 transformer 略好一些。
所以这绝对值得探索。
好的，酷，谢谢。
重新排序 PM2D 部分的优势是什么，这样做会放弃大规模搜索的许多可能性吗？
这方面的权衡是什么？
是的，这取决于问题。
我认为你可能想做的是用 VM25 投下一张白网，然后用密集搜索来缩小范围。
所以你经常会看到这种是一个两阶段的过程，第一阶段有点嘈杂。
你实际上可以给你的检索添加噪声，然后使用密集的部分来过滤它。
是的，每个人都试图可能将他们的模型调整到某种领域特定的领域。
我认为有很多方法可以解决这个问题。
一种方法是在多种学习方式或微调方法中使用指导调整。
另一种方法就是这次讲座的主要主题，即使用虚拟增强方式。
所以我想知道虚拟整体增强方法的优势。
你认为虚拟增强方法的能力或质量能够与微调方法相匹配，我认为，对于类型学习来说？
是的，所以我认为实际上会发生的是，所有这些都会融合在一起。
所以，如果你真的像端到端的RAG 2.0风格一样训练这些东西，那么你也可以在一些用例端到端地对该系统进行微调。
所以为什么你只会采用检索增强系统，如果你也可以在你关心的事情上进行微调呢？
所以我认为最终，每个人都会做所有这些事情。
然后还有一些问题，比如，你如何高效地做到这一点？
所以这就是为什么你会在之后使用它的原因。
诸如此类的事情。
我想还有另一个问题。
我对硬件很好奇。
你说它会变成数据库那种东西，就像一个智能数据库。
那检索硬件呢？
而且你知道，因为我们已经做了这么多的学习部分，但是，因为它是巨大的。
是的，是的。
这是数万亿的集合。
你有没有什么想法就是数据库问题吗？
所以我不知道我是否被允许这样说，但是最近一家最大的芯片制造商，他们的股票表现得非常好，他们有一些专门的检索硬件要推出。
我想其中一些可能已经推出了。
是的，是的，非常高效的稠密检索是一个非常大的业务。
其他问题？
我真的认为解决云端问题会解决业务中的这个问题。
是的，我认为如果你把它推向极端的话。
所以现在一个很大的问题是，如果你对一个已经会产生幻觉的现有语言模型进行语境化，那么要想消除幻觉就会变得相当困难，对吧？
所以如果你在 GPT-4 上做重新连接，GPT-4 仍然可能产生幻觉。
所以它基本上可能会忽略你检索到的所有内容，然后随心所欲地做任何事情。
所以这就是为什么你希望端到端地训练系统的原因之一。
如果你把这个推向极端，就像我说的，如果你只让语言模型进行推理和说话，所以它知道英语和推理，但它没有知识，所有的知识都来自其他地方，那么它就无法产生幻觉。
所以它实际上完全是基于你的索引中的内容。
但它们都涉及到了幻觉。
我有点沮丧，因为领域里很多人甚至不理解幻觉是什么意思。
所以很多人把幻觉和正确性或错误性混为一谈。
他们说，哦，模型犯了一个错误。
它产生了幻觉。
这就像，不，它犯了一个错误。
那与幻觉不同。
幻觉，我认为是非常具体的。
我检索到了一些东西。
所以我有某种反事实的真相，但我所说的并不符合那个真相。
是的，我认为斯坦福大学也有一群人在努力改进幻觉的更好测量和定义等方面。
我理解得对，你对幻觉的定义只在与人有关的情境中才有意义。
是的，某种真相。
所以幻觉真的就像，有一件事是真实的。
所以如果我们在谈论幻觉，是的，如果我们只是谈论一般的参数化语言模型，那么真相就是我们认为的真实。
但在语言模型犯错之前，我们必须处理一些工作，才能称之为犯错。
是的，在真相面前，我想你在解决我能够走上那条路的幻觉问题。
你在努力为保存一个术语的真相工作吗？如果我生成了操作的能力并说，嗯，我从未有过一个总统能够为所有党派做任何事情。
你在这个主题上分享你的工作吗？
是的，我也喜欢独立的豪宅区域。
所以我认为整个重点是你可以有不同的索引和不同的地面真相的定义。
所以我想你可以说我只信任档案，或者我只信任像同行评审的论文，而不仅仅是档案。
所以你可以在测试时在你的架构中做出关于你如何定义地面真相的决定。
我也认为实际上，现在有很多关于这个问题的工作正在进行，你可以控制你希望它在你的地面真相中根植的程度。
所以这是关于幻觉的另一种误解，有时幻觉实际上是好的。
如果你有一个创意写作助手，你想提出一些新颖的想法，你希望语言模型能产生幻觉。
所以我认为你想要的是一种可调节的旋钮，你可以说，哦，现在你可以产生幻觉，现在也许你应该只告诉我真相。
还有别的吗？
它确实，我认为那已经被享受了有多少被隐藏了。
这只是一些问题。
是的，但是温度，那只是关于你如何采样的，对吧？
你从中抽样的分布有多平坦。
是的，但是是的，所以即使你的温度很低，它仍然可能生成随机的东西，对吧？
所以它只是说，那么你很可能会像贪婪抽样一样。
所以我认为你想要的是比那更复杂的东西。
好的，很多有趣的问题。
是的，我喜欢这个问题。
现在再次回到精彩的演讲。
